{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8Ij1vxZQ6Ix"
   },
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLA service initialization - CUDA\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mWfJ0Z5QQ6JE"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blTVtc4MQ6JK"
   },
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uGCYPABJQ6JM"
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "573Lxpckb6dO"
   },
   "source": [
    "#### Check what the max, min and mean time values are. This will help us in defining the 'next_step' function in the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3OrekPlbwp4",
    "outputId": "5229ed02-87f0-44d6-9e7c-551e7166007f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "11.0\n",
      "0.0\n",
      "3.0542857142857143\n",
      "7.93705306122449\n"
     ]
    }
   ],
   "source": [
    "print(type(Time_matrix))\n",
    "print(Time_matrix.max())\n",
    "print(Time_matrix.min())\n",
    "print(Time_matrix.mean())\n",
    "print(Time_matrix.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cBe6IZqcDbF"
   },
   "source": [
    "#### Since the max time is 11 hours between any 2 points, the next state of the cab driver may increase at most by 1 day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJvDTRo3Q6JN"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SQnwpFsrp3Tv"
   },
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUeWmqVJQ6JS"
   },
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BCagoCqNQ6JU"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay = -0.0005 #for 15k\n",
    "        #self.epsilon_decay = -0.00015 #for 20k\n",
    "        self.epsilon_min = 0.00001\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # Initialize the value of the states tracked\n",
    "        self.states_tracked = []\n",
    "        \n",
    "        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n",
    "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Function that takes in the agent and constructs the network\n",
    "        to train it\n",
    "        @return model\n",
    "        @params agent\n",
    "        \"\"\"\n",
    "        input_shape = self.state_size\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, possible_actions_index, actions):\n",
    "        \"\"\"\n",
    "        get action in a state according to an epsilon-greedy approach\n",
    "        possible_actions_index, actions are the 'ride requests' that teh driver got.\n",
    "        \"\"\"        \n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after each episode       \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from the ride requests\n",
    "            return random.choice(possible_actions_index)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "\n",
    "            # Use the model to predict the Q_values.\n",
    "            q_value = self.model.predict(state)\n",
    "\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "\n",
    "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, done):\n",
    "        \"\"\"appends the new agent run output to replay buffer\"\"\"\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "        \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \"\"\" \n",
    "        Function to train the model on eacg step run.\n",
    "        Picks the random memory events according to batch size and \n",
    "        runs it through the network to train it.\n",
    "        \"\"\"\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch1(state)     \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "            # model fit\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "    def save_tracking_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_tracked.append(q_value[0][2])\n",
    "        \n",
    "    def save_test_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_test.append(q_value[0][2])\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSRJU7n8Q6JW"
   },
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WTQx9gYeNTQ"
   },
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mKOKQKCceIHB"
   },
   "outputs": [],
   "source": [
    "episode_time = 24*30 #30 days before which car has to be recharged\n",
    "n_episodes = 15000\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "\n",
    "# Invoke Env class\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()\n",
    "\n",
    "# Set up state and action sizes.\n",
    "state_size = m+t+d\n",
    "action_size = len(action_space)\n",
    "\n",
    "# Invoke agent class\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "# Rewards for state [0,0,0] being tracked.\n",
    "rewards_init_state = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-LjiJlpeiJU"
   },
   "source": [
    "#### Run the episodes, build up replay buffer and train the model.\n",
    "Note:\n",
    "The moment total episode time exceeds 720 (30 days), we ignore the most recent ride and do NOT save that experience in the replay memory\n",
    "The init state is randomly picked from the state space for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUED4EPpQ6JX",
    "outputId": "fec1562e-6dea-4243-9fd7-9e06335b2920",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model 0\n",
      "episode 9, reward -175.0, memory_length 1369, epsilon 0.9955001547284723 total_time 734.0\n",
      "episode 19, reward 191.0, memory_length 2000, epsilon 0.9905350769930761 total_time 727.0\n",
      "episode 29, reward -216.0, memory_length 2000, epsilon 0.9855947626861951 total_time 722.0\n",
      "episode 39, reward -243.0, memory_length 2000, epsilon 0.9806790882997144 total_time 721.0\n",
      "episode 49, reward -198.0, memory_length 2000, epsilon 0.9757879309415182 total_time 726.0\n",
      "episode 59, reward -292.0, memory_length 2000, epsilon 0.9709211683324178 total_time 724.0\n",
      "episode 69, reward 99.0, memory_length 2000, epsilon 0.9660786788030947 total_time 729.0\n",
      "episode 79, reward -165.0, memory_length 2000, epsilon 0.9612603412910584 total_time 724.0\n",
      "episode 89, reward -337.0, memory_length 2000, epsilon 0.9564660353376199 total_time 725.0\n",
      "episode 99, reward -63.0, memory_length 2000, epsilon 0.9516956410848808 total_time 732.0\n",
      "episode 109, reward -206.0, memory_length 2000, epsilon 0.9469490392727365 total_time 725.0\n",
      "episode 119, reward -402.0, memory_length 2000, epsilon 0.9422261112358942 total_time 722.0\n",
      "episode 129, reward 185.0, memory_length 2000, epsilon 0.9375267389009072 total_time 728.0\n",
      "episode 139, reward -328.0, memory_length 2000, epsilon 0.9328508047832221 total_time 722.0\n",
      "episode 149, reward -96.0, memory_length 2000, epsilon 0.9281981919842428 total_time 722.0\n",
      "episode 159, reward -126.0, memory_length 2000, epsilon 0.9235687841884068 total_time 726.0\n",
      "episode 169, reward -171.0, memory_length 2000, epsilon 0.918962465660278 total_time 721.0\n",
      "episode 179, reward 80.0, memory_length 2000, epsilon 0.9143791212416534 total_time 724.0\n",
      "episode 189, reward 182.0, memory_length 2000, epsilon 0.9098186363486838 total_time 724.0\n",
      "episode 199, reward -151.0, memory_length 2000, epsilon 0.9052808969690094 total_time 726.0\n",
      "episode 209, reward -75.0, memory_length 2000, epsilon 0.9007657896589091 total_time 727.0\n",
      "episode 219, reward -187.0, memory_length 2000, epsilon 0.8962732015404654 total_time 726.0\n",
      "episode 229, reward 162.0, memory_length 2000, epsilon 0.891803020298741 total_time 730.0\n",
      "episode 239, reward -332.0, memory_length 2000, epsilon 0.8873551341789723 total_time 722.0\n",
      "episode 249, reward 46.0, memory_length 2000, epsilon 0.8829294319837746 total_time 722.0\n",
      "episode 259, reward -119.0, memory_length 2000, epsilon 0.8785258030703623 total_time 726.0\n",
      "episode 269, reward 249.0, memory_length 2000, epsilon 0.8741441373477834 total_time 725.0\n",
      "episode 279, reward 123.0, memory_length 2000, epsilon 0.8697843252741666 total_time 721.0\n",
      "episode 289, reward -237.0, memory_length 2000, epsilon 0.8654462578539829 total_time 721.0\n",
      "episode 299, reward 158.0, memory_length 2000, epsilon 0.8611298266353209 total_time 725.0\n",
      "episode 309, reward 176.0, memory_length 2000, epsilon 0.8568349237071754 total_time 726.0\n",
      "episode 319, reward 115.0, memory_length 2000, epsilon 0.8525614416967494 total_time 726.0\n",
      "episode 329, reward -67.0, memory_length 2000, epsilon 0.84830927376677 total_time 724.0\n",
      "episode 339, reward -54.0, memory_length 2000, epsilon 0.8440783136128177 total_time 724.0\n",
      "episode 349, reward -312.0, memory_length 2000, epsilon 0.8398684554606681 total_time 721.0\n",
      "episode 359, reward -117.0, memory_length 2000, epsilon 0.8356795940636483 total_time 731.0\n",
      "episode 369, reward 375.0, memory_length 2000, epsilon 0.8315116247000052 total_time 723.0\n",
      "episode 379, reward 117.0, memory_length 2000, epsilon 0.8273644431702872 total_time 736.0\n",
      "episode 389, reward 203.0, memory_length 2000, epsilon 0.8232379457947406 total_time 729.0\n",
      "episode 399, reward 286.0, memory_length 2000, epsilon 0.819132029410716 total_time 726.0\n",
      "episode 409, reward -112.0, memory_length 2000, epsilon 0.8150465913700896 total_time 726.0\n",
      "episode 419, reward -72.0, memory_length 2000, epsilon 0.8109815295366979 total_time 732.0\n",
      "episode 429, reward 36.0, memory_length 2000, epsilon 0.8069367422837833 total_time 721.0\n",
      "episode 439, reward 347.0, memory_length 2000, epsilon 0.8029121284914538 total_time 725.0\n",
      "episode 449, reward 26.0, memory_length 2000, epsilon 0.7989075875441549 total_time 723.0\n",
      "episode 459, reward 203.0, memory_length 2000, epsilon 0.7949230193281545 total_time 730.0\n",
      "episode 469, reward 117.0, memory_length 2000, epsilon 0.7909583242290396 total_time 724.0\n",
      "episode 479, reward 198.0, memory_length 2000, epsilon 0.7870134031292261 total_time 721.0\n",
      "episode 489, reward 440.0, memory_length 2000, epsilon 0.7830881574054811 total_time 725.0\n",
      "episode 499, reward 135.0, memory_length 2000, epsilon 0.7791824889264571 total_time 730.0\n",
      "episode 509, reward 74.0, memory_length 2000, epsilon 0.7752963000502389 total_time 721.0\n",
      "episode 519, reward 562.0, memory_length 2000, epsilon 0.7714294936219019 total_time 721.0\n",
      "episode 529, reward -148.0, memory_length 2000, epsilon 0.7675819729710842 total_time 726.0\n",
      "episode 539, reward 391.0, memory_length 2000, epsilon 0.763753641909569 total_time 722.0\n",
      "episode 549, reward 100.0, memory_length 2000, epsilon 0.7599444047288803 total_time 724.0\n",
      "episode 559, reward 280.0, memory_length 2000, epsilon 0.7561541661978903 total_time 724.0\n",
      "episode 569, reward 396.0, memory_length 2000, epsilon 0.7523828315604384 total_time 730.0\n",
      "episode 579, reward 145.0, memory_length 2000, epsilon 0.7486303065329623 total_time 725.0\n",
      "episode 589, reward 492.0, memory_length 2000, epsilon 0.7448964973021404 total_time 726.0\n",
      "episode 599, reward -93.0, memory_length 2000, epsilon 0.7411813105225479 total_time 722.0\n",
      "episode 609, reward 231.0, memory_length 2000, epsilon 0.7374846533143217 total_time 722.0\n",
      "episode 619, reward 168.0, memory_length 2000, epsilon 0.733806433260839 total_time 723.0\n",
      "episode 629, reward 306.0, memory_length 2000, epsilon 0.7301465584064071 total_time 723.0\n",
      "episode 639, reward 297.0, memory_length 2000, epsilon 0.7265049372539636 total_time 726.0\n",
      "episode 649, reward -21.0, memory_length 2000, epsilon 0.7228814787627905 total_time 721.0\n",
      "episode 659, reward -234.0, memory_length 2000, epsilon 0.7192760923462365 total_time 728.0\n",
      "episode 669, reward 236.0, memory_length 2000, epsilon 0.7156886878694535 total_time 723.0\n",
      "episode 679, reward 193.0, memory_length 2000, epsilon 0.7121191756471427 total_time 727.0\n",
      "episode 689, reward 406.0, memory_length 2000, epsilon 0.7085674664413126 total_time 723.0\n",
      "episode 699, reward 111.0, memory_length 2000, epsilon 0.7050334714590482 total_time 721.0\n",
      "episode 709, reward 354.0, memory_length 2000, epsilon 0.7015171023502909 total_time 725.0\n",
      "episode 719, reward 239.0, memory_length 2000, epsilon 0.6980182712056295 total_time 722.0\n",
      "episode 729, reward 622.0, memory_length 2000, epsilon 0.6945368905541035 total_time 724.0\n",
      "episode 739, reward 455.0, memory_length 2000, epsilon 0.6910728733610152 total_time 726.0\n",
      "episode 749, reward 555.0, memory_length 2000, epsilon 0.6876261330257543 total_time 724.0\n",
      "episode 759, reward 432.0, memory_length 2000, epsilon 0.684196583379633 total_time 721.0\n",
      "episode 769, reward 484.0, memory_length 2000, epsilon 0.6807841386837313 total_time 721.0\n",
      "episode 779, reward -97.0, memory_length 2000, epsilon 0.6773887136267543 total_time 722.0\n",
      "episode 789, reward -3.0, memory_length 2000, epsilon 0.6740102233228988 total_time 721.0\n",
      "episode 799, reward 78.0, memory_length 2000, epsilon 0.670648583309731 total_time 721.0\n",
      "episode 809, reward 189.0, memory_length 2000, epsilon 0.6673037095460755 total_time 727.0\n",
      "episode 819, reward 434.0, memory_length 2000, epsilon 0.6639755184099142 total_time 724.0\n",
      "episode 829, reward 438.0, memory_length 2000, epsilon 0.6606639266962953 total_time 724.0\n",
      "episode 839, reward 320.0, memory_length 2000, epsilon 0.6573688516152534 total_time 732.0\n",
      "episode 849, reward 391.0, memory_length 2000, epsilon 0.6540902107897397 total_time 724.0\n",
      "episode 859, reward 304.0, memory_length 2000, epsilon 0.6508279222535631 total_time 725.0\n",
      "episode 869, reward 363.0, memory_length 2000, epsilon 0.64758190444934 total_time 729.0\n",
      "episode 879, reward 172.0, memory_length 2000, epsilon 0.6443520762264566 total_time 725.0\n",
      "episode 889, reward 270.0, memory_length 2000, epsilon 0.6411383568390387 total_time 726.0\n",
      "episode 899, reward 585.0, memory_length 2000, epsilon 0.6379406659439346 total_time 722.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 909, reward 216.0, memory_length 2000, epsilon 0.6347589235987051 total_time 725.0\n",
      "episode 919, reward 314.0, memory_length 2000, epsilon 0.631593050259626 total_time 721.0\n",
      "episode 929, reward 170.0, memory_length 2000, epsilon 0.6284429667796988 total_time 721.0\n",
      "episode 939, reward 374.0, memory_length 2000, epsilon 0.6253085944066726 total_time 727.0\n",
      "episode 949, reward 701.0, memory_length 2000, epsilon 0.6221898547810748 total_time 722.0\n",
      "episode 959, reward 646.0, memory_length 2000, epsilon 0.6190866699342522 total_time 726.0\n",
      "episode 969, reward 541.0, memory_length 2000, epsilon 0.6159989622864221 total_time 722.0\n",
      "episode 979, reward 321.0, memory_length 2000, epsilon 0.6129266546447325 total_time 726.0\n",
      "episode 989, reward 335.0, memory_length 2000, epsilon 0.6098696702013323 total_time 722.0\n",
      "episode 999, reward 474.0, memory_length 2000, epsilon 0.6068279325314512 total_time 726.0\n",
      "Saving Model 1000\n",
      "episode 1009, reward 339.0, memory_length 2000, epsilon 0.6038013655914889 total_time 727.0\n",
      "episode 1019, reward 614.0, memory_length 2000, epsilon 0.6007898937171146 total_time 733.0\n",
      "episode 1029, reward 682.0, memory_length 2000, epsilon 0.5977934416213744 total_time 721.0\n",
      "episode 1039, reward 388.0, memory_length 2000, epsilon 0.5948119343928097 total_time 726.0\n",
      "episode 1049, reward 775.0, memory_length 2000, epsilon 0.5918452974935846 total_time 723.0\n",
      "episode 1059, reward 280.0, memory_length 2000, epsilon 0.5888934567576223 total_time 721.0\n",
      "episode 1069, reward 477.0, memory_length 2000, epsilon 0.5859563383887504 total_time 736.0\n",
      "episode 1079, reward 90.0, memory_length 2000, epsilon 0.5830338689588568 total_time 734.0\n",
      "episode 1089, reward 421.0, memory_length 2000, epsilon 0.5801259754060536 total_time 723.0\n",
      "episode 1099, reward 722.0, memory_length 2000, epsilon 0.5772325850328504 total_time 729.0\n",
      "episode 1109, reward 481.0, memory_length 2000, epsilon 0.5743536255043372 total_time 723.0\n",
      "episode 1119, reward 334.0, memory_length 2000, epsilon 0.5714890248463761 total_time 727.0\n",
      "episode 1129, reward 417.0, memory_length 2000, epsilon 0.5686387114438011 total_time 725.0\n",
      "episode 1139, reward 485.0, memory_length 2000, epsilon 0.5658026140386287 total_time 721.0\n",
      "episode 1149, reward 585.0, memory_length 2000, epsilon 0.5629806617282764 total_time 730.0\n",
      "episode 1159, reward 675.0, memory_length 2000, epsilon 0.5601727839637891 total_time 731.0\n",
      "episode 1169, reward 625.0, memory_length 2000, epsilon 0.5573789105480766 total_time 728.0\n",
      "episode 1179, reward 806.0, memory_length 2000, epsilon 0.5545989716341581 total_time 723.0\n",
      "episode 1189, reward 720.0, memory_length 2000, epsilon 0.5518328977234156 total_time 731.0\n",
      "episode 1199, reward 821.0, memory_length 2000, epsilon 0.5490806196638577 total_time 727.0\n",
      "episode 1209, reward 925.0, memory_length 2000, epsilon 0.5463420686483893 total_time 725.0\n",
      "episode 1219, reward 469.0, memory_length 2000, epsilon 0.5436171762130925 total_time 731.0\n",
      "episode 1229, reward 860.0, memory_length 2000, epsilon 0.5409058742355145 total_time 725.0\n",
      "episode 1239, reward 471.0, memory_length 2000, epsilon 0.5382080949329644 total_time 725.0\n",
      "episode 1249, reward 743.0, memory_length 2000, epsilon 0.5355237708608195 total_time 722.0\n",
      "episode 1259, reward 337.0, memory_length 2000, epsilon 0.5328528349108379 total_time 721.0\n",
      "episode 1269, reward 568.0, memory_length 2000, epsilon 0.5301952203094819 total_time 721.0\n",
      "episode 1279, reward 414.0, memory_length 2000, epsilon 0.5275508606162479 total_time 721.0\n",
      "episode 1289, reward 598.0, memory_length 2000, epsilon 0.5249196897220061 total_time 723.0\n",
      "episode 1299, reward 799.0, memory_length 2000, epsilon 0.5223016418473468 total_time 724.0\n",
      "episode 1309, reward 417.0, memory_length 2000, epsilon 0.519696651540937 total_time 726.0\n",
      "episode 1319, reward 690.0, memory_length 2000, epsilon 0.5171046536778833 total_time 728.0\n",
      "episode 1329, reward 556.0, memory_length 2000, epsilon 0.514525583458104 total_time 721.0\n",
      "episode 1339, reward 641.0, memory_length 2000, epsilon 0.5119593764047093 total_time 722.0\n",
      "episode 1349, reward 662.0, memory_length 2000, epsilon 0.5094059683623896 total_time 723.0\n",
      "episode 1359, reward 847.0, memory_length 2000, epsilon 0.5068652954958104 total_time 737.0\n",
      "episode 1369, reward 909.0, memory_length 2000, epsilon 0.5043372942880178 total_time 721.0\n",
      "episode 1379, reward 891.0, memory_length 2000, epsilon 0.50182190153885 total_time 725.0\n",
      "episode 1389, reward 762.0, memory_length 2000, epsilon 0.49931905436335716 total_time 726.0\n",
      "episode 1399, reward 717.0, memory_length 2000, epsilon 0.49682869019022974 total_time 722.0\n",
      "episode 1409, reward 645.0, memory_length 2000, epsilon 0.49435074676023355 total_time 726.0\n",
      "episode 1419, reward 610.0, memory_length 2000, epsilon 0.4918851621246539 total_time 725.0\n",
      "episode 1429, reward 977.0, memory_length 2000, epsilon 0.4894318746437464 total_time 722.0\n",
      "episode 1439, reward 806.0, memory_length 2000, epsilon 0.4869908229851962 total_time 727.0\n",
      "episode 1449, reward 584.0, memory_length 2000, epsilon 0.48456194612258474 total_time 721.0\n",
      "episode 1459, reward 1028.0, memory_length 2000, epsilon 0.48214518333386397 total_time 721.0\n",
      "episode 1469, reward 757.0, memory_length 2000, epsilon 0.47974047419983834 total_time 724.0\n",
      "episode 1479, reward 626.0, memory_length 2000, epsilon 0.4773477586026542 total_time 732.0\n",
      "episode 1489, reward 774.0, memory_length 2000, epsilon 0.474966976724297 total_time 724.0\n",
      "episode 1499, reward 729.0, memory_length 2000, epsilon 0.47259806904509577 total_time 727.0\n",
      "episode 1509, reward 869.0, memory_length 2000, epsilon 0.4702409763422352 total_time 724.0\n",
      "episode 1519, reward 903.0, memory_length 2000, epsilon 0.4678956396882749 total_time 721.0\n",
      "episode 1529, reward 671.0, memory_length 2000, epsilon 0.4655620004496764 total_time 734.0\n",
      "episode 1539, reward 855.0, memory_length 2000, epsilon 0.46324000028533724 total_time 729.0\n",
      "episode 1549, reward 923.0, memory_length 2000, epsilon 0.4609295811451323 total_time 727.0\n",
      "episode 1559, reward 1008.0, memory_length 2000, epsilon 0.4586306852684627 total_time 727.0\n",
      "episode 1569, reward 825.0, memory_length 2000, epsilon 0.4563432551828119 total_time 721.0\n",
      "episode 1579, reward 712.0, memory_length 2000, epsilon 0.4540672337023085 total_time 722.0\n",
      "episode 1589, reward 737.0, memory_length 2000, epsilon 0.45180256392629703 total_time 721.0\n",
      "episode 1599, reward 1029.0, memory_length 2000, epsilon 0.4495491892379152 total_time 727.0\n",
      "episode 1609, reward 973.0, memory_length 2000, epsilon 0.4473070533026783 total_time 729.0\n",
      "episode 1619, reward 1022.0, memory_length 2000, epsilon 0.4450761000670712 total_time 732.0\n",
      "episode 1629, reward 1014.0, memory_length 2000, epsilon 0.4428562737571469 total_time 721.0\n",
      "episode 1639, reward 526.0, memory_length 2000, epsilon 0.44064751887713194 total_time 723.0\n",
      "episode 1649, reward 631.0, memory_length 2000, epsilon 0.4384497802080393 total_time 729.0\n",
      "episode 1659, reward 446.0, memory_length 2000, epsilon 0.4362630028062879 total_time 725.0\n",
      "episode 1669, reward 1128.0, memory_length 2000, epsilon 0.43408713200232857 total_time 725.0\n",
      "episode 1679, reward 656.0, memory_length 2000, epsilon 0.43192211339927816 total_time 721.0\n",
      "episode 1689, reward 896.0, memory_length 2000, epsilon 0.4297678928715586 total_time 724.0\n",
      "episode 1699, reward 468.0, memory_length 2000, epsilon 0.4276244165635446 total_time 727.0\n",
      "episode 1709, reward 919.0, memory_length 2000, epsilon 0.42549163088821684 total_time 726.0\n",
      "episode 1719, reward 839.0, memory_length 2000, epsilon 0.4233694825258223 total_time 722.0\n",
      "episode 1729, reward 999.0, memory_length 2000, epsilon 0.4212579184225415 total_time 730.0\n",
      "episode 1739, reward 534.0, memory_length 2000, epsilon 0.4191568857891617 total_time 723.0\n",
      "episode 1749, reward 717.0, memory_length 2000, epsilon 0.4170663320997578 total_time 726.0\n",
      "episode 1759, reward 894.0, memory_length 2000, epsilon 0.4149862050903786 total_time 721.0\n",
      "episode 1769, reward 748.0, memory_length 2000, epsilon 0.4129164527577405 total_time 728.0\n",
      "episode 1779, reward 1105.0, memory_length 2000, epsilon 0.41085702335792745 total_time 730.0\n",
      "episode 1789, reward 950.0, memory_length 2000, epsilon 0.40880786540509717 total_time 721.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1799, reward 867.0, memory_length 2000, epsilon 0.4067689276701942 total_time 729.0\n",
      "episode 1809, reward 941.0, memory_length 2000, epsilon 0.40474015917966877 total_time 726.0\n",
      "episode 1819, reward 946.0, memory_length 2000, epsilon 0.4027215092142031 total_time 731.0\n",
      "episode 1829, reward 1086.0, memory_length 2000, epsilon 0.4007129273074429 total_time 729.0\n",
      "episode 1839, reward 1081.0, memory_length 2000, epsilon 0.39871436324473586 total_time 721.0\n",
      "episode 1849, reward 911.0, memory_length 2000, epsilon 0.3967257670618763 total_time 721.0\n",
      "episode 1859, reward 847.0, memory_length 2000, epsilon 0.3947470890438561 total_time 732.0\n",
      "episode 1869, reward 419.0, memory_length 2000, epsilon 0.3927782797236218 total_time 722.0\n",
      "episode 1879, reward 956.0, memory_length 2000, epsilon 0.3908192898808378 total_time 721.0\n",
      "episode 1889, reward 707.0, memory_length 2000, epsilon 0.388870070540656 total_time 726.0\n",
      "episode 1899, reward 971.0, memory_length 2000, epsilon 0.38693057297249134 total_time 724.0\n",
      "episode 1909, reward 907.0, memory_length 2000, epsilon 0.3850007486888037 total_time 725.0\n",
      "episode 1919, reward 735.0, memory_length 2000, epsilon 0.3830805494438854 total_time 726.0\n",
      "episode 1929, reward 740.0, memory_length 2000, epsilon 0.38116992723265536 total_time 723.0\n",
      "episode 1939, reward 1161.0, memory_length 2000, epsilon 0.3792688342894587 total_time 722.0\n",
      "episode 1949, reward 340.0, memory_length 2000, epsilon 0.3773772230868729 total_time 721.0\n",
      "episode 1959, reward 902.0, memory_length 2000, epsilon 0.37549504633451936 total_time 722.0\n",
      "episode 1969, reward 857.0, memory_length 2000, epsilon 0.37362225697788115 total_time 722.0\n",
      "episode 1979, reward 902.0, memory_length 2000, epsilon 0.37175880819712703 total_time 721.0\n",
      "episode 1989, reward 975.0, memory_length 2000, epsilon 0.3699046534059402 total_time 723.0\n",
      "episode 1999, reward 656.0, memory_length 2000, epsilon 0.3680597462503545 total_time 721.0\n",
      "Saving Model 2000\n",
      "episode 2009, reward 862.0, memory_length 2000, epsilon 0.3662240406075948 total_time 721.0\n",
      "episode 2019, reward 925.0, memory_length 2000, epsilon 0.3643974905849244 total_time 725.0\n",
      "episode 2029, reward 1225.0, memory_length 2000, epsilon 0.3625800505184978 total_time 726.0\n",
      "episode 2039, reward 487.0, memory_length 2000, epsilon 0.3607716749722184 total_time 726.0\n",
      "episode 2049, reward 825.0, memory_length 2000, epsilon 0.3589723187366037 total_time 723.0\n",
      "episode 2059, reward 752.0, memory_length 2000, epsilon 0.35718193682765376 total_time 724.0\n",
      "episode 2069, reward 904.0, memory_length 2000, epsilon 0.3554004844857278 total_time 723.0\n",
      "episode 2079, reward 833.0, memory_length 2000, epsilon 0.35362791717442443 total_time 721.0\n",
      "episode 2089, reward 873.0, memory_length 2000, epsilon 0.35186419057946866 total_time 723.0\n",
      "episode 2099, reward 776.0, memory_length 2000, epsilon 0.3501092606076035 total_time 725.0\n",
      "episode 2109, reward 984.0, memory_length 2000, epsilon 0.3483630833854885 total_time 721.0\n",
      "episode 2119, reward 995.0, memory_length 2000, epsilon 0.34662561525860197 total_time 721.0\n",
      "episode 2129, reward 969.0, memory_length 2000, epsilon 0.34489681279015044 total_time 722.0\n",
      "episode 2139, reward 828.0, memory_length 2000, epsilon 0.34317663275998195 total_time 725.0\n",
      "episode 2149, reward 617.0, memory_length 2000, epsilon 0.3414650321635064 total_time 731.0\n",
      "episode 2159, reward 1112.0, memory_length 2000, epsilon 0.33976196821061944 total_time 724.0\n",
      "episode 2169, reward 955.0, memory_length 2000, epsilon 0.3380673983246338 total_time 730.0\n",
      "episode 2179, reward 966.0, memory_length 2000, epsilon 0.336381280141214 total_time 726.0\n",
      "episode 2189, reward 1130.0, memory_length 2000, epsilon 0.33470357150731744 total_time 727.0\n",
      "episode 2199, reward 786.0, memory_length 2000, epsilon 0.3330342304801412 total_time 723.0\n",
      "episode 2209, reward 846.0, memory_length 2000, epsilon 0.33137321532607245 total_time 722.0\n",
      "episode 2219, reward 979.0, memory_length 2000, epsilon 0.3297204845196459 total_time 725.0\n",
      "episode 2229, reward 1014.0, memory_length 2000, epsilon 0.32807599674250526 total_time 726.0\n",
      "episode 2239, reward 828.0, memory_length 2000, epsilon 0.32643971088237056 total_time 726.0\n",
      "episode 2249, reward 1038.0, memory_length 2000, epsilon 0.32481158603200994 total_time 725.0\n",
      "episode 2259, reward 1020.0, memory_length 2000, epsilon 0.32319158148821747 total_time 728.0\n",
      "episode 2269, reward 1043.0, memory_length 2000, epsilon 0.3215796567507951 total_time 722.0\n",
      "episode 2279, reward 910.0, memory_length 2000, epsilon 0.31997577152154044 total_time 721.0\n",
      "episode 2289, reward 1162.0, memory_length 2000, epsilon 0.31837988570323916 total_time 733.0\n",
      "episode 2299, reward 1102.0, memory_length 2000, epsilon 0.3167919593986629 total_time 728.0\n",
      "episode 2309, reward 1333.0, memory_length 2000, epsilon 0.3152119529095711 total_time 728.0\n",
      "episode 2319, reward 1027.0, memory_length 2000, epsilon 0.3136398267357194 total_time 725.0\n",
      "episode 2329, reward 935.0, memory_length 2000, epsilon 0.31207554157387146 total_time 725.0\n",
      "episode 2339, reward 1078.0, memory_length 2000, epsilon 0.3105190583168168 total_time 724.0\n",
      "episode 2349, reward 1184.0, memory_length 2000, epsilon 0.30897033805239293 total_time 723.0\n",
      "episode 2359, reward 771.0, memory_length 2000, epsilon 0.3074293420625127 total_time 723.0\n",
      "episode 2369, reward 976.0, memory_length 2000, epsilon 0.3058960318221959 total_time 722.0\n",
      "episode 2379, reward 1340.0, memory_length 2000, epsilon 0.30437036899860687 total_time 723.0\n",
      "episode 2389, reward 1144.0, memory_length 2000, epsilon 0.3028523154500953 total_time 727.0\n",
      "episode 2399, reward 1119.0, memory_length 2000, epsilon 0.30134183322524366 total_time 725.0\n",
      "episode 2409, reward 627.0, memory_length 2000, epsilon 0.29983888456191743 total_time 723.0\n",
      "episode 2419, reward 1158.0, memory_length 2000, epsilon 0.29834343188632195 total_time 721.0\n",
      "episode 2429, reward 981.0, memory_length 2000, epsilon 0.2968554378120623 total_time 723.0\n",
      "episode 2439, reward 937.0, memory_length 2000, epsilon 0.2953748651392093 total_time 728.0\n",
      "episode 2449, reward 968.0, memory_length 2000, epsilon 0.2939016768533689 total_time 725.0\n",
      "episode 2459, reward 1067.0, memory_length 2000, epsilon 0.2924358361247571 total_time 722.0\n",
      "episode 2469, reward 943.0, memory_length 2000, epsilon 0.2909773063072796 total_time 722.0\n",
      "episode 2479, reward 1105.0, memory_length 2000, epsilon 0.28952605093761474 total_time 722.0\n",
      "episode 2489, reward 1233.0, memory_length 2000, epsilon 0.28808203373430286 total_time 721.0\n",
      "episode 2499, reward 836.0, memory_length 2000, epsilon 0.28664521859683867 total_time 721.0\n",
      "episode 2509, reward 749.0, memory_length 2000, epsilon 0.2852155696047688 total_time 722.0\n",
      "episode 2519, reward 851.0, memory_length 2000, epsilon 0.28379305101679403 total_time 722.0\n",
      "episode 2529, reward 885.0, memory_length 2000, epsilon 0.28237762726987564 total_time 723.0\n",
      "episode 2539, reward 868.0, memory_length 2000, epsilon 0.28096926297834607 total_time 724.0\n",
      "episode 2549, reward 1103.0, memory_length 2000, epsilon 0.2795679229330249 total_time 728.0\n",
      "episode 2559, reward 1052.0, memory_length 2000, epsilon 0.27817357210033783 total_time 723.0\n",
      "episode 2569, reward 986.0, memory_length 2000, epsilon 0.27678617562144153 total_time 725.0\n",
      "episode 2579, reward 1070.0, memory_length 2000, epsilon 0.2754056988113517 total_time 722.0\n",
      "episode 2589, reward 1104.0, memory_length 2000, epsilon 0.27403210715807624 total_time 726.0\n",
      "episode 2599, reward 773.0, memory_length 2000, epsilon 0.2726653663217522 total_time 724.0\n",
      "episode 2609, reward 1314.0, memory_length 2000, epsilon 0.27130544213378754 total_time 723.0\n",
      "episode 2619, reward 906.0, memory_length 2000, epsilon 0.2699523005960067 total_time 722.0\n",
      "episode 2629, reward 965.0, memory_length 2000, epsilon 0.26860590787980093 total_time 726.0\n",
      "episode 2639, reward 650.0, memory_length 2000, epsilon 0.26726623032528185 total_time 721.0\n",
      "episode 2649, reward 953.0, memory_length 2000, epsilon 0.2659332344404412 total_time 722.0\n",
      "episode 2659, reward 941.0, memory_length 2000, epsilon 0.2646068869003122 total_time 724.0\n",
      "episode 2669, reward 1071.0, memory_length 2000, epsilon 0.2632871545461373 total_time 721.0\n",
      "episode 2679, reward 1046.0, memory_length 2000, epsilon 0.261974004384539 total_time 722.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2689, reward 1196.0, memory_length 2000, epsilon 0.26066740358669477 total_time 721.0\n",
      "episode 2699, reward 873.0, memory_length 2000, epsilon 0.25936731948751673 total_time 721.0\n",
      "episode 2709, reward 1140.0, memory_length 2000, epsilon 0.2580737195848345 total_time 723.0\n",
      "episode 2719, reward 1126.0, memory_length 2000, epsilon 0.25678657153858325 total_time 724.0\n",
      "episode 2729, reward 956.0, memory_length 2000, epsilon 0.2555058431699948 total_time 722.0\n",
      "episode 2739, reward 1381.0, memory_length 2000, epsilon 0.25423150246079323 total_time 727.0\n",
      "episode 2749, reward 1055.0, memory_length 2000, epsilon 0.2529635175523944 total_time 721.0\n",
      "episode 2759, reward 826.0, memory_length 2000, epsilon 0.25170185674510953 total_time 725.0\n",
      "episode 2769, reward 938.0, memory_length 2000, epsilon 0.25044648849735274 total_time 727.0\n",
      "episode 2779, reward 1406.0, memory_length 2000, epsilon 0.2491973814248526 total_time 724.0\n",
      "episode 2789, reward 1449.0, memory_length 2000, epsilon 0.24795450429986704 total_time 735.0\n",
      "episode 2799, reward 1003.0, memory_length 2000, epsilon 0.24671782605040335 total_time 721.0\n",
      "episode 2809, reward 1035.0, memory_length 2000, epsilon 0.24548731575944074 total_time 733.0\n",
      "episode 2819, reward 965.0, memory_length 2000, epsilon 0.244262942664158 total_time 724.0\n",
      "episode 2829, reward 1193.0, memory_length 2000, epsilon 0.24304467615516384 total_time 725.0\n",
      "episode 2839, reward 1202.0, memory_length 2000, epsilon 0.24183248577573216 total_time 729.0\n",
      "episode 2849, reward 920.0, memory_length 2000, epsilon 0.24062634122104032 total_time 722.0\n",
      "episode 2859, reward 1107.0, memory_length 2000, epsilon 0.2394262123374117 total_time 726.0\n",
      "episode 2869, reward 1219.0, memory_length 2000, epsilon 0.23823206912156156 total_time 722.0\n",
      "episode 2879, reward 1106.0, memory_length 2000, epsilon 0.23704388171984747 total_time 725.0\n",
      "episode 2889, reward 1368.0, memory_length 2000, epsilon 0.23586162042752232 total_time 727.0\n",
      "episode 2899, reward 1287.0, memory_length 2000, epsilon 0.23468525568799242 total_time 722.0\n",
      "episode 2909, reward 878.0, memory_length 2000, epsilon 0.23351475809207786 total_time 723.0\n",
      "episode 2919, reward 1576.0, memory_length 2000, epsilon 0.2323500983772779 total_time 726.0\n",
      "episode 2929, reward 918.0, memory_length 2000, epsilon 0.2311912474270389 total_time 731.0\n",
      "episode 2939, reward 1388.0, memory_length 2000, epsilon 0.23003817627002682 total_time 721.0\n",
      "episode 2949, reward 1123.0, memory_length 2000, epsilon 0.22889085607940265 total_time 727.0\n",
      "episode 2959, reward 1138.0, memory_length 2000, epsilon 0.22774925817210187 total_time 721.0\n",
      "episode 2969, reward 972.0, memory_length 2000, epsilon 0.22661335400811736 total_time 721.0\n",
      "episode 2979, reward 1313.0, memory_length 2000, epsilon 0.2254831151897858 total_time 722.0\n",
      "episode 2989, reward 1300.0, memory_length 2000, epsilon 0.22435851346107796 total_time 722.0\n",
      "episode 2999, reward 1195.0, memory_length 2000, epsilon 0.22323952070689196 total_time 722.0\n",
      "Saving Model 3000\n",
      "episode 3009, reward 1112.0, memory_length 2000, epsilon 0.22212610895235071 total_time 721.0\n",
      "episode 3019, reward 1213.0, memory_length 2000, epsilon 0.22101825036210232 total_time 728.0\n",
      "episode 3029, reward 1134.0, memory_length 2000, epsilon 0.21991591723962442 total_time 725.0\n",
      "episode 3039, reward 1283.0, memory_length 2000, epsilon 0.21881908202653141 total_time 726.0\n",
      "episode 3049, reward 1250.0, memory_length 2000, epsilon 0.21772771730188595 total_time 721.0\n",
      "episode 3059, reward 1064.0, memory_length 2000, epsilon 0.216641795781513 total_time 723.0\n",
      "episode 3069, reward 558.0, memory_length 2000, epsilon 0.21556129031731802 total_time 724.0\n",
      "episode 3079, reward 802.0, memory_length 2000, epsilon 0.21448617389660815 total_time 728.0\n",
      "episode 3089, reward 1224.0, memory_length 2000, epsilon 0.21341641964141686 total_time 724.0\n",
      "episode 3099, reward 1423.0, memory_length 2000, epsilon 0.21235200080783204 total_time 723.0\n",
      "episode 3109, reward 1272.0, memory_length 2000, epsilon 0.21129289078532745 total_time 723.0\n",
      "episode 3119, reward 1130.0, memory_length 2000, epsilon 0.2102390630960973 total_time 722.0\n",
      "episode 3129, reward 1426.0, memory_length 2000, epsilon 0.20919049139439458 total_time 721.0\n",
      "episode 3139, reward 1044.0, memory_length 2000, epsilon 0.20814714946587198 total_time 726.0\n",
      "episode 3149, reward 1200.0, memory_length 2000, epsilon 0.2071090112269271 total_time 722.0\n",
      "episode 3159, reward 1401.0, memory_length 2000, epsilon 0.2060760507240498 total_time 721.0\n",
      "episode 3169, reward 1378.0, memory_length 2000, epsilon 0.20504824213317377 total_time 728.0\n",
      "episode 3179, reward 1099.0, memory_length 2000, epsilon 0.2040255597590306 total_time 728.0\n",
      "episode 3189, reward 1102.0, memory_length 2000, epsilon 0.20300797803450785 total_time 723.0\n",
      "episode 3199, reward 725.0, memory_length 2000, epsilon 0.2019954715200092 total_time 723.0\n",
      "episode 3209, reward 1411.0, memory_length 2000, epsilon 0.20098801490281926 total_time 725.0\n",
      "episode 3219, reward 1631.0, memory_length 2000, epsilon 0.19998558299646998 total_time 721.0\n",
      "episode 3229, reward 1322.0, memory_length 2000, epsilon 0.1989881507401115 total_time 724.0\n",
      "episode 3239, reward 887.0, memory_length 2000, epsilon 0.19799569319788554 total_time 722.0\n",
      "episode 3249, reward 1127.0, memory_length 2000, epsilon 0.1970081855583018 total_time 725.0\n",
      "episode 3259, reward 1098.0, memory_length 2000, epsilon 0.19602560313361786 total_time 723.0\n",
      "episode 3269, reward 1132.0, memory_length 2000, epsilon 0.19504792135922194 total_time 725.0\n",
      "episode 3279, reward 1401.0, memory_length 2000, epsilon 0.19407511579301878 total_time 723.0\n",
      "episode 3289, reward 1102.0, memory_length 2000, epsilon 0.1931071621148185 total_time 726.0\n",
      "episode 3299, reward 1161.0, memory_length 2000, epsilon 0.19214403612572878 total_time 722.0\n",
      "episode 3309, reward 1176.0, memory_length 2000, epsilon 0.19118571374754967 total_time 722.0\n",
      "episode 3319, reward 1044.0, memory_length 2000, epsilon 0.1902321710221719 total_time 728.0\n",
      "episode 3329, reward 1365.0, memory_length 2000, epsilon 0.1892833841109776 total_time 725.0\n",
      "episode 3339, reward 1566.0, memory_length 2000, epsilon 0.1883393292942446 total_time 725.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "score_tracked = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    track_reward = False\n",
    "\n",
    "    # reset at the start of each episode\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
    "    initial_state = env.state_init\n",
    "\n",
    "\n",
    "    total_time = 0  # Total time driver rode in this episode\n",
    "    while not done:\n",
    "        # 1. Get a list of the ride requests driver got.\n",
    "        possible_actions_indices, actions = env.requests(state)\n",
    "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
    "        action = agent.get_action(state, possible_actions_indices, actions)\n",
    "\n",
    "        # 3. Evaluate your reward and next state\n",
    "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
    "        # 4. Total time driver rode in this episode\n",
    "        total_time += step_time\n",
    "        if (total_time > episode_time):\n",
    "            # if ride does not complete in stipu;ated time skip\n",
    "            # it and move to next episode.\n",
    "            done = True\n",
    "        else:\n",
    "            # 5. Append the experience to the memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # 6. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            # 7. Keep a track of rewards, Q-values, loss\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "\n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "\n",
    "    # every 10 episodes:\n",
    "    if ((episode + 1) % 10 == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, total_time))\n",
    "    # Save the Q_value of the state, action pair we are tracking\n",
    "    if ((episode + 1) % 5 == 0):\n",
    "        agent.save_tracking_states()\n",
    "\n",
    "    # Total rewards per episode\n",
    "    score_tracked.append(score)\n",
    "\n",
    "    if(episode % 1000 == 0):\n",
    "        print(\"Saving Model {}\".format(episode))\n",
    "        agent.save(name=\"model_weights.h5\")\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB5JiWZtQ6JZ"
   },
   "outputs": [],
   "source": [
    "agent.save(name=\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J34clOaAQ6JY"
   },
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahZvx4XpQ6JZ"
   },
   "outputs": [],
   "source": [
    "agent.states_tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGYxuCR3Q6JZ"
   },
   "outputs": [],
   "source": [
    "state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6JWcayI0wNq"
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Q_value for state [0,0,0]  action (0,2)')\n",
    "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
    "plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8-rEJ1N08Qa"
   },
   "outputs": [],
   "source": [
    "score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EiYguio09uy"
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Rewards per episode')\n",
    "xaxis = np.asarray(range(0, len(score_tracked_sample)))\n",
    "plt.plot(xaxis,np.asarray(score_tracked_sample))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stD5OunxQ6Jg"
   },
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Qc7xMkRQ6Jg"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tm6xR85Q6Jg"
   },
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg5jfTOwQ6Jh"
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "DQN_Agent_Arch1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
