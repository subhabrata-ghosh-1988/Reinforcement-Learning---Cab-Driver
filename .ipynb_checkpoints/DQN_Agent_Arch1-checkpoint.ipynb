{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8Ij1vxZQ6Ix"
   },
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contributors : Subhabrata Ghosh / Anushaa Vemuri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLA service initialization - CUDA\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mWfJ0Z5QQ6JE"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blTVtc4MQ6JK"
   },
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uGCYPABJQ6JM"
   },
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "573Lxpckb6dO"
   },
   "source": [
    "#### Check what the max, min and mean time values are. This will help us in defining the 'next_step' function in the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3OrekPlbwp4",
    "outputId": "5229ed02-87f0-44d6-9e7c-551e7166007f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "11.0\n",
      "0.0\n",
      "3.0542857142857143\n",
      "7.93705306122449\n"
     ]
    }
   ],
   "source": [
    "print(type(Time_matrix))\n",
    "print(Time_matrix.max())\n",
    "print(Time_matrix.min())\n",
    "print(Time_matrix.mean())\n",
    "print(Time_matrix.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cBe6IZqcDbF"
   },
   "source": [
    "#### Since the max time is 11 hours between any 2 points, the next state of the cab driver may increase at most by 1 day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJvDTRo3Q6JN"
   },
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SQnwpFsrp3Tv"
   },
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUeWmqVJQ6JS"
   },
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BCagoCqNQ6JU"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay = -0.0005 #for 15k\n",
    "        #self.epsilon_decay = -0.00015 #for 20k\n",
    "        self.epsilon_min = 0.00001\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # Initialize the value of the states tracked\n",
    "        self.states_tracked = []\n",
    "        \n",
    "        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n",
    "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Function that takes in the agent and constructs the network\n",
    "        to train it\n",
    "        @return model\n",
    "        @params agent\n",
    "        \"\"\"\n",
    "        input_shape = self.state_size\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, possible_actions_index, actions):\n",
    "        \"\"\"\n",
    "        get action in a state according to an epsilon-greedy approach\n",
    "        possible_actions_index, actions are the 'ride requests' that teh driver got.\n",
    "        \"\"\"        \n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after each episode       \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from the ride requests\n",
    "            return random.choice(possible_actions_index)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "\n",
    "            # Use the model to predict the Q_values.\n",
    "            q_value = self.model.predict(state)\n",
    "\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "\n",
    "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, done):\n",
    "        \"\"\"appends the new agent run output to replay buffer\"\"\"\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "        \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \"\"\" \n",
    "        Function to train the model on eacg step run.\n",
    "        Picks the random memory events according to batch size and \n",
    "        runs it through the network to train it.\n",
    "        \"\"\"\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch1(state)     \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "            # model fit\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "    def save_tracking_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_tracked.append(q_value[0][2])\n",
    "        \n",
    "    def save_test_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_test.append(q_value[0][2])\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSRJU7n8Q6JW"
   },
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WTQx9gYeNTQ"
   },
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mKOKQKCceIHB"
   },
   "outputs": [],
   "source": [
    "episode_time = 24*30 #30 days before which car has to be recharged\n",
    "n_episodes = 15000\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "\n",
    "# Invoke Env class\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()\n",
    "\n",
    "# Set up state and action sizes.\n",
    "state_size = m+t+d\n",
    "action_size = len(action_space)\n",
    "\n",
    "# Invoke agent class\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "# Rewards for state [0,0,0] being tracked.\n",
    "rewards_init_state = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-LjiJlpeiJU"
   },
   "source": [
    "#### Run the episodes, build up replay buffer and train the model.\n",
    "Note:\n",
    "The moment total episode time exceeds 720 (30 days), we ignore the most recent ride and do NOT save that experience in the replay memory\n",
    "The init state is randomly picked from the state space for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUED4EPpQ6JX",
    "outputId": "fec1562e-6dea-4243-9fd7-9e06335b2920",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model 0\n",
      "episode 9, reward -175.0, memory_length 1369, epsilon 0.9955001547284723 total_time 734.0\n",
      "episode 19, reward 191.0, memory_length 2000, epsilon 0.9905350769930761 total_time 727.0\n",
      "episode 29, reward -216.0, memory_length 2000, epsilon 0.9855947626861951 total_time 722.0\n",
      "episode 39, reward -243.0, memory_length 2000, epsilon 0.9806790882997144 total_time 721.0\n",
      "episode 49, reward -198.0, memory_length 2000, epsilon 0.9757879309415182 total_time 726.0\n",
      "episode 59, reward -292.0, memory_length 2000, epsilon 0.9709211683324178 total_time 724.0\n",
      "episode 69, reward 99.0, memory_length 2000, epsilon 0.9660786788030947 total_time 729.0\n",
      "episode 79, reward -165.0, memory_length 2000, epsilon 0.9612603412910584 total_time 724.0\n",
      "episode 89, reward -337.0, memory_length 2000, epsilon 0.9564660353376199 total_time 725.0\n",
      "episode 99, reward -63.0, memory_length 2000, epsilon 0.9516956410848808 total_time 732.0\n",
      "episode 109, reward -206.0, memory_length 2000, epsilon 0.9469490392727365 total_time 725.0\n",
      "episode 119, reward -402.0, memory_length 2000, epsilon 0.9422261112358942 total_time 722.0\n",
      "episode 129, reward 185.0, memory_length 2000, epsilon 0.9375267389009072 total_time 728.0\n",
      "episode 139, reward -328.0, memory_length 2000, epsilon 0.9328508047832221 total_time 722.0\n",
      "episode 149, reward -96.0, memory_length 2000, epsilon 0.9281981919842428 total_time 722.0\n",
      "episode 159, reward -126.0, memory_length 2000, epsilon 0.9235687841884068 total_time 726.0\n",
      "episode 169, reward -171.0, memory_length 2000, epsilon 0.918962465660278 total_time 721.0\n",
      "episode 179, reward 80.0, memory_length 2000, epsilon 0.9143791212416534 total_time 724.0\n",
      "episode 189, reward 182.0, memory_length 2000, epsilon 0.9098186363486838 total_time 724.0\n",
      "episode 199, reward -151.0, memory_length 2000, epsilon 0.9052808969690094 total_time 726.0\n",
      "episode 209, reward -75.0, memory_length 2000, epsilon 0.9007657896589091 total_time 727.0\n",
      "episode 219, reward -187.0, memory_length 2000, epsilon 0.8962732015404654 total_time 726.0\n",
      "episode 229, reward 162.0, memory_length 2000, epsilon 0.891803020298741 total_time 730.0\n",
      "episode 239, reward -332.0, memory_length 2000, epsilon 0.8873551341789723 total_time 722.0\n",
      "episode 249, reward 46.0, memory_length 2000, epsilon 0.8829294319837746 total_time 722.0\n",
      "episode 259, reward -119.0, memory_length 2000, epsilon 0.8785258030703623 total_time 726.0\n",
      "episode 269, reward 249.0, memory_length 2000, epsilon 0.8741441373477834 total_time 725.0\n",
      "episode 279, reward 123.0, memory_length 2000, epsilon 0.8697843252741666 total_time 721.0\n",
      "episode 289, reward -237.0, memory_length 2000, epsilon 0.8654462578539829 total_time 721.0\n",
      "episode 299, reward 158.0, memory_length 2000, epsilon 0.8611298266353209 total_time 725.0\n",
      "episode 309, reward 176.0, memory_length 2000, epsilon 0.8568349237071754 total_time 726.0\n",
      "episode 319, reward 115.0, memory_length 2000, epsilon 0.8525614416967494 total_time 726.0\n",
      "episode 329, reward -67.0, memory_length 2000, epsilon 0.84830927376677 total_time 724.0\n",
      "episode 339, reward -54.0, memory_length 2000, epsilon 0.8440783136128177 total_time 724.0\n",
      "episode 349, reward -312.0, memory_length 2000, epsilon 0.8398684554606681 total_time 721.0\n",
      "episode 359, reward -117.0, memory_length 2000, epsilon 0.8356795940636483 total_time 731.0\n",
      "episode 369, reward 375.0, memory_length 2000, epsilon 0.8315116247000052 total_time 723.0\n",
      "episode 379, reward 117.0, memory_length 2000, epsilon 0.8273644431702872 total_time 736.0\n",
      "episode 389, reward 203.0, memory_length 2000, epsilon 0.8232379457947406 total_time 729.0\n",
      "episode 399, reward 286.0, memory_length 2000, epsilon 0.819132029410716 total_time 726.0\n",
      "episode 409, reward -112.0, memory_length 2000, epsilon 0.8150465913700896 total_time 726.0\n",
      "episode 419, reward -72.0, memory_length 2000, epsilon 0.8109815295366979 total_time 732.0\n",
      "episode 429, reward 36.0, memory_length 2000, epsilon 0.8069367422837833 total_time 721.0\n",
      "episode 439, reward 347.0, memory_length 2000, epsilon 0.8029121284914538 total_time 725.0\n",
      "episode 449, reward 26.0, memory_length 2000, epsilon 0.7989075875441549 total_time 723.0\n",
      "episode 459, reward 203.0, memory_length 2000, epsilon 0.7949230193281545 total_time 730.0\n",
      "episode 469, reward 117.0, memory_length 2000, epsilon 0.7909583242290396 total_time 724.0\n",
      "episode 479, reward 198.0, memory_length 2000, epsilon 0.7870134031292261 total_time 721.0\n",
      "episode 489, reward 440.0, memory_length 2000, epsilon 0.7830881574054811 total_time 725.0\n",
      "episode 499, reward 135.0, memory_length 2000, epsilon 0.7791824889264571 total_time 730.0\n",
      "episode 509, reward 74.0, memory_length 2000, epsilon 0.7752963000502389 total_time 721.0\n",
      "episode 519, reward 562.0, memory_length 2000, epsilon 0.7714294936219019 total_time 721.0\n",
      "episode 529, reward -148.0, memory_length 2000, epsilon 0.7675819729710842 total_time 726.0\n",
      "episode 539, reward 391.0, memory_length 2000, epsilon 0.763753641909569 total_time 722.0\n",
      "episode 549, reward 100.0, memory_length 2000, epsilon 0.7599444047288803 total_time 724.0\n",
      "episode 559, reward 280.0, memory_length 2000, epsilon 0.7561541661978903 total_time 724.0\n",
      "episode 569, reward 396.0, memory_length 2000, epsilon 0.7523828315604384 total_time 730.0\n",
      "episode 579, reward 145.0, memory_length 2000, epsilon 0.7486303065329623 total_time 725.0\n",
      "episode 589, reward 492.0, memory_length 2000, epsilon 0.7448964973021404 total_time 726.0\n",
      "episode 599, reward -93.0, memory_length 2000, epsilon 0.7411813105225479 total_time 722.0\n",
      "episode 609, reward 231.0, memory_length 2000, epsilon 0.7374846533143217 total_time 722.0\n",
      "episode 619, reward 168.0, memory_length 2000, epsilon 0.733806433260839 total_time 723.0\n",
      "episode 629, reward 306.0, memory_length 2000, epsilon 0.7301465584064071 total_time 723.0\n",
      "episode 639, reward 297.0, memory_length 2000, epsilon 0.7265049372539636 total_time 726.0\n",
      "episode 649, reward -21.0, memory_length 2000, epsilon 0.7228814787627905 total_time 721.0\n",
      "episode 659, reward -234.0, memory_length 2000, epsilon 0.7192760923462365 total_time 728.0\n",
      "episode 669, reward 236.0, memory_length 2000, epsilon 0.7156886878694535 total_time 723.0\n",
      "episode 679, reward 193.0, memory_length 2000, epsilon 0.7121191756471427 total_time 727.0\n",
      "episode 689, reward 406.0, memory_length 2000, epsilon 0.7085674664413126 total_time 723.0\n",
      "episode 699, reward 111.0, memory_length 2000, epsilon 0.7050334714590482 total_time 721.0\n",
      "episode 709, reward 354.0, memory_length 2000, epsilon 0.7015171023502909 total_time 725.0\n",
      "episode 719, reward 239.0, memory_length 2000, epsilon 0.6980182712056295 total_time 722.0\n",
      "episode 729, reward 622.0, memory_length 2000, epsilon 0.6945368905541035 total_time 724.0\n",
      "episode 739, reward 455.0, memory_length 2000, epsilon 0.6910728733610152 total_time 726.0\n",
      "episode 749, reward 555.0, memory_length 2000, epsilon 0.6876261330257543 total_time 724.0\n",
      "episode 759, reward 432.0, memory_length 2000, epsilon 0.684196583379633 total_time 721.0\n",
      "episode 769, reward 484.0, memory_length 2000, epsilon 0.6807841386837313 total_time 721.0\n",
      "episode 779, reward -97.0, memory_length 2000, epsilon 0.6773887136267543 total_time 722.0\n",
      "episode 789, reward -3.0, memory_length 2000, epsilon 0.6740102233228988 total_time 721.0\n",
      "episode 799, reward 78.0, memory_length 2000, epsilon 0.670648583309731 total_time 721.0\n",
      "episode 809, reward 189.0, memory_length 2000, epsilon 0.6673037095460755 total_time 727.0\n",
      "episode 819, reward 434.0, memory_length 2000, epsilon 0.6639755184099142 total_time 724.0\n",
      "episode 829, reward 438.0, memory_length 2000, epsilon 0.6606639266962953 total_time 724.0\n",
      "episode 839, reward 320.0, memory_length 2000, epsilon 0.6573688516152534 total_time 732.0\n",
      "episode 849, reward 391.0, memory_length 2000, epsilon 0.6540902107897397 total_time 724.0\n",
      "episode 859, reward 304.0, memory_length 2000, epsilon 0.6508279222535631 total_time 725.0\n",
      "episode 869, reward 363.0, memory_length 2000, epsilon 0.64758190444934 total_time 729.0\n",
      "episode 879, reward 172.0, memory_length 2000, epsilon 0.6443520762264566 total_time 725.0\n",
      "episode 889, reward 270.0, memory_length 2000, epsilon 0.6411383568390387 total_time 726.0\n",
      "episode 899, reward 585.0, memory_length 2000, epsilon 0.6379406659439346 total_time 722.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 909, reward 216.0, memory_length 2000, epsilon 0.6347589235987051 total_time 725.0\n",
      "episode 919, reward 314.0, memory_length 2000, epsilon 0.631593050259626 total_time 721.0\n",
      "episode 929, reward 170.0, memory_length 2000, epsilon 0.6284429667796988 total_time 721.0\n",
      "episode 939, reward 374.0, memory_length 2000, epsilon 0.6253085944066726 total_time 727.0\n",
      "episode 949, reward 701.0, memory_length 2000, epsilon 0.6221898547810748 total_time 722.0\n",
      "episode 959, reward 646.0, memory_length 2000, epsilon 0.6190866699342522 total_time 726.0\n",
      "episode 969, reward 541.0, memory_length 2000, epsilon 0.6159989622864221 total_time 722.0\n",
      "episode 979, reward 321.0, memory_length 2000, epsilon 0.6129266546447325 total_time 726.0\n",
      "episode 989, reward 335.0, memory_length 2000, epsilon 0.6098696702013323 total_time 722.0\n",
      "episode 999, reward 474.0, memory_length 2000, epsilon 0.6068279325314512 total_time 726.0\n",
      "Saving Model 1000\n",
      "episode 1009, reward 339.0, memory_length 2000, epsilon 0.6038013655914889 total_time 727.0\n",
      "episode 1019, reward 614.0, memory_length 2000, epsilon 0.6007898937171146 total_time 733.0\n",
      "episode 1029, reward 682.0, memory_length 2000, epsilon 0.5977934416213744 total_time 721.0\n",
      "episode 1039, reward 388.0, memory_length 2000, epsilon 0.5948119343928097 total_time 726.0\n",
      "episode 1049, reward 775.0, memory_length 2000, epsilon 0.5918452974935846 total_time 723.0\n",
      "episode 1059, reward 280.0, memory_length 2000, epsilon 0.5888934567576223 total_time 721.0\n",
      "episode 1069, reward 477.0, memory_length 2000, epsilon 0.5859563383887504 total_time 736.0\n",
      "episode 1079, reward 90.0, memory_length 2000, epsilon 0.5830338689588568 total_time 734.0\n",
      "episode 1089, reward 421.0, memory_length 2000, epsilon 0.5801259754060536 total_time 723.0\n",
      "episode 1099, reward 722.0, memory_length 2000, epsilon 0.5772325850328504 total_time 729.0\n",
      "episode 1109, reward 481.0, memory_length 2000, epsilon 0.5743536255043372 total_time 723.0\n",
      "episode 1119, reward 334.0, memory_length 2000, epsilon 0.5714890248463761 total_time 727.0\n",
      "episode 1129, reward 417.0, memory_length 2000, epsilon 0.5686387114438011 total_time 725.0\n",
      "episode 1139, reward 485.0, memory_length 2000, epsilon 0.5658026140386287 total_time 721.0\n",
      "episode 1149, reward 585.0, memory_length 2000, epsilon 0.5629806617282764 total_time 730.0\n",
      "episode 1159, reward 675.0, memory_length 2000, epsilon 0.5601727839637891 total_time 731.0\n",
      "episode 1169, reward 625.0, memory_length 2000, epsilon 0.5573789105480766 total_time 728.0\n",
      "episode 1179, reward 806.0, memory_length 2000, epsilon 0.5545989716341581 total_time 723.0\n",
      "episode 1189, reward 720.0, memory_length 2000, epsilon 0.5518328977234156 total_time 731.0\n",
      "episode 1199, reward 821.0, memory_length 2000, epsilon 0.5490806196638577 total_time 727.0\n",
      "episode 1209, reward 925.0, memory_length 2000, epsilon 0.5463420686483893 total_time 725.0\n",
      "episode 1219, reward 469.0, memory_length 2000, epsilon 0.5436171762130925 total_time 731.0\n",
      "episode 1229, reward 860.0, memory_length 2000, epsilon 0.5409058742355145 total_time 725.0\n",
      "episode 1239, reward 471.0, memory_length 2000, epsilon 0.5382080949329644 total_time 725.0\n",
      "episode 1249, reward 743.0, memory_length 2000, epsilon 0.5355237708608195 total_time 722.0\n",
      "episode 1259, reward 337.0, memory_length 2000, epsilon 0.5328528349108379 total_time 721.0\n",
      "episode 1269, reward 568.0, memory_length 2000, epsilon 0.5301952203094819 total_time 721.0\n",
      "episode 1279, reward 414.0, memory_length 2000, epsilon 0.5275508606162479 total_time 721.0\n",
      "episode 1289, reward 598.0, memory_length 2000, epsilon 0.5249196897220061 total_time 723.0\n",
      "episode 1299, reward 799.0, memory_length 2000, epsilon 0.5223016418473468 total_time 724.0\n",
      "episode 1309, reward 417.0, memory_length 2000, epsilon 0.519696651540937 total_time 726.0\n",
      "episode 1319, reward 690.0, memory_length 2000, epsilon 0.5171046536778833 total_time 728.0\n",
      "episode 1329, reward 556.0, memory_length 2000, epsilon 0.514525583458104 total_time 721.0\n",
      "episode 1339, reward 641.0, memory_length 2000, epsilon 0.5119593764047093 total_time 722.0\n",
      "episode 1349, reward 662.0, memory_length 2000, epsilon 0.5094059683623896 total_time 723.0\n",
      "episode 1359, reward 847.0, memory_length 2000, epsilon 0.5068652954958104 total_time 737.0\n",
      "episode 1369, reward 909.0, memory_length 2000, epsilon 0.5043372942880178 total_time 721.0\n",
      "episode 1379, reward 891.0, memory_length 2000, epsilon 0.50182190153885 total_time 725.0\n",
      "episode 1389, reward 762.0, memory_length 2000, epsilon 0.49931905436335716 total_time 726.0\n",
      "episode 1399, reward 717.0, memory_length 2000, epsilon 0.49682869019022974 total_time 722.0\n",
      "episode 1409, reward 645.0, memory_length 2000, epsilon 0.49435074676023355 total_time 726.0\n",
      "episode 1419, reward 610.0, memory_length 2000, epsilon 0.4918851621246539 total_time 725.0\n",
      "episode 1429, reward 977.0, memory_length 2000, epsilon 0.4894318746437464 total_time 722.0\n",
      "episode 1439, reward 806.0, memory_length 2000, epsilon 0.4869908229851962 total_time 727.0\n",
      "episode 1449, reward 584.0, memory_length 2000, epsilon 0.48456194612258474 total_time 721.0\n",
      "episode 1459, reward 1028.0, memory_length 2000, epsilon 0.48214518333386397 total_time 721.0\n",
      "episode 1469, reward 757.0, memory_length 2000, epsilon 0.47974047419983834 total_time 724.0\n",
      "episode 1479, reward 626.0, memory_length 2000, epsilon 0.4773477586026542 total_time 732.0\n",
      "episode 1489, reward 774.0, memory_length 2000, epsilon 0.474966976724297 total_time 724.0\n",
      "episode 1499, reward 729.0, memory_length 2000, epsilon 0.47259806904509577 total_time 727.0\n",
      "episode 1509, reward 869.0, memory_length 2000, epsilon 0.4702409763422352 total_time 724.0\n",
      "episode 1519, reward 903.0, memory_length 2000, epsilon 0.4678956396882749 total_time 721.0\n",
      "episode 1529, reward 671.0, memory_length 2000, epsilon 0.4655620004496764 total_time 734.0\n",
      "episode 1539, reward 855.0, memory_length 2000, epsilon 0.46324000028533724 total_time 729.0\n",
      "episode 1549, reward 923.0, memory_length 2000, epsilon 0.4609295811451323 total_time 727.0\n",
      "episode 1559, reward 1008.0, memory_length 2000, epsilon 0.4586306852684627 total_time 727.0\n",
      "episode 1569, reward 825.0, memory_length 2000, epsilon 0.4563432551828119 total_time 721.0\n",
      "episode 1579, reward 712.0, memory_length 2000, epsilon 0.4540672337023085 total_time 722.0\n",
      "episode 1589, reward 737.0, memory_length 2000, epsilon 0.45180256392629703 total_time 721.0\n",
      "episode 1599, reward 1029.0, memory_length 2000, epsilon 0.4495491892379152 total_time 727.0\n",
      "episode 1609, reward 973.0, memory_length 2000, epsilon 0.4473070533026783 total_time 729.0\n",
      "episode 1619, reward 1022.0, memory_length 2000, epsilon 0.4450761000670712 total_time 732.0\n",
      "episode 1629, reward 1014.0, memory_length 2000, epsilon 0.4428562737571469 total_time 721.0\n",
      "episode 1639, reward 526.0, memory_length 2000, epsilon 0.44064751887713194 total_time 723.0\n",
      "episode 1649, reward 631.0, memory_length 2000, epsilon 0.4384497802080393 total_time 729.0\n",
      "episode 1659, reward 446.0, memory_length 2000, epsilon 0.4362630028062879 total_time 725.0\n",
      "episode 1669, reward 1128.0, memory_length 2000, epsilon 0.43408713200232857 total_time 725.0\n",
      "episode 1679, reward 656.0, memory_length 2000, epsilon 0.43192211339927816 total_time 721.0\n",
      "episode 1689, reward 896.0, memory_length 2000, epsilon 0.4297678928715586 total_time 724.0\n",
      "episode 1699, reward 468.0, memory_length 2000, epsilon 0.4276244165635446 total_time 727.0\n",
      "episode 1709, reward 919.0, memory_length 2000, epsilon 0.42549163088821684 total_time 726.0\n",
      "episode 1719, reward 839.0, memory_length 2000, epsilon 0.4233694825258223 total_time 722.0\n",
      "episode 1729, reward 999.0, memory_length 2000, epsilon 0.4212579184225415 total_time 730.0\n",
      "episode 1739, reward 534.0, memory_length 2000, epsilon 0.4191568857891617 total_time 723.0\n",
      "episode 1749, reward 717.0, memory_length 2000, epsilon 0.4170663320997578 total_time 726.0\n",
      "episode 1759, reward 894.0, memory_length 2000, epsilon 0.4149862050903786 total_time 721.0\n",
      "episode 1769, reward 748.0, memory_length 2000, epsilon 0.4129164527577405 total_time 728.0\n",
      "episode 1779, reward 1105.0, memory_length 2000, epsilon 0.41085702335792745 total_time 730.0\n",
      "episode 1789, reward 950.0, memory_length 2000, epsilon 0.40880786540509717 total_time 721.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1799, reward 867.0, memory_length 2000, epsilon 0.4067689276701942 total_time 729.0\n",
      "episode 1809, reward 941.0, memory_length 2000, epsilon 0.40474015917966877 total_time 726.0\n",
      "episode 1819, reward 946.0, memory_length 2000, epsilon 0.4027215092142031 total_time 731.0\n",
      "episode 1829, reward 1086.0, memory_length 2000, epsilon 0.4007129273074429 total_time 729.0\n",
      "episode 1839, reward 1081.0, memory_length 2000, epsilon 0.39871436324473586 total_time 721.0\n",
      "episode 1849, reward 911.0, memory_length 2000, epsilon 0.3967257670618763 total_time 721.0\n",
      "episode 1859, reward 847.0, memory_length 2000, epsilon 0.3947470890438561 total_time 732.0\n",
      "episode 1869, reward 419.0, memory_length 2000, epsilon 0.3927782797236218 total_time 722.0\n",
      "episode 1879, reward 956.0, memory_length 2000, epsilon 0.3908192898808378 total_time 721.0\n",
      "episode 1889, reward 707.0, memory_length 2000, epsilon 0.388870070540656 total_time 726.0\n",
      "episode 1899, reward 971.0, memory_length 2000, epsilon 0.38693057297249134 total_time 724.0\n",
      "episode 1909, reward 907.0, memory_length 2000, epsilon 0.3850007486888037 total_time 725.0\n",
      "episode 1919, reward 735.0, memory_length 2000, epsilon 0.3830805494438854 total_time 726.0\n",
      "episode 1929, reward 740.0, memory_length 2000, epsilon 0.38116992723265536 total_time 723.0\n",
      "episode 1939, reward 1161.0, memory_length 2000, epsilon 0.3792688342894587 total_time 722.0\n",
      "episode 1949, reward 340.0, memory_length 2000, epsilon 0.3773772230868729 total_time 721.0\n",
      "episode 1959, reward 902.0, memory_length 2000, epsilon 0.37549504633451936 total_time 722.0\n",
      "episode 1969, reward 857.0, memory_length 2000, epsilon 0.37362225697788115 total_time 722.0\n",
      "episode 1979, reward 902.0, memory_length 2000, epsilon 0.37175880819712703 total_time 721.0\n",
      "episode 1989, reward 975.0, memory_length 2000, epsilon 0.3699046534059402 total_time 723.0\n",
      "episode 1999, reward 656.0, memory_length 2000, epsilon 0.3680597462503545 total_time 721.0\n",
      "Saving Model 2000\n",
      "episode 2009, reward 862.0, memory_length 2000, epsilon 0.3662240406075948 total_time 721.0\n",
      "episode 2019, reward 925.0, memory_length 2000, epsilon 0.3643974905849244 total_time 725.0\n",
      "episode 2029, reward 1225.0, memory_length 2000, epsilon 0.3625800505184978 total_time 726.0\n",
      "episode 2039, reward 487.0, memory_length 2000, epsilon 0.3607716749722184 total_time 726.0\n",
      "episode 2049, reward 825.0, memory_length 2000, epsilon 0.3589723187366037 total_time 723.0\n",
      "episode 2059, reward 752.0, memory_length 2000, epsilon 0.35718193682765376 total_time 724.0\n",
      "episode 2069, reward 904.0, memory_length 2000, epsilon 0.3554004844857278 total_time 723.0\n",
      "episode 2079, reward 833.0, memory_length 2000, epsilon 0.35362791717442443 total_time 721.0\n",
      "episode 2089, reward 873.0, memory_length 2000, epsilon 0.35186419057946866 total_time 723.0\n",
      "episode 2099, reward 776.0, memory_length 2000, epsilon 0.3501092606076035 total_time 725.0\n",
      "episode 2109, reward 984.0, memory_length 2000, epsilon 0.3483630833854885 total_time 721.0\n",
      "episode 2119, reward 995.0, memory_length 2000, epsilon 0.34662561525860197 total_time 721.0\n",
      "episode 2129, reward 969.0, memory_length 2000, epsilon 0.34489681279015044 total_time 722.0\n",
      "episode 2139, reward 828.0, memory_length 2000, epsilon 0.34317663275998195 total_time 725.0\n",
      "episode 2149, reward 617.0, memory_length 2000, epsilon 0.3414650321635064 total_time 731.0\n",
      "episode 2159, reward 1112.0, memory_length 2000, epsilon 0.33976196821061944 total_time 724.0\n",
      "episode 2169, reward 955.0, memory_length 2000, epsilon 0.3380673983246338 total_time 730.0\n",
      "episode 2179, reward 966.0, memory_length 2000, epsilon 0.336381280141214 total_time 726.0\n",
      "episode 2189, reward 1130.0, memory_length 2000, epsilon 0.33470357150731744 total_time 727.0\n",
      "episode 2199, reward 786.0, memory_length 2000, epsilon 0.3330342304801412 total_time 723.0\n",
      "episode 2209, reward 846.0, memory_length 2000, epsilon 0.33137321532607245 total_time 722.0\n",
      "episode 2219, reward 979.0, memory_length 2000, epsilon 0.3297204845196459 total_time 725.0\n",
      "episode 2229, reward 1014.0, memory_length 2000, epsilon 0.32807599674250526 total_time 726.0\n",
      "episode 2239, reward 828.0, memory_length 2000, epsilon 0.32643971088237056 total_time 726.0\n",
      "episode 2249, reward 1038.0, memory_length 2000, epsilon 0.32481158603200994 total_time 725.0\n",
      "episode 2259, reward 1020.0, memory_length 2000, epsilon 0.32319158148821747 total_time 728.0\n",
      "episode 2269, reward 1043.0, memory_length 2000, epsilon 0.3215796567507951 total_time 722.0\n",
      "episode 2279, reward 910.0, memory_length 2000, epsilon 0.31997577152154044 total_time 721.0\n",
      "episode 2289, reward 1162.0, memory_length 2000, epsilon 0.31837988570323916 total_time 733.0\n",
      "episode 2299, reward 1102.0, memory_length 2000, epsilon 0.3167919593986629 total_time 728.0\n",
      "episode 2309, reward 1333.0, memory_length 2000, epsilon 0.3152119529095711 total_time 728.0\n",
      "episode 2319, reward 1027.0, memory_length 2000, epsilon 0.3136398267357194 total_time 725.0\n",
      "episode 2329, reward 935.0, memory_length 2000, epsilon 0.31207554157387146 total_time 725.0\n",
      "episode 2339, reward 1078.0, memory_length 2000, epsilon 0.3105190583168168 total_time 724.0\n",
      "episode 2349, reward 1184.0, memory_length 2000, epsilon 0.30897033805239293 total_time 723.0\n",
      "episode 2359, reward 771.0, memory_length 2000, epsilon 0.3074293420625127 total_time 723.0\n",
      "episode 2369, reward 976.0, memory_length 2000, epsilon 0.3058960318221959 total_time 722.0\n",
      "episode 2379, reward 1340.0, memory_length 2000, epsilon 0.30437036899860687 total_time 723.0\n",
      "episode 2389, reward 1144.0, memory_length 2000, epsilon 0.3028523154500953 total_time 727.0\n",
      "episode 2399, reward 1119.0, memory_length 2000, epsilon 0.30134183322524366 total_time 725.0\n",
      "episode 2409, reward 627.0, memory_length 2000, epsilon 0.29983888456191743 total_time 723.0\n",
      "episode 2419, reward 1158.0, memory_length 2000, epsilon 0.29834343188632195 total_time 721.0\n",
      "episode 2429, reward 981.0, memory_length 2000, epsilon 0.2968554378120623 total_time 723.0\n",
      "episode 2439, reward 937.0, memory_length 2000, epsilon 0.2953748651392093 total_time 728.0\n",
      "episode 2449, reward 968.0, memory_length 2000, epsilon 0.2939016768533689 total_time 725.0\n",
      "episode 2459, reward 1067.0, memory_length 2000, epsilon 0.2924358361247571 total_time 722.0\n",
      "episode 2469, reward 943.0, memory_length 2000, epsilon 0.2909773063072796 total_time 722.0\n",
      "episode 2479, reward 1105.0, memory_length 2000, epsilon 0.28952605093761474 total_time 722.0\n",
      "episode 2489, reward 1233.0, memory_length 2000, epsilon 0.28808203373430286 total_time 721.0\n",
      "episode 2499, reward 836.0, memory_length 2000, epsilon 0.28664521859683867 total_time 721.0\n",
      "episode 2509, reward 749.0, memory_length 2000, epsilon 0.2852155696047688 total_time 722.0\n",
      "episode 2519, reward 851.0, memory_length 2000, epsilon 0.28379305101679403 total_time 722.0\n",
      "episode 2529, reward 885.0, memory_length 2000, epsilon 0.28237762726987564 total_time 723.0\n",
      "episode 2539, reward 868.0, memory_length 2000, epsilon 0.28096926297834607 total_time 724.0\n",
      "episode 2549, reward 1103.0, memory_length 2000, epsilon 0.2795679229330249 total_time 728.0\n",
      "episode 2559, reward 1052.0, memory_length 2000, epsilon 0.27817357210033783 total_time 723.0\n",
      "episode 2569, reward 986.0, memory_length 2000, epsilon 0.27678617562144153 total_time 725.0\n",
      "episode 2579, reward 1070.0, memory_length 2000, epsilon 0.2754056988113517 total_time 722.0\n",
      "episode 2589, reward 1104.0, memory_length 2000, epsilon 0.27403210715807624 total_time 726.0\n",
      "episode 2599, reward 773.0, memory_length 2000, epsilon 0.2726653663217522 total_time 724.0\n",
      "episode 2609, reward 1314.0, memory_length 2000, epsilon 0.27130544213378754 total_time 723.0\n",
      "episode 2619, reward 906.0, memory_length 2000, epsilon 0.2699523005960067 total_time 722.0\n",
      "episode 2629, reward 965.0, memory_length 2000, epsilon 0.26860590787980093 total_time 726.0\n",
      "episode 2639, reward 650.0, memory_length 2000, epsilon 0.26726623032528185 total_time 721.0\n",
      "episode 2649, reward 953.0, memory_length 2000, epsilon 0.2659332344404412 total_time 722.0\n",
      "episode 2659, reward 941.0, memory_length 2000, epsilon 0.2646068869003122 total_time 724.0\n",
      "episode 2669, reward 1071.0, memory_length 2000, epsilon 0.2632871545461373 total_time 721.0\n",
      "episode 2679, reward 1046.0, memory_length 2000, epsilon 0.261974004384539 total_time 722.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2689, reward 1196.0, memory_length 2000, epsilon 0.26066740358669477 total_time 721.0\n",
      "episode 2699, reward 873.0, memory_length 2000, epsilon 0.25936731948751673 total_time 721.0\n",
      "episode 2709, reward 1140.0, memory_length 2000, epsilon 0.2580737195848345 total_time 723.0\n",
      "episode 2719, reward 1126.0, memory_length 2000, epsilon 0.25678657153858325 total_time 724.0\n",
      "episode 2729, reward 956.0, memory_length 2000, epsilon 0.2555058431699948 total_time 722.0\n",
      "episode 2739, reward 1381.0, memory_length 2000, epsilon 0.25423150246079323 total_time 727.0\n",
      "episode 2749, reward 1055.0, memory_length 2000, epsilon 0.2529635175523944 total_time 721.0\n",
      "episode 2759, reward 826.0, memory_length 2000, epsilon 0.25170185674510953 total_time 725.0\n",
      "episode 2769, reward 938.0, memory_length 2000, epsilon 0.25044648849735274 total_time 727.0\n",
      "episode 2779, reward 1406.0, memory_length 2000, epsilon 0.2491973814248526 total_time 724.0\n",
      "episode 2789, reward 1449.0, memory_length 2000, epsilon 0.24795450429986704 total_time 735.0\n",
      "episode 2799, reward 1003.0, memory_length 2000, epsilon 0.24671782605040335 total_time 721.0\n",
      "episode 2809, reward 1035.0, memory_length 2000, epsilon 0.24548731575944074 total_time 733.0\n",
      "episode 2819, reward 965.0, memory_length 2000, epsilon 0.244262942664158 total_time 724.0\n",
      "episode 2829, reward 1193.0, memory_length 2000, epsilon 0.24304467615516384 total_time 725.0\n",
      "episode 2839, reward 1202.0, memory_length 2000, epsilon 0.24183248577573216 total_time 729.0\n",
      "episode 2849, reward 920.0, memory_length 2000, epsilon 0.24062634122104032 total_time 722.0\n",
      "episode 2859, reward 1107.0, memory_length 2000, epsilon 0.2394262123374117 total_time 726.0\n",
      "episode 2869, reward 1219.0, memory_length 2000, epsilon 0.23823206912156156 total_time 722.0\n",
      "episode 2879, reward 1106.0, memory_length 2000, epsilon 0.23704388171984747 total_time 725.0\n",
      "episode 2889, reward 1368.0, memory_length 2000, epsilon 0.23586162042752232 total_time 727.0\n",
      "episode 2899, reward 1287.0, memory_length 2000, epsilon 0.23468525568799242 total_time 722.0\n",
      "episode 2909, reward 878.0, memory_length 2000, epsilon 0.23351475809207786 total_time 723.0\n",
      "episode 2919, reward 1576.0, memory_length 2000, epsilon 0.2323500983772779 total_time 726.0\n",
      "episode 2929, reward 918.0, memory_length 2000, epsilon 0.2311912474270389 total_time 731.0\n",
      "episode 2939, reward 1388.0, memory_length 2000, epsilon 0.23003817627002682 total_time 721.0\n",
      "episode 2949, reward 1123.0, memory_length 2000, epsilon 0.22889085607940265 total_time 727.0\n",
      "episode 2959, reward 1138.0, memory_length 2000, epsilon 0.22774925817210187 total_time 721.0\n",
      "episode 2969, reward 972.0, memory_length 2000, epsilon 0.22661335400811736 total_time 721.0\n",
      "episode 2979, reward 1313.0, memory_length 2000, epsilon 0.2254831151897858 total_time 722.0\n",
      "episode 2989, reward 1300.0, memory_length 2000, epsilon 0.22435851346107796 total_time 722.0\n",
      "episode 2999, reward 1195.0, memory_length 2000, epsilon 0.22323952070689196 total_time 722.0\n",
      "Saving Model 3000\n",
      "episode 3009, reward 1112.0, memory_length 2000, epsilon 0.22212610895235071 total_time 721.0\n",
      "episode 3019, reward 1213.0, memory_length 2000, epsilon 0.22101825036210232 total_time 728.0\n",
      "episode 3029, reward 1134.0, memory_length 2000, epsilon 0.21991591723962442 total_time 725.0\n",
      "episode 3039, reward 1283.0, memory_length 2000, epsilon 0.21881908202653141 total_time 726.0\n",
      "episode 3049, reward 1250.0, memory_length 2000, epsilon 0.21772771730188595 total_time 721.0\n",
      "episode 3059, reward 1064.0, memory_length 2000, epsilon 0.216641795781513 total_time 723.0\n",
      "episode 3069, reward 558.0, memory_length 2000, epsilon 0.21556129031731802 total_time 724.0\n",
      "episode 3079, reward 802.0, memory_length 2000, epsilon 0.21448617389660815 total_time 728.0\n",
      "episode 3089, reward 1224.0, memory_length 2000, epsilon 0.21341641964141686 total_time 724.0\n",
      "episode 3099, reward 1423.0, memory_length 2000, epsilon 0.21235200080783204 total_time 723.0\n",
      "episode 3109, reward 1272.0, memory_length 2000, epsilon 0.21129289078532745 total_time 723.0\n",
      "episode 3119, reward 1130.0, memory_length 2000, epsilon 0.2102390630960973 total_time 722.0\n",
      "episode 3129, reward 1426.0, memory_length 2000, epsilon 0.20919049139439458 total_time 721.0\n",
      "episode 3139, reward 1044.0, memory_length 2000, epsilon 0.20814714946587198 total_time 726.0\n",
      "episode 3149, reward 1200.0, memory_length 2000, epsilon 0.2071090112269271 total_time 722.0\n",
      "episode 3159, reward 1401.0, memory_length 2000, epsilon 0.2060760507240498 total_time 721.0\n",
      "episode 3169, reward 1378.0, memory_length 2000, epsilon 0.20504824213317377 total_time 728.0\n",
      "episode 3179, reward 1099.0, memory_length 2000, epsilon 0.2040255597590306 total_time 728.0\n",
      "episode 3189, reward 1102.0, memory_length 2000, epsilon 0.20300797803450785 total_time 723.0\n",
      "episode 3199, reward 725.0, memory_length 2000, epsilon 0.2019954715200092 total_time 723.0\n",
      "episode 3209, reward 1411.0, memory_length 2000, epsilon 0.20098801490281926 total_time 725.0\n",
      "episode 3219, reward 1631.0, memory_length 2000, epsilon 0.19998558299646998 total_time 721.0\n",
      "episode 3229, reward 1322.0, memory_length 2000, epsilon 0.1989881507401115 total_time 724.0\n",
      "episode 3239, reward 887.0, memory_length 2000, epsilon 0.19799569319788554 total_time 722.0\n",
      "episode 3249, reward 1127.0, memory_length 2000, epsilon 0.1970081855583018 total_time 725.0\n",
      "episode 3259, reward 1098.0, memory_length 2000, epsilon 0.19602560313361786 total_time 723.0\n",
      "episode 3269, reward 1132.0, memory_length 2000, epsilon 0.19504792135922194 total_time 725.0\n",
      "episode 3279, reward 1401.0, memory_length 2000, epsilon 0.19407511579301878 total_time 723.0\n",
      "episode 3289, reward 1102.0, memory_length 2000, epsilon 0.1931071621148185 total_time 726.0\n",
      "episode 3299, reward 1161.0, memory_length 2000, epsilon 0.19214403612572878 total_time 722.0\n",
      "episode 3309, reward 1176.0, memory_length 2000, epsilon 0.19118571374754967 total_time 722.0\n",
      "episode 3319, reward 1044.0, memory_length 2000, epsilon 0.1902321710221719 total_time 728.0\n",
      "episode 3329, reward 1365.0, memory_length 2000, epsilon 0.1892833841109776 total_time 725.0\n",
      "episode 3339, reward 1566.0, memory_length 2000, epsilon 0.1883393292942446 total_time 725.0\n",
      "episode 3349, reward 1368.0, memory_length 2000, epsilon 0.18739998297055327 total_time 729.0\n",
      "episode 3359, reward 932.0, memory_length 2000, epsilon 0.18646532165619667 total_time 727.0\n",
      "episode 3369, reward 1050.0, memory_length 2000, epsilon 0.1855353219845932 total_time 723.0\n",
      "episode 3379, reward 1036.0, memory_length 2000, epsilon 0.18460996070570268 total_time 724.0\n",
      "episode 3389, reward 1130.0, memory_length 2000, epsilon 0.18368921468544486 total_time 726.0\n",
      "episode 3399, reward 973.0, memory_length 2000, epsilon 0.18277306090512138 total_time 724.0\n",
      "episode 3409, reward 1374.0, memory_length 2000, epsilon 0.18186147646083992 total_time 722.0\n",
      "episode 3419, reward 1010.0, memory_length 2000, epsilon 0.18095443856294197 total_time 722.0\n",
      "episode 3429, reward 894.0, memory_length 2000, epsilon 0.1800519245354328 total_time 722.0\n",
      "episode 3439, reward 1095.0, memory_length 2000, epsilon 0.17915391181541473 total_time 721.0\n",
      "episode 3449, reward 1548.0, memory_length 2000, epsilon 0.17826037795252297 total_time 725.0\n",
      "episode 3459, reward 1352.0, memory_length 2000, epsilon 0.17737130060836448 total_time 724.0\n",
      "episode 3469, reward 1066.0, memory_length 2000, epsilon 0.17648665755595927 total_time 722.0\n",
      "episode 3479, reward 1297.0, memory_length 2000, epsilon 0.17560642667918497 total_time 727.0\n",
      "episode 3489, reward 905.0, memory_length 2000, epsilon 0.17473058597222385 total_time 724.0\n",
      "episode 3499, reward 1288.0, memory_length 2000, epsilon 0.17385911353901257 total_time 727.0\n",
      "episode 3509, reward 1306.0, memory_length 2000, epsilon 0.17299198759269493 total_time 721.0\n",
      "episode 3519, reward 1352.0, memory_length 2000, epsilon 0.1721291864550771 total_time 723.0\n",
      "episode 3529, reward 1394.0, memory_length 2000, epsilon 0.17127068855608577 total_time 724.0\n",
      "episode 3539, reward 1136.0, memory_length 2000, epsilon 0.17041647243322863 total_time 723.0\n",
      "episode 3549, reward 1195.0, memory_length 2000, epsilon 0.16956651673105824 total_time 722.0\n",
      "episode 3559, reward 815.0, memory_length 2000, epsilon 0.16872080020063768 total_time 724.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3569, reward 1376.0, memory_length 2000, epsilon 0.16787930169900972 total_time 722.0\n",
      "episode 3579, reward 762.0, memory_length 2000, epsilon 0.16704200018866794 total_time 722.0\n",
      "episode 3589, reward 1492.0, memory_length 2000, epsilon 0.166208874737031 total_time 724.0\n",
      "episode 3599, reward 1415.0, memory_length 2000, epsilon 0.1653799045159192 total_time 724.0\n",
      "episode 3609, reward 1148.0, memory_length 2000, epsilon 0.16455506880103385 total_time 721.0\n",
      "episode 3619, reward 1156.0, memory_length 2000, epsilon 0.1637343469714391 total_time 721.0\n",
      "episode 3629, reward 1059.0, memory_length 2000, epsilon 0.1629177185090465 total_time 728.0\n",
      "episode 3639, reward 1170.0, memory_length 2000, epsilon 0.16210516299810185 total_time 725.0\n",
      "episode 3649, reward 1224.0, memory_length 2000, epsilon 0.16129666012467522 total_time 726.0\n",
      "episode 3659, reward 1513.0, memory_length 2000, epsilon 0.16049218967615253 total_time 724.0\n",
      "episode 3669, reward 1321.0, memory_length 2000, epsilon 0.15969173154073077 total_time 723.0\n",
      "episode 3679, reward 911.0, memory_length 2000, epsilon 0.15889526570691476 total_time 721.0\n",
      "episode 3689, reward 1157.0, memory_length 2000, epsilon 0.15810277226301725 total_time 728.0\n",
      "episode 3699, reward 1256.0, memory_length 2000, epsilon 0.15731423139666081 total_time 729.0\n",
      "episode 3709, reward 1571.0, memory_length 2000, epsilon 0.15652962339428284 total_time 727.0\n",
      "episode 3719, reward 1488.0, memory_length 2000, epsilon 0.15574892864064224 total_time 724.0\n",
      "episode 3729, reward 1093.0, memory_length 2000, epsilon 0.1549721276183296 total_time 723.0\n",
      "episode 3739, reward 1257.0, memory_length 2000, epsilon 0.1541992009072789 total_time 726.0\n",
      "episode 3749, reward 1155.0, memory_length 2000, epsilon 0.1534301291842821 total_time 725.0\n",
      "episode 3759, reward 1319.0, memory_length 2000, epsilon 0.15266489322250604 total_time 728.0\n",
      "episode 3769, reward 1248.0, memory_length 2000, epsilon 0.15190347389101183 total_time 727.0\n",
      "episode 3779, reward 1460.0, memory_length 2000, epsilon 0.1511458521542766 total_time 723.0\n",
      "episode 3789, reward 1270.0, memory_length 2000, epsilon 0.15039200907171735 total_time 723.0\n",
      "episode 3799, reward 1030.0, memory_length 2000, epsilon 0.14964192579721786 total_time 727.0\n",
      "episode 3809, reward 1652.0, memory_length 2000, epsilon 0.14889558357865715 total_time 732.0\n",
      "episode 3819, reward 1706.0, memory_length 2000, epsilon 0.1481529637574409 total_time 721.0\n",
      "episode 3829, reward 998.0, memory_length 2000, epsilon 0.14741404776803485 total_time 721.0\n",
      "episode 3839, reward 1015.0, memory_length 2000, epsilon 0.1466788171375009 total_time 724.0\n",
      "episode 3849, reward 1210.0, memory_length 2000, epsilon 0.14594725348503484 total_time 722.0\n",
      "episode 3859, reward 1325.0, memory_length 2000, epsilon 0.14521933852150737 total_time 724.0\n",
      "episode 3869, reward 1274.0, memory_length 2000, epsilon 0.14449505404900642 total_time 721.0\n",
      "episode 3879, reward 1283.0, memory_length 2000, epsilon 0.1437743819603825 total_time 723.0\n",
      "episode 3889, reward 1170.0, memory_length 2000, epsilon 0.14305730423879587 total_time 727.0\n",
      "episode 3899, reward 707.0, memory_length 2000, epsilon 0.1423438029572661 total_time 722.0\n",
      "episode 3909, reward 1067.0, memory_length 2000, epsilon 0.141633860278224 total_time 723.0\n",
      "episode 3919, reward 1013.0, memory_length 2000, epsilon 0.14092745845306562 total_time 729.0\n",
      "episode 3929, reward 1071.0, memory_length 2000, epsilon 0.14022457982170855 total_time 727.0\n",
      "episode 3939, reward 1637.0, memory_length 2000, epsilon 0.13952520681215042 total_time 723.0\n",
      "episode 3949, reward 1446.0, memory_length 2000, epsilon 0.1388293219400295 total_time 723.0\n",
      "episode 3959, reward 786.0, memory_length 2000, epsilon 0.1381369078081878 total_time 724.0\n",
      "episode 3969, reward 1098.0, memory_length 2000, epsilon 0.13744794710623595 total_time 730.0\n",
      "episode 3979, reward 996.0, memory_length 2000, epsilon 0.13676242261012048 total_time 722.0\n",
      "episode 3989, reward 1826.0, memory_length 2000, epsilon 0.13608031718169336 total_time 724.0\n",
      "episode 3999, reward 1601.0, memory_length 2000, epsilon 0.13540161376828327 total_time 723.0\n",
      "Saving Model 4000\n",
      "episode 4009, reward 1178.0, memory_length 2000, epsilon 0.13472629540226955 total_time 721.0\n",
      "episode 4019, reward 1550.0, memory_length 2000, epsilon 0.1340543452006579 total_time 725.0\n",
      "episode 4029, reward 1522.0, memory_length 2000, epsilon 0.1333857463646583 total_time 726.0\n",
      "episode 4039, reward 1424.0, memory_length 2000, epsilon 0.132720482179265 total_time 724.0\n",
      "episode 4049, reward 1092.0, memory_length 2000, epsilon 0.1320585360128386 total_time 722.0\n",
      "episode 4059, reward 1286.0, memory_length 2000, epsilon 0.1313998913166907 total_time 730.0\n",
      "episode 4069, reward 1460.0, memory_length 2000, epsilon 0.1307445316246694 total_time 727.0\n",
      "episode 4079, reward 1306.0, memory_length 2000, epsilon 0.13009244055274838 total_time 722.0\n",
      "episode 4089, reward 1242.0, memory_length 2000, epsilon 0.12944360179861678 total_time 723.0\n",
      "episode 4099, reward 1352.0, memory_length 2000, epsilon 0.12879799914127205 total_time 727.0\n",
      "episode 4109, reward 1198.0, memory_length 2000, epsilon 0.12815561644061407 total_time 722.0\n",
      "episode 4119, reward 1586.0, memory_length 2000, epsilon 0.1275164376370419 total_time 724.0\n",
      "episode 4129, reward 1007.0, memory_length 2000, epsilon 0.1268804467510521 total_time 727.0\n",
      "episode 4139, reward 1229.0, memory_length 2000, epsilon 0.12624762788283944 total_time 725.0\n",
      "episode 4149, reward 1173.0, memory_length 2000, epsilon 0.1256179652118993 total_time 725.0\n",
      "episode 4159, reward 1404.0, memory_length 2000, epsilon 0.12499144299663205 total_time 721.0\n",
      "episode 4169, reward 862.0, memory_length 2000, epsilon 0.12436804557394966 total_time 723.0\n",
      "episode 4179, reward 1024.0, memory_length 2000, epsilon 0.12374775735888416 total_time 721.0\n",
      "episode 4189, reward 1458.0, memory_length 2000, epsilon 0.12313056284419784 total_time 733.0\n",
      "episode 4199, reward 1193.0, memory_length 2000, epsilon 0.12251644659999568 total_time 725.0\n",
      "episode 4209, reward 1416.0, memory_length 2000, epsilon 0.12190539327333952 total_time 721.0\n",
      "episode 4219, reward 1195.0, memory_length 2000, epsilon 0.12129738758786451 total_time 724.0\n",
      "episode 4229, reward 1358.0, memory_length 2000, epsilon 0.12069241434339678 total_time 721.0\n",
      "episode 4239, reward 1208.0, memory_length 2000, epsilon 0.12009045841557368 total_time 722.0\n",
      "episode 4249, reward 1323.0, memory_length 2000, epsilon 0.1194915047554657 total_time 721.0\n",
      "episode 4259, reward 1447.0, memory_length 2000, epsilon 0.11889553838920008 total_time 727.0\n",
      "episode 4269, reward 1356.0, memory_length 2000, epsilon 0.11830254441758672 total_time 721.0\n",
      "episode 4279, reward 1465.0, memory_length 2000, epsilon 0.1177125080157454 total_time 722.0\n",
      "episode 4289, reward 1035.0, memory_length 2000, epsilon 0.11712541443273533 total_time 729.0\n",
      "episode 4299, reward 1269.0, memory_length 2000, epsilon 0.11654124899118631 total_time 726.0\n",
      "episode 4309, reward 1811.0, memory_length 2000, epsilon 0.115959997086932 total_time 722.0\n",
      "episode 4319, reward 1267.0, memory_length 2000, epsilon 0.11538164418864444 total_time 726.0\n",
      "episode 4329, reward 1121.0, memory_length 2000, epsilon 0.11480617583747106 total_time 724.0\n",
      "episode 4339, reward 1341.0, memory_length 2000, epsilon 0.11423357764667307 total_time 727.0\n",
      "episode 4349, reward 1398.0, memory_length 2000, epsilon 0.11366383530126595 total_time 726.0\n",
      "episode 4359, reward 1718.0, memory_length 2000, epsilon 0.11309693455766137 total_time 723.0\n",
      "episode 4369, reward 1121.0, memory_length 2000, epsilon 0.1125328612433112 total_time 724.0\n",
      "episode 4379, reward 1134.0, memory_length 2000, epsilon 0.11197160125635315 total_time 732.0\n",
      "episode 4389, reward 1260.0, memory_length 2000, epsilon 0.11141314056525843 total_time 725.0\n",
      "episode 4399, reward 1287.0, memory_length 2000, epsilon 0.11085746520848058 total_time 726.0\n",
      "episode 4409, reward 1540.0, memory_length 2000, epsilon 0.11030456129410682 total_time 724.0\n",
      "episode 4419, reward 1440.0, memory_length 2000, epsilon 0.10975441499951036 total_time 730.0\n",
      "episode 4429, reward 1718.0, memory_length 2000, epsilon 0.10920701257100535 total_time 724.0\n",
      "episode 4439, reward 1341.0, memory_length 2000, epsilon 0.10866234032350246 total_time 725.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4449, reward 1772.0, memory_length 2000, epsilon 0.10812038464016717 total_time 722.0\n",
      "episode 4459, reward 1275.0, memory_length 2000, epsilon 0.10758113197207911 total_time 723.0\n",
      "episode 4469, reward 1638.0, memory_length 2000, epsilon 0.10704456883789358 total_time 730.0\n",
      "episode 4479, reward 1611.0, memory_length 2000, epsilon 0.10651068182350425 total_time 721.0\n",
      "episode 4489, reward 1638.0, memory_length 2000, epsilon 0.10597945758170793 total_time 726.0\n",
      "episode 4499, reward 1261.0, memory_length 2000, epsilon 0.10545088283187094 total_time 723.0\n",
      "episode 4509, reward 1336.0, memory_length 2000, epsilon 0.10492494435959693 total_time 727.0\n",
      "episode 4519, reward 1278.0, memory_length 2000, epsilon 0.1044016290163968 total_time 729.0\n",
      "episode 4529, reward 1292.0, memory_length 2000, epsilon 0.10388092371935967 total_time 727.0\n",
      "episode 4539, reward 1364.0, memory_length 2000, epsilon 0.103362815450826 total_time 721.0\n",
      "episode 4549, reward 1330.0, memory_length 2000, epsilon 0.10284729125806202 total_time 729.0\n",
      "episode 4559, reward 1449.0, memory_length 2000, epsilon 0.1023343382529362 total_time 721.0\n",
      "episode 4569, reward 1526.0, memory_length 2000, epsilon 0.10182394361159659 total_time 726.0\n",
      "episode 4579, reward 1176.0, memory_length 2000, epsilon 0.10131609457415063 total_time 730.0\n",
      "episode 4589, reward 1235.0, memory_length 2000, epsilon 0.10081077844434586 total_time 736.0\n",
      "episode 4599, reward 771.0, memory_length 2000, epsilon 0.10030798258925279 total_time 723.0\n",
      "episode 4609, reward 1094.0, memory_length 2000, epsilon 0.09980769443894884 total_time 722.0\n",
      "episode 4619, reward 1571.0, memory_length 2000, epsilon 0.0993099014862042 total_time 724.0\n",
      "episode 4629, reward 1567.0, memory_length 2000, epsilon 0.09881459128616905 total_time 723.0\n",
      "episode 4639, reward 1323.0, memory_length 2000, epsilon 0.09832175145606269 total_time 722.0\n",
      "episode 4649, reward 1409.0, memory_length 2000, epsilon 0.09783136967486368 total_time 721.0\n",
      "episode 4659, reward 936.0, memory_length 2000, epsilon 0.09734343368300191 total_time 726.0\n",
      "episode 4669, reward 1242.0, memory_length 2000, epsilon 0.09685793128205217 total_time 730.0\n",
      "episode 4679, reward 1774.0, memory_length 2000, epsilon 0.09637485033442919 total_time 726.0\n",
      "episode 4689, reward 1309.0, memory_length 2000, epsilon 0.0958941787630841 total_time 722.0\n",
      "episode 4699, reward 1229.0, memory_length 2000, epsilon 0.0954159045512026 total_time 729.0\n",
      "episode 4709, reward 973.0, memory_length 2000, epsilon 0.0949400157419044 total_time 726.0\n",
      "episode 4719, reward 1360.0, memory_length 2000, epsilon 0.09446650043794459 total_time 721.0\n",
      "episode 4729, reward 1536.0, memory_length 2000, epsilon 0.09399534680141587 total_time 728.0\n",
      "episode 4739, reward 802.0, memory_length 2000, epsilon 0.09352654305345277 total_time 729.0\n",
      "episode 4749, reward 1213.0, memory_length 2000, epsilon 0.0930600774739372 total_time 726.0\n",
      "episode 4759, reward 1335.0, memory_length 2000, epsilon 0.09259593840120531 total_time 724.0\n",
      "episode 4769, reward 1080.0, memory_length 2000, epsilon 0.0921341142317562 total_time 729.0\n",
      "episode 4779, reward 1158.0, memory_length 2000, epsilon 0.09167459341996154 total_time 721.0\n",
      "episode 4789, reward 1625.0, memory_length 2000, epsilon 0.0912173644777771 total_time 725.0\n",
      "episode 4799, reward 1110.0, memory_length 2000, epsilon 0.09076241597445547 total_time 721.0\n",
      "episode 4809, reward 1617.0, memory_length 2000, epsilon 0.09030973653626047 total_time 730.0\n",
      "episode 4819, reward 1292.0, memory_length 2000, epsilon 0.0898593148461825 total_time 730.0\n",
      "episode 4829, reward 1456.0, memory_length 2000, epsilon 0.08941113964365587 total_time 724.0\n",
      "episode 4839, reward 1535.0, memory_length 2000, epsilon 0.08896519972427712 total_time 722.0\n",
      "episode 4849, reward 1332.0, memory_length 2000, epsilon 0.08852148393952511 total_time 721.0\n",
      "episode 4859, reward 1237.0, memory_length 2000, epsilon 0.08807998119648211 total_time 721.0\n",
      "episode 4869, reward 1350.0, memory_length 2000, epsilon 0.0876406804575565 total_time 723.0\n",
      "episode 4879, reward 1336.0, memory_length 2000, epsilon 0.08720357074020693 total_time 721.0\n",
      "episode 4889, reward 1078.0, memory_length 2000, epsilon 0.08676864111666777 total_time 721.0\n",
      "episode 4899, reward 1230.0, memory_length 2000, epsilon 0.0863358807136757 total_time 723.0\n",
      "episode 4909, reward 1349.0, memory_length 2000, epsilon 0.08590527871219816 total_time 723.0\n",
      "episode 4919, reward 1302.0, memory_length 2000, epsilon 0.08547682434716262 total_time 735.0\n",
      "episode 4929, reward 1409.0, memory_length 2000, epsilon 0.08505050690718771 total_time 726.0\n",
      "episode 4939, reward 1107.0, memory_length 2000, epsilon 0.08462631573431521 total_time 728.0\n",
      "episode 4949, reward 1486.0, memory_length 2000, epsilon 0.08420424022374369 total_time 726.0\n",
      "episode 4959, reward 1171.0, memory_length 2000, epsilon 0.08378426982356336 total_time 723.0\n",
      "episode 4969, reward 1566.0, memory_length 2000, epsilon 0.08336639403449243 total_time 734.0\n",
      "episode 4979, reward 1266.0, memory_length 2000, epsilon 0.08295060240961435 total_time 723.0\n",
      "episode 4989, reward 1270.0, memory_length 2000, epsilon 0.08253688455411688 total_time 725.0\n",
      "episode 4999, reward 927.0, memory_length 2000, epsilon 0.08212523012503205 total_time 727.0\n",
      "Saving Model 5000\n",
      "episode 5009, reward 969.0, memory_length 2000, epsilon 0.08171562883097767 total_time 724.0\n",
      "episode 5019, reward 1059.0, memory_length 2000, epsilon 0.08130807043190014 total_time 731.0\n",
      "episode 5029, reward 1454.0, memory_length 2000, epsilon 0.0809025447388182 total_time 724.0\n",
      "episode 5039, reward 1573.0, memory_length 2000, epsilon 0.08049904161356838 total_time 724.0\n",
      "episode 5049, reward 817.0, memory_length 2000, epsilon 0.08009755096855156 total_time 725.0\n",
      "episode 5059, reward 1198.0, memory_length 2000, epsilon 0.07969806276648073 total_time 727.0\n",
      "episode 5069, reward 1353.0, memory_length 2000, epsilon 0.07930056702013001 total_time 725.0\n",
      "episode 5079, reward 1279.0, memory_length 2000, epsilon 0.07890505379208503 total_time 730.0\n",
      "episode 5089, reward 1171.0, memory_length 2000, epsilon 0.07851151319449445 total_time 725.0\n",
      "episode 5099, reward 1556.0, memory_length 2000, epsilon 0.07811993538882292 total_time 721.0\n",
      "episode 5109, reward 1657.0, memory_length 2000, epsilon 0.07773031058560487 total_time 722.0\n",
      "episode 5119, reward 1380.0, memory_length 2000, epsilon 0.07734262904419989 total_time 722.0\n",
      "episode 5129, reward 1310.0, memory_length 2000, epsilon 0.07695688107254926 total_time 725.0\n",
      "episode 5139, reward 1262.0, memory_length 2000, epsilon 0.07657305702693368 total_time 724.0\n",
      "episode 5149, reward 1473.0, memory_length 2000, epsilon 0.07619114731173192 total_time 725.0\n",
      "episode 5159, reward 1639.0, memory_length 2000, epsilon 0.07581114237918127 total_time 721.0\n",
      "episode 5169, reward 1573.0, memory_length 2000, epsilon 0.07543303272913854 total_time 722.0\n",
      "episode 5179, reward 1126.0, memory_length 2000, epsilon 0.07505680890884289 total_time 724.0\n",
      "episode 5189, reward 1013.0, memory_length 2000, epsilon 0.07468246151267918 total_time 728.0\n",
      "episode 5199, reward 1261.0, memory_length 2000, epsilon 0.074309981181943 total_time 727.0\n",
      "episode 5209, reward 1361.0, memory_length 2000, epsilon 0.07393935860460665 total_time 727.0\n",
      "episode 5219, reward 1221.0, memory_length 2000, epsilon 0.07357058451508645 total_time 722.0\n",
      "episode 5229, reward 1426.0, memory_length 2000, epsilon 0.07320364969401096 total_time 721.0\n",
      "episode 5239, reward 1542.0, memory_length 2000, epsilon 0.07283854496799048 total_time 724.0\n",
      "episode 5249, reward 1582.0, memory_length 2000, epsilon 0.0724752612093879 total_time 721.0\n",
      "episode 5259, reward 1253.0, memory_length 2000, epsilon 0.07211378933609025 total_time 722.0\n",
      "episode 5269, reward 1386.0, memory_length 2000, epsilon 0.07175412031128199 total_time 722.0\n",
      "episode 5279, reward 1276.0, memory_length 2000, epsilon 0.0713962451432187 total_time 722.0\n",
      "episode 5289, reward 1369.0, memory_length 2000, epsilon 0.07104015488500255 total_time 723.0\n",
      "episode 5299, reward 1026.0, memory_length 2000, epsilon 0.07068584063435851 total_time 722.0\n",
      "episode 5309, reward 1517.0, memory_length 2000, epsilon 0.07033329353341192 total_time 726.0\n",
      "episode 5319, reward 1404.0, memory_length 2000, epsilon 0.06998250476846683 total_time 727.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 5329, reward 1140.0, memory_length 2000, epsilon 0.0696334655697859 total_time 723.0\n",
      "episode 5339, reward 1488.0, memory_length 2000, epsilon 0.06928616721137094 total_time 723.0\n",
      "episode 5349, reward 1409.0, memory_length 2000, epsilon 0.06894060101074495 total_time 721.0\n",
      "episode 5359, reward 1200.0, memory_length 2000, epsilon 0.06859675832873488 total_time 722.0\n",
      "episode 5369, reward 1205.0, memory_length 2000, epsilon 0.06825463056925578 total_time 724.0\n",
      "episode 5379, reward 1631.0, memory_length 2000, epsilon 0.06791420917909581 total_time 721.0\n",
      "episode 5389, reward 1481.0, memory_length 2000, epsilon 0.06757548564770255 total_time 722.0\n",
      "episode 5399, reward 1317.0, memory_length 2000, epsilon 0.06723845150697004 total_time 724.0\n",
      "episode 5409, reward 1700.0, memory_length 2000, epsilon 0.06690309833102723 total_time 723.0\n",
      "episode 5419, reward 931.0, memory_length 2000, epsilon 0.06656941773602718 total_time 727.0\n",
      "episode 5429, reward 1533.0, memory_length 2000, epsilon 0.06623740137993772 total_time 723.0\n",
      "episode 5439, reward 1489.0, memory_length 2000, epsilon 0.06590704096233263 total_time 722.0\n",
      "episode 5449, reward 1209.0, memory_length 2000, epsilon 0.06557832822418427 total_time 725.0\n",
      "episode 5459, reward 1023.0, memory_length 2000, epsilon 0.06525125494765702 total_time 725.0\n",
      "episode 5469, reward 1339.0, memory_length 2000, epsilon 0.064925812955902 total_time 726.0\n",
      "episode 5479, reward 1401.0, memory_length 2000, epsilon 0.06460199411285243 total_time 730.0\n",
      "episode 5489, reward 1350.0, memory_length 2000, epsilon 0.06427979032302036 total_time 731.0\n",
      "episode 5499, reward 1413.0, memory_length 2000, epsilon 0.06395919353129426 total_time 727.0\n",
      "episode 5509, reward 1238.0, memory_length 2000, epsilon 0.06364019572273769 total_time 730.0\n",
      "episode 5519, reward 1442.0, memory_length 2000, epsilon 0.06332278892238877 total_time 726.0\n",
      "episode 5529, reward 1225.0, memory_length 2000, epsilon 0.06300696519506098 total_time 729.0\n",
      "episode 5539, reward 1409.0, memory_length 2000, epsilon 0.06269271664514467 total_time 727.0\n",
      "episode 5549, reward 1337.0, memory_length 2000, epsilon 0.06238003541640971 total_time 723.0\n",
      "episode 5559, reward 1212.0, memory_length 2000, epsilon 0.06206891369180917 total_time 729.0\n",
      "episode 5569, reward 1340.0, memory_length 2000, epsilon 0.061759343693283675 total_time 722.0\n",
      "episode 5579, reward 1171.0, memory_length 2000, epsilon 0.06145131768156714 total_time 732.0\n",
      "episode 5589, reward 1377.0, memory_length 2000, epsilon 0.061144827955993214 total_time 728.0\n",
      "episode 5599, reward 1437.0, memory_length 2000, epsilon 0.06083986685430284 total_time 731.0\n",
      "episode 5609, reward 1537.0, memory_length 2000, epsilon 0.06053642675245258 total_time 723.0\n",
      "episode 5619, reward 1311.0, memory_length 2000, epsilon 0.06023450006442407 total_time 721.0\n",
      "episode 5629, reward 1611.0, memory_length 2000, epsilon 0.059934079242034345 total_time 724.0\n",
      "episode 5639, reward 948.0, memory_length 2000, epsilon 0.05963515677474728 total_time 725.0\n",
      "episode 5649, reward 1639.0, memory_length 2000, epsilon 0.05933772518948558 total_time 726.0\n",
      "episode 5659, reward 1485.0, memory_length 2000, epsilon 0.05904177705044413 total_time 729.0\n",
      "episode 5669, reward 1211.0, memory_length 2000, epsilon 0.05874730495890401 total_time 728.0\n",
      "episode 5679, reward 1468.0, memory_length 2000, epsilon 0.05845430155304764 total_time 722.0\n",
      "episode 5689, reward 1489.0, memory_length 2000, epsilon 0.05816275950777461 total_time 727.0\n",
      "episode 5699, reward 905.0, memory_length 2000, epsilon 0.05787267153451857 total_time 727.0\n",
      "episode 5709, reward 1312.0, memory_length 2000, epsilon 0.05758403038106508 total_time 722.0\n",
      "episode 5719, reward 1204.0, memory_length 2000, epsilon 0.05729682883137031 total_time 723.0\n",
      "episode 5729, reward 1010.0, memory_length 2000, epsilon 0.057011059705380535 total_time 724.0\n",
      "episode 5739, reward 1485.0, memory_length 2000, epsilon 0.05672671585885272 total_time 722.0\n",
      "episode 5749, reward 1113.0, memory_length 2000, epsilon 0.05644379018317588 total_time 723.0\n",
      "episode 5759, reward 1655.0, memory_length 2000, epsilon 0.056162275605193414 total_time 722.0\n",
      "episode 5769, reward 1336.0, memory_length 2000, epsilon 0.05588216508702621 total_time 723.0\n",
      "episode 5779, reward 1344.0, memory_length 2000, epsilon 0.055603451625896715 total_time 725.0\n",
      "episode 5789, reward 1445.0, memory_length 2000, epsilon 0.05532612825395388 total_time 732.0\n",
      "episode 5799, reward 1418.0, memory_length 2000, epsilon 0.055050188038098934 total_time 721.0\n",
      "episode 5809, reward 1360.0, memory_length 2000, epsilon 0.05477562407981217 total_time 725.0\n",
      "episode 5819, reward 1519.0, memory_length 2000, epsilon 0.0545024295149803 total_time 724.0\n",
      "episode 5829, reward 1278.0, memory_length 2000, epsilon 0.054230597513724985 total_time 724.0\n",
      "episode 5839, reward 1017.0, memory_length 2000, epsilon 0.05396012128023198 total_time 726.0\n",
      "episode 5849, reward 1628.0, memory_length 2000, epsilon 0.05369099405258145 total_time 722.0\n",
      "episode 5859, reward 1337.0, memory_length 2000, epsilon 0.05342320910257864 total_time 724.0\n",
      "episode 5869, reward 1335.0, memory_length 2000, epsilon 0.05315675973558585 total_time 725.0\n",
      "episode 5879, reward 1329.0, memory_length 2000, epsilon 0.05289163929035501 total_time 722.0\n",
      "episode 5889, reward 1174.0, memory_length 2000, epsilon 0.05262784113886122 total_time 721.0\n",
      "episode 5899, reward 1683.0, memory_length 2000, epsilon 0.05236535868613695 total_time 737.0\n",
      "episode 5909, reward 1143.0, memory_length 2000, epsilon 0.0521041853701072 total_time 722.0\n",
      "episode 5919, reward 1629.0, memory_length 2000, epsilon 0.05184431466142543 total_time 722.0\n",
      "episode 5929, reward 1484.0, memory_length 2000, epsilon 0.051585740063310445 total_time 723.0\n",
      "episode 5939, reward 1477.0, memory_length 2000, epsilon 0.051328455111383814 total_time 724.0\n",
      "episode 5949, reward 1301.0, memory_length 2000, epsilon 0.05107245337350832 total_time 724.0\n",
      "episode 5959, reward 1355.0, memory_length 2000, epsilon 0.05081772844962717 total_time 724.0\n",
      "episode 5969, reward 1329.0, memory_length 2000, epsilon 0.05056427397160404 total_time 721.0\n",
      "episode 5979, reward 1510.0, memory_length 2000, epsilon 0.05031208360306376 total_time 721.0\n",
      "episode 5989, reward 1377.0, memory_length 2000, epsilon 0.050061151039233975 total_time 729.0\n",
      "episode 5999, reward 1523.0, memory_length 2000, epsilon 0.049811470006787505 total_time 731.0\n",
      "Saving Model 6000\n",
      "episode 6009, reward 1287.0, memory_length 2000, epsilon 0.04956303426368557 total_time 722.0\n",
      "episode 6019, reward 1131.0, memory_length 2000, epsilon 0.04931583759902165 total_time 725.0\n",
      "episode 6029, reward 945.0, memory_length 2000, epsilon 0.049069873832866234 total_time 733.0\n",
      "episode 6039, reward 1472.0, memory_length 2000, epsilon 0.04882513681611236 total_time 726.0\n",
      "episode 6049, reward 1397.0, memory_length 2000, epsilon 0.04858162043032186 total_time 726.0\n",
      "episode 6059, reward 1359.0, memory_length 2000, epsilon 0.04833931858757242 total_time 721.0\n",
      "episode 6069, reward 1886.0, memory_length 2000, epsilon 0.04809822523030535 total_time 723.0\n",
      "episode 6079, reward 1253.0, memory_length 2000, epsilon 0.04785833433117416 total_time 725.0\n",
      "episode 6089, reward 1337.0, memory_length 2000, epsilon 0.04761963989289384 total_time 726.0\n",
      "episode 6099, reward 1305.0, memory_length 2000, epsilon 0.04738213594809106 total_time 738.0\n",
      "episode 6109, reward 1401.0, memory_length 2000, epsilon 0.04714581655915481 total_time 721.0\n",
      "episode 6119, reward 1316.0, memory_length 2000, epsilon 0.04691067581808805 total_time 727.0\n",
      "episode 6129, reward 1526.0, memory_length 2000, epsilon 0.04667670784635998 total_time 727.0\n",
      "episode 6139, reward 991.0, memory_length 2000, epsilon 0.046443906794759175 total_time 729.0\n",
      "episode 6149, reward 1041.0, memory_length 2000, epsilon 0.046212266843247196 total_time 724.0\n",
      "episode 6159, reward 1770.0, memory_length 2000, epsilon 0.04598178220081319 total_time 725.0\n",
      "episode 6169, reward 1639.0, memory_length 2000, epsilon 0.04575244710532907 total_time 724.0\n",
      "episode 6179, reward 1358.0, memory_length 2000, epsilon 0.045524255823405545 total_time 722.0\n",
      "episode 6189, reward 1216.0, memory_length 2000, epsilon 0.04529720265024866 total_time 723.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 6199, reward 1504.0, memory_length 2000, epsilon 0.045071281909517265 total_time 726.0\n",
      "episode 6209, reward 1125.0, memory_length 2000, epsilon 0.04484648795318105 total_time 722.0\n",
      "episode 6219, reward 1276.0, memory_length 2000, epsilon 0.04462281516137944 total_time 721.0\n",
      "episode 6229, reward 1700.0, memory_length 2000, epsilon 0.044400257942280974 total_time 721.0\n",
      "episode 6239, reward 1359.0, memory_length 2000, epsilon 0.04417881073194358 total_time 726.0\n",
      "episode 6249, reward 1692.0, memory_length 2000, epsilon 0.04395846799417545 total_time 721.0\n",
      "episode 6259, reward 1707.0, memory_length 2000, epsilon 0.043739224220396694 total_time 725.0\n",
      "episode 6269, reward 1641.0, memory_length 2000, epsilon 0.04352107392950154 total_time 721.0\n",
      "episode 6279, reward 1149.0, memory_length 2000, epsilon 0.04330401166772134 total_time 726.0\n",
      "episode 6289, reward 1621.0, memory_length 2000, epsilon 0.04308803200848826 total_time 728.0\n",
      "episode 6299, reward 1364.0, memory_length 2000, epsilon 0.042873129552299535 total_time 726.0\n",
      "episode 6309, reward 1179.0, memory_length 2000, epsilon 0.04265929892658262 total_time 724.0\n",
      "episode 6319, reward 1226.0, memory_length 2000, epsilon 0.04244653478556071 total_time 725.0\n",
      "episode 6329, reward 1326.0, memory_length 2000, epsilon 0.042234831810119194 total_time 728.0\n",
      "episode 6339, reward 858.0, memory_length 2000, epsilon 0.04202418470767265 total_time 728.0\n",
      "episode 6349, reward 1236.0, memory_length 2000, epsilon 0.04181458821203258 total_time 722.0\n",
      "episode 6359, reward 1275.0, memory_length 2000, epsilon 0.04160603708327565 total_time 727.0\n",
      "episode 6369, reward 1530.0, memory_length 2000, epsilon 0.041398526107612785 total_time 731.0\n",
      "episode 6379, reward 1302.0, memory_length 2000, epsilon 0.041192050097258764 total_time 727.0\n",
      "episode 6389, reward 1476.0, memory_length 2000, epsilon 0.04098660389030262 total_time 724.0\n",
      "episode 6399, reward 1418.0, memory_length 2000, epsilon 0.04078218235057845 total_time 729.0\n",
      "episode 6409, reward 1806.0, memory_length 2000, epsilon 0.04057878036753712 total_time 724.0\n",
      "episode 6419, reward 1436.0, memory_length 2000, epsilon 0.04037639285611844 total_time 727.0\n",
      "episode 6429, reward 1481.0, memory_length 2000, epsilon 0.04017501475662412 total_time 722.0\n",
      "episode 6439, reward 1465.0, memory_length 2000, epsilon 0.03997464103459117 total_time 721.0\n",
      "episode 6449, reward 1414.0, memory_length 2000, epsilon 0.039775266680666096 total_time 724.0\n",
      "episode 6459, reward 1798.0, memory_length 2000, epsilon 0.039576886710479646 total_time 724.0\n",
      "episode 6469, reward 1093.0, memory_length 2000, epsilon 0.03937949616452228 total_time 721.0\n",
      "episode 6479, reward 1337.0, memory_length 2000, epsilon 0.03918309010802005 total_time 724.0\n",
      "episode 6489, reward 1508.0, memory_length 2000, epsilon 0.03898766363081129 total_time 729.0\n",
      "episode 6499, reward 880.0, memory_length 2000, epsilon 0.0387932118472239 total_time 723.0\n",
      "episode 6509, reward 1341.0, memory_length 2000, epsilon 0.0385997298959532 total_time 726.0\n",
      "episode 6519, reward 1128.0, memory_length 2000, epsilon 0.03840721293994029 total_time 724.0\n",
      "episode 6529, reward 1301.0, memory_length 2000, epsilon 0.03821565616625126 total_time 723.0\n",
      "episode 6539, reward 1337.0, memory_length 2000, epsilon 0.03802505478595679 total_time 724.0\n",
      "episode 6549, reward 1570.0, memory_length 2000, epsilon 0.03783540403401242 total_time 723.0\n",
      "episode 6559, reward 1076.0, memory_length 2000, epsilon 0.03764669916913952 total_time 724.0\n",
      "episode 6569, reward 1549.0, memory_length 2000, epsilon 0.037458935473706614 total_time 731.0\n",
      "episode 6579, reward 1726.0, memory_length 2000, epsilon 0.03727210825361153 total_time 724.0\n",
      "episode 6589, reward 1464.0, memory_length 2000, epsilon 0.03708621283816403 total_time 727.0\n",
      "episode 6599, reward 1494.0, memory_length 2000, epsilon 0.03690124457996908 total_time 729.0\n",
      "episode 6609, reward 1054.0, memory_length 2000, epsilon 0.036717198854810576 total_time 727.0\n",
      "episode 6619, reward 1657.0, memory_length 2000, epsilon 0.036534071061535785 total_time 727.0\n",
      "episode 6629, reward 1436.0, memory_length 2000, epsilon 0.03635185662194034 total_time 725.0\n",
      "episode 6639, reward 1503.0, memory_length 2000, epsilon 0.03617055098065379 total_time 724.0\n",
      "episode 6649, reward 1361.0, memory_length 2000, epsilon 0.03599014960502563 total_time 727.0\n",
      "episode 6659, reward 1661.0, memory_length 2000, epsilon 0.035810647985012094 total_time 727.0\n",
      "episode 6669, reward 1294.0, memory_length 2000, epsilon 0.03563204163306331 total_time 726.0\n",
      "episode 6679, reward 1719.0, memory_length 2000, epsilon 0.035454326084011195 total_time 727.0\n",
      "episode 6689, reward 1160.0, memory_length 2000, epsilon 0.03527749689495777 total_time 721.0\n",
      "episode 6699, reward 1608.0, memory_length 2000, epsilon 0.03510154964516409 total_time 723.0\n",
      "episode 6709, reward 1635.0, memory_length 2000, epsilon 0.03492647993593973 total_time 723.0\n",
      "episode 6719, reward 1188.0, memory_length 2000, epsilon 0.034752283390532865 total_time 722.0\n",
      "episode 6729, reward 1585.0, memory_length 2000, epsilon 0.03457895565402079 total_time 727.0\n",
      "episode 6739, reward 1260.0, memory_length 2000, epsilon 0.03440649239320105 total_time 725.0\n",
      "episode 6749, reward 1181.0, memory_length 2000, epsilon 0.03423488929648313 total_time 722.0\n",
      "episode 6759, reward 1616.0, memory_length 2000, epsilon 0.0340641420737807 total_time 722.0\n",
      "episode 6769, reward 1611.0, memory_length 2000, epsilon 0.0338942464564043 total_time 724.0\n",
      "episode 6779, reward 1339.0, memory_length 2000, epsilon 0.03372519819695464 total_time 721.0\n",
      "episode 6789, reward 1595.0, memory_length 2000, epsilon 0.03355699306921642 total_time 724.0\n",
      "episode 6799, reward 1909.0, memory_length 2000, epsilon 0.03338962686805266 total_time 725.0\n",
      "episode 6809, reward 1496.0, memory_length 2000, epsilon 0.033223095409299686 total_time 721.0\n",
      "episode 6819, reward 1430.0, memory_length 2000, epsilon 0.03305739452966231 total_time 723.0\n",
      "episode 6829, reward 1603.0, memory_length 2000, epsilon 0.032892520086609915 total_time 729.0\n",
      "episode 6839, reward 1566.0, memory_length 2000, epsilon 0.03272846795827282 total_time 733.0\n",
      "episode 6849, reward 1541.0, memory_length 2000, epsilon 0.0325652340433393 total_time 721.0\n",
      "episode 6859, reward 1477.0, memory_length 2000, epsilon 0.03240281426095298 total_time 729.0\n",
      "episode 6869, reward 1466.0, memory_length 2000, epsilon 0.03224120455061084 total_time 726.0\n",
      "episode 6879, reward 1119.0, memory_length 2000, epsilon 0.03208040087206167 total_time 722.0\n",
      "episode 6889, reward 1242.0, memory_length 2000, epsilon 0.03192039920520517 total_time 723.0\n",
      "episode 6899, reward 1632.0, memory_length 2000, epsilon 0.03176119554999133 total_time 723.0\n",
      "episode 6909, reward 1348.0, memory_length 2000, epsilon 0.031602785926320466 total_time 726.0\n",
      "episode 6919, reward 1206.0, memory_length 2000, epsilon 0.03144516637394371 total_time 721.0\n",
      "episode 6929, reward 1647.0, memory_length 2000, epsilon 0.03128833295236411 total_time 735.0\n",
      "episode 6939, reward 846.0, memory_length 2000, epsilon 0.031132281740737917 total_time 723.0\n",
      "episode 6949, reward 1581.0, memory_length 2000, epsilon 0.030977008837776713 total_time 721.0\n",
      "episode 6959, reward 1351.0, memory_length 2000, epsilon 0.030822510361649826 total_time 721.0\n",
      "episode 6969, reward 941.0, memory_length 2000, epsilon 0.030668782449887338 total_time 730.0\n",
      "episode 6979, reward 1023.0, memory_length 2000, epsilon 0.03051582125928343 total_time 730.0\n",
      "episode 6989, reward 1241.0, memory_length 2000, epsilon 0.030363622965800367 total_time 721.0\n",
      "episode 6999, reward 996.0, memory_length 2000, epsilon 0.030212183764472877 total_time 731.0\n",
      "Saving Model 7000\n",
      "episode 7009, reward 1234.0, memory_length 2000, epsilon 0.030061499869313068 total_time 729.0\n",
      "episode 7019, reward 1591.0, memory_length 2000, epsilon 0.029911567513215692 total_time 722.0\n",
      "episode 7029, reward 1478.0, memory_length 2000, epsilon 0.029762382947864045 total_time 724.0\n",
      "episode 7039, reward 880.0, memory_length 2000, epsilon 0.029613942443636205 total_time 729.0\n",
      "episode 7049, reward 1549.0, memory_length 2000, epsilon 0.029466242289511866 total_time 722.0\n",
      "episode 7059, reward 1491.0, memory_length 2000, epsilon 0.029319278792979464 total_time 725.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7069, reward 1275.0, memory_length 2000, epsilon 0.029173048279943936 total_time 732.0\n",
      "episode 7079, reward 1557.0, memory_length 2000, epsilon 0.029027547094634832 total_time 729.0\n",
      "episode 7089, reward 1605.0, memory_length 2000, epsilon 0.02888277159951494 total_time 728.0\n",
      "episode 7099, reward 1436.0, memory_length 2000, epsilon 0.028738718175189356 total_time 730.0\n",
      "episode 7109, reward 1426.0, memory_length 2000, epsilon 0.028595383220314963 total_time 721.0\n",
      "episode 7119, reward 1307.0, memory_length 2000, epsilon 0.02845276315151042 total_time 722.0\n",
      "episode 7129, reward 1607.0, memory_length 2000, epsilon 0.028310854403266573 total_time 727.0\n",
      "episode 7139, reward 1079.0, memory_length 2000, epsilon 0.028169653427857343 total_time 722.0\n",
      "episode 7149, reward 1584.0, memory_length 2000, epsilon 0.028029156695250978 total_time 721.0\n",
      "episode 7159, reward 1704.0, memory_length 2000, epsilon 0.027889360693021847 total_time 721.0\n",
      "episode 7169, reward 1602.0, memory_length 2000, epsilon 0.027750261926262603 total_time 724.0\n",
      "episode 7179, reward 1401.0, memory_length 2000, epsilon 0.027611856917496857 total_time 730.0\n",
      "episode 7189, reward 1168.0, memory_length 2000, epsilon 0.027474142206592167 total_time 722.0\n",
      "episode 7199, reward 1306.0, memory_length 2000, epsilon 0.027337114350673587 total_time 721.0\n",
      "episode 7209, reward 610.0, memory_length 2000, epsilon 0.02720076992403757 total_time 722.0\n",
      "episode 7219, reward 1708.0, memory_length 2000, epsilon 0.027065105518066374 total_time 725.0\n",
      "episode 7229, reward 1654.0, memory_length 2000, epsilon 0.026930117741142772 total_time 721.0\n",
      "episode 7239, reward 1288.0, memory_length 2000, epsilon 0.026795803218565308 total_time 726.0\n",
      "episode 7249, reward 1311.0, memory_length 2000, epsilon 0.026662158592463917 total_time 724.0\n",
      "episode 7259, reward 1257.0, memory_length 2000, epsilon 0.026529180521716 total_time 721.0\n",
      "episode 7269, reward 1658.0, memory_length 2000, epsilon 0.02639686568186286 total_time 723.0\n",
      "episode 7279, reward 1715.0, memory_length 2000, epsilon 0.026265210765026602 total_time 731.0\n",
      "episode 7289, reward 1559.0, memory_length 2000, epsilon 0.02613421247982744 total_time 721.0\n",
      "episode 7299, reward 1279.0, memory_length 2000, epsilon 0.02600386755130144 total_time 723.0\n",
      "episode 7309, reward 1167.0, memory_length 2000, epsilon 0.02587417272081859 total_time 721.0\n",
      "episode 7319, reward 1044.0, memory_length 2000, epsilon 0.02574512474600138 total_time 731.0\n",
      "episode 7329, reward 1753.0, memory_length 2000, epsilon 0.02561672040064371 total_time 723.0\n",
      "episode 7339, reward 1493.0, memory_length 2000, epsilon 0.025488956474630255 total_time 722.0\n",
      "episode 7349, reward 1576.0, memory_length 2000, epsilon 0.025361829773856225 total_time 724.0\n",
      "episode 7359, reward 1524.0, memory_length 2000, epsilon 0.02523533712014747 total_time 721.0\n",
      "episode 7369, reward 1576.0, memory_length 2000, epsilon 0.02510947535118106 total_time 721.0\n",
      "episode 7379, reward 1316.0, memory_length 2000, epsilon 0.024984241320406206 total_time 724.0\n",
      "episode 7389, reward 1198.0, memory_length 2000, epsilon 0.024859631896965637 total_time 725.0\n",
      "episode 7399, reward 1567.0, memory_length 2000, epsilon 0.02473564396561726 total_time 723.0\n",
      "episode 7409, reward 1497.0, memory_length 2000, epsilon 0.024612274426656346 total_time 725.0\n",
      "episode 7419, reward 1689.0, memory_length 2000, epsilon 0.02448952019583798 total_time 730.0\n",
      "episode 7429, reward 1431.0, memory_length 2000, epsilon 0.024367378204300013 total_time 731.0\n",
      "episode 7439, reward 1525.0, memory_length 2000, epsilon 0.024245845398486288 total_time 721.0\n",
      "episode 7449, reward 1296.0, memory_length 2000, epsilon 0.024124918740070334 total_time 728.0\n",
      "episode 7459, reward 1250.0, memory_length 2000, epsilon 0.024004595205879373 total_time 724.0\n",
      "episode 7469, reward 1396.0, memory_length 2000, epsilon 0.023884871787818816 total_time 724.0\n",
      "episode 7479, reward 999.0, memory_length 2000, epsilon 0.023765745492796957 total_time 725.0\n",
      "episode 7489, reward 1395.0, memory_length 2000, epsilon 0.023647213342650217 total_time 724.0\n",
      "episode 7499, reward 1608.0, memory_length 2000, epsilon 0.023529272374068662 total_time 724.0\n",
      "episode 7509, reward 1434.0, memory_length 2000, epsilon 0.02341191963852195 total_time 722.0\n",
      "episode 7519, reward 1208.0, memory_length 2000, epsilon 0.023295152202185577 total_time 721.0\n",
      "episode 7529, reward 1796.0, memory_length 2000, epsilon 0.023178967145867545 total_time 729.0\n",
      "episode 7539, reward 1086.0, memory_length 2000, epsilon 0.02306336156493539 total_time 725.0\n",
      "episode 7549, reward 1474.0, memory_length 2000, epsilon 0.022948332569243588 total_time 723.0\n",
      "episode 7559, reward 1742.0, memory_length 2000, epsilon 0.02283387728306124 total_time 729.0\n",
      "episode 7569, reward 1313.0, memory_length 2000, epsilon 0.022719992845000238 total_time 724.0\n",
      "episode 7579, reward 1314.0, memory_length 2000, epsilon 0.022606676407943692 total_time 726.0\n",
      "episode 7589, reward 1569.0, memory_length 2000, epsilon 0.022493925138974767 total_time 722.0\n",
      "episode 7599, reward 991.0, memory_length 2000, epsilon 0.022381736219305885 total_time 731.0\n",
      "episode 7609, reward 1320.0, memory_length 2000, epsilon 0.022270106844208205 total_time 724.0\n",
      "episode 7619, reward 1377.0, memory_length 2000, epsilon 0.02215903422294153 total_time 730.0\n",
      "episode 7629, reward 1467.0, memory_length 2000, epsilon 0.022048515578684535 total_time 726.0\n",
      "episode 7639, reward 1329.0, memory_length 2000, epsilon 0.021938548148465385 total_time 721.0\n",
      "episode 7649, reward 1338.0, memory_length 2000, epsilon 0.02182912918309257 total_time 724.0\n",
      "episode 7659, reward 1770.0, memory_length 2000, epsilon 0.021720255947086275 total_time 723.0\n",
      "episode 7669, reward 1549.0, memory_length 2000, epsilon 0.02161192571860991 total_time 724.0\n",
      "episode 7679, reward 1591.0, memory_length 2000, epsilon 0.02150413578940214 total_time 722.0\n",
      "episode 7689, reward 887.0, memory_length 2000, epsilon 0.021396883464709117 total_time 721.0\n",
      "episode 7699, reward 1261.0, memory_length 2000, epsilon 0.02129016606321713 total_time 724.0\n",
      "episode 7709, reward 2003.0, memory_length 2000, epsilon 0.02118398091698558 total_time 730.0\n",
      "episode 7719, reward 1481.0, memory_length 2000, epsilon 0.021078325371380293 total_time 725.0\n",
      "episode 7729, reward 1446.0, memory_length 2000, epsilon 0.020973196785007125 total_time 723.0\n",
      "episode 7739, reward 1243.0, memory_length 2000, epsilon 0.020868592529645933 total_time 730.0\n",
      "episode 7749, reward 1487.0, memory_length 2000, epsilon 0.020764509990184882 total_time 727.0\n",
      "episode 7759, reward 1243.0, memory_length 2000, epsilon 0.020660946564555086 total_time 725.0\n",
      "episode 7769, reward 1433.0, memory_length 2000, epsilon 0.020557899663665488 total_time 721.0\n",
      "episode 7779, reward 1188.0, memory_length 2000, epsilon 0.02045536671133821 total_time 722.0\n",
      "episode 7789, reward 1449.0, memory_length 2000, epsilon 0.020353345144244087 total_time 723.0\n",
      "episode 7799, reward 1227.0, memory_length 2000, epsilon 0.020251832411838658 total_time 722.0\n",
      "episode 7809, reward 1512.0, memory_length 2000, epsilon 0.0201508259762983 total_time 729.0\n",
      "episode 7819, reward 1418.0, memory_length 2000, epsilon 0.020050323312456878 total_time 726.0\n",
      "episode 7829, reward 1334.0, memory_length 2000, epsilon 0.019950321907742555 total_time 724.0\n",
      "episode 7839, reward 1265.0, memory_length 2000, epsilon 0.019850819262114995 total_time 733.0\n",
      "episode 7849, reward 1481.0, memory_length 2000, epsilon 0.019751812888002897 total_time 730.0\n",
      "episode 7859, reward 1405.0, memory_length 2000, epsilon 0.019653300310241737 total_time 727.0\n",
      "episode 7869, reward 1427.0, memory_length 2000, epsilon 0.019555279066011948 total_time 725.0\n",
      "episode 7879, reward 1472.0, memory_length 2000, epsilon 0.0194577467047773 total_time 730.0\n",
      "episode 7889, reward 1667.0, memory_length 2000, epsilon 0.01936070078822371 total_time 722.0\n",
      "episode 7899, reward 1247.0, memory_length 2000, epsilon 0.019264138890198193 total_time 730.0\n",
      "episode 7909, reward 1517.0, memory_length 2000, epsilon 0.019168058596648274 total_time 726.0\n",
      "episode 7919, reward 1051.0, memory_length 2000, epsilon 0.0190724575055616 total_time 724.0\n",
      "episode 7929, reward 1321.0, memory_length 2000, epsilon 0.018977333226905934 total_time 726.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7939, reward 1329.0, memory_length 2000, epsilon 0.018882683382569338 total_time 723.0\n",
      "episode 7949, reward 1122.0, memory_length 2000, epsilon 0.018788505606300788 total_time 723.0\n",
      "episode 7959, reward 1357.0, memory_length 2000, epsilon 0.01869479754365095 total_time 725.0\n",
      "episode 7969, reward 1260.0, memory_length 2000, epsilon 0.0186015568519134 total_time 731.0\n",
      "episode 7979, reward 1306.0, memory_length 2000, epsilon 0.018508781200065983 total_time 724.0\n",
      "episode 7989, reward 1062.0, memory_length 2000, epsilon 0.018416468268712564 total_time 725.0\n",
      "episode 7999, reward 1616.0, memory_length 2000, epsilon 0.018324615750025048 total_time 724.0\n",
      "Saving Model 8000\n",
      "episode 8009, reward 1134.0, memory_length 2000, epsilon 0.018233221347685697 total_time 731.0\n",
      "episode 8019, reward 1586.0, memory_length 2000, epsilon 0.018142282776829687 total_time 725.0\n",
      "episode 8029, reward 1217.0, memory_length 2000, epsilon 0.01805179776398801 total_time 726.0\n",
      "episode 8039, reward 1202.0, memory_length 2000, epsilon 0.01796176404703063 total_time 728.0\n",
      "episode 8049, reward 1142.0, memory_length 2000, epsilon 0.017872179375109938 total_time 723.0\n",
      "episode 8059, reward 1346.0, memory_length 2000, epsilon 0.01778304150860445 total_time 726.0\n",
      "episode 8069, reward 1496.0, memory_length 2000, epsilon 0.01769434821906289 total_time 723.0\n",
      "episode 8079, reward 1247.0, memory_length 2000, epsilon 0.017606097289148394 total_time 727.0\n",
      "episode 8089, reward 1664.0, memory_length 2000, epsilon 0.017518286512583105 total_time 721.0\n",
      "episode 8099, reward 1044.0, memory_length 2000, epsilon 0.01743091369409305 total_time 723.0\n",
      "episode 8109, reward 1200.0, memory_length 2000, epsilon 0.017343976649353204 total_time 721.0\n",
      "episode 8119, reward 1508.0, memory_length 2000, epsilon 0.017257473204932924 total_time 724.0\n",
      "episode 8129, reward 1556.0, memory_length 2000, epsilon 0.017171401198241596 total_time 722.0\n",
      "episode 8139, reward 1418.0, memory_length 2000, epsilon 0.017085758477474566 total_time 724.0\n",
      "episode 8149, reward 1387.0, memory_length 2000, epsilon 0.017000542901559345 total_time 725.0\n",
      "episode 8159, reward 1994.0, memory_length 2000, epsilon 0.016915752340102123 total_time 725.0\n",
      "episode 8169, reward 1429.0, memory_length 2000, epsilon 0.01683138467333443 total_time 721.0\n",
      "episode 8179, reward 1638.0, memory_length 2000, epsilon 0.016747437792060213 total_time 722.0\n",
      "episode 8189, reward 1449.0, memory_length 2000, epsilon 0.01666390959760305 total_time 728.0\n",
      "episode 8199, reward 1607.0, memory_length 2000, epsilon 0.016580798001753747 total_time 730.0\n",
      "episode 8209, reward 1305.0, memory_length 2000, epsilon 0.016498100926718072 total_time 733.0\n",
      "episode 8219, reward 1302.0, memory_length 2000, epsilon 0.016415816305064838 total_time 723.0\n",
      "episode 8229, reward 1958.0, memory_length 2000, epsilon 0.016333942079674212 total_time 726.0\n",
      "episode 8239, reward 1846.0, memory_length 2000, epsilon 0.016252476203686316 total_time 729.0\n",
      "episode 8249, reward 1241.0, memory_length 2000, epsilon 0.016171416640449996 total_time 723.0\n",
      "episode 8259, reward 1483.0, memory_length 2000, epsilon 0.01609076136347195 total_time 721.0\n",
      "episode 8269, reward 1268.0, memory_length 2000, epsilon 0.016010508356366054 total_time 726.0\n",
      "episode 8279, reward 1311.0, memory_length 2000, epsilon 0.015930655612802946 total_time 725.0\n",
      "episode 8289, reward 1479.0, memory_length 2000, epsilon 0.01585120113645988 total_time 723.0\n",
      "episode 8299, reward 1287.0, memory_length 2000, epsilon 0.01577214294097081 total_time 728.0\n",
      "episode 8309, reward 1612.0, memory_length 2000, epsilon 0.015693479049876717 total_time 722.0\n",
      "episode 8319, reward 1928.0, memory_length 2000, epsilon 0.015615207496576253 total_time 724.0\n",
      "episode 8329, reward 1526.0, memory_length 2000, epsilon 0.015537326324276499 total_time 732.0\n",
      "episode 8339, reward 1463.0, memory_length 2000, epsilon 0.015459833585944086 total_time 724.0\n",
      "episode 8349, reward 1612.0, memory_length 2000, epsilon 0.015382727344256523 total_time 721.0\n",
      "episode 8359, reward 1599.0, memory_length 2000, epsilon 0.015306005671553751 total_time 721.0\n",
      "episode 8369, reward 1249.0, memory_length 2000, epsilon 0.015229666649789956 total_time 726.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "score_tracked = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    track_reward = False\n",
    "\n",
    "    # reset at the start of each episode\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
    "    initial_state = env.state_init\n",
    "\n",
    "\n",
    "    total_time = 0  # Total time driver rode in this episode\n",
    "    while not done:\n",
    "        # 1. Get a list of the ride requests driver got.\n",
    "        possible_actions_indices, actions = env.requests(state)\n",
    "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
    "        action = agent.get_action(state, possible_actions_indices, actions)\n",
    "\n",
    "        # 3. Evaluate your reward and next state\n",
    "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
    "        # 4. Total time driver rode in this episode\n",
    "        total_time += step_time\n",
    "        if (total_time > episode_time):\n",
    "            # if ride does not complete in stipu;ated time skip\n",
    "            # it and move to next episode.\n",
    "            done = True\n",
    "        else:\n",
    "            # 5. Append the experience to the memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # 6. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            # 7. Keep a track of rewards, Q-values, loss\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "\n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "\n",
    "    # every 10 episodes:\n",
    "    if ((episode + 1) % 10 == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, total_time))\n",
    "    # Save the Q_value of the state, action pair we are tracking\n",
    "    if ((episode + 1) % 5 == 0):\n",
    "        agent.save_tracking_states()\n",
    "\n",
    "    # Total rewards per episode\n",
    "    score_tracked.append(score)\n",
    "\n",
    "    if(episode % 1000 == 0):\n",
    "        print(\"Saving Model {}\".format(episode))\n",
    "        agent.save(name=\"model_weights.h5\")\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LB5JiWZtQ6JZ"
   },
   "outputs": [],
   "source": [
    "agent.save(name=\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J34clOaAQ6JY"
   },
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahZvx4XpQ6JZ"
   },
   "outputs": [],
   "source": [
    "agent.states_tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGYxuCR3Q6JZ"
   },
   "outputs": [],
   "source": [
    "state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6JWcayI0wNq"
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Q_value for state [0,0,0]  action (0,2)')\n",
    "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
    "plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8-rEJ1N08Qa"
   },
   "outputs": [],
   "source": [
    "score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EiYguio09uy"
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Rewards per episode')\n",
    "xaxis = np.asarray(range(0, len(score_tracked_sample)))\n",
    "plt.plot(xaxis,np.asarray(score_tracked_sample))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stD5OunxQ6Jg"
   },
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Qc7xMkRQ6Jg"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tm6xR85Q6Jg"
   },
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg5jfTOwQ6Jh"
   },
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "DQN_Agent_Arch1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
