{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon = 1\n",
    "        # the epsilon decay rate has been estimated from the graph shown at the end of the file\n",
    "        self.epsilon_decay = 0.9991\n",
    "        #self.epsilon_decay = 0.993\n",
    "        #self.epsilon_decay = 0.992\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.batch_size = 32        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        # hidden layers\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state,env):\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment\n",
    "        #print(\"Get Action state is \",state)\n",
    "        possible_actions_index,actions = env.requests(state) # Find possible action indexes and append 0\n",
    "        possible_actions_index.append(0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from all possible actions\n",
    "            # Give a random action only amongst possible action\n",
    "            return random.sample(possible_actions_index,1)[0]\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = state.reshape(1, self.state_size)\n",
    "            q_value = self.model.predict(state)\n",
    "            # Give action with max q_value only amongst possible action\n",
    "            return np.where(q_value[0] == np.max(np.array([q_value[0][i] for i in possible_actions_index])))[0][0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def append_sample(self, state, action, reward, next_state,done):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state,done))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self,env):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size)) # write here\n",
    "            update_input = np.zeros((self.batch_size, self.state_size)) # write here\n",
    "            \n",
    "            actions, rewards, done = [], [], []\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                \n",
    "                update_input[i] = state\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                done.append(done_boolean)\n",
    "                update_output[i] = next_state\n",
    "                \n",
    "                # Write your code from here\n",
    "                \n",
    "            # 1. Predict the target from earlier model           \n",
    "            target = self.model.predict(update_input)\n",
    "            \n",
    "            # 2. Get the target for the Q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "                \n",
    "                #3. Update your 'update_output' and 'update_input' batch\n",
    "            for i in range(self.batch_size):\n",
    "                # Find possible actions from next state\n",
    "                next_possible_actions_index,_ = env.requests(update_output[i])\n",
    "                next_possible_actions_index.append(0)\n",
    "                if not done[i]:\n",
    "                    # Only take the max q_value from valid actions from next state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(np.array([target_qval[i][j] for j in next_possible_actions_index]))\n",
    "                else:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                \n",
    "                \n",
    "        # 4. Fit your model and track the loss values\n",
    "            #print(\"Training Model\")\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            #print(\"Model Training Model\")\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State is  [1, 20, 4]\n",
      "WARNING:tensorflow:From /Users/sumitdwivedi/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "episode 0, reward -347.0, memory_length 134, epsilon 0.9991, time 734.0, rides 133\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 1, reward -312.0, memory_length 254, epsilon 0.9982008099999999, time 728.0, rides 119\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 2, reward -245.0, memory_length 381, epsilon 0.9973024292709999, time 736.0, rides 126\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 3, reward -489.0, memory_length 505, epsilon 0.996404857084656, time 731.0, rides 123\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 4, reward -268.0, memory_length 626, epsilon 0.9955080927132798, time 726.0, rides 120\n",
      "Initial State is  [0, 13, 3]\n",
      "episode 5, reward -103.0, memory_length 753, epsilon 0.9946121354298378, time 733.0, rides 126\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 6, reward -315.0, memory_length 875, epsilon 0.993716984507951, time 723.0, rides 121\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 7, reward -465.0, memory_length 1003, epsilon 0.9928226392218938, time 726.0, rides 127\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 8, reward -156.0, memory_length 1129, epsilon 0.9919290988465941, time 726.0, rides 125\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 9, reward -128.0, memory_length 1276, epsilon 0.9910363626576322, time 738.0, rides 146\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 10, reward -231.0, memory_length 1404, epsilon 0.9901444299312403, time 732.0, rides 127\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 11, reward 5.0, memory_length 1542, epsilon 0.9892532999443022, time 723.0, rides 137\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 12, reward -378.0, memory_length 1667, epsilon 0.9883629719743523, time 728.0, rides 124\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 13, reward -222.0, memory_length 1797, epsilon 0.9874734452995754, time 729.0, rides 129\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 14, reward -379.0, memory_length 1929, epsilon 0.9865847191988057, time 725.0, rides 131\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 15, reward -276.0, memory_length 2000, epsilon 0.9856967929515268, time 732.0, rides 116\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 16, reward -265.0, memory_length 2000, epsilon 0.9848096658378704, time 732.0, rides 129\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 17, reward -125.0, memory_length 2000, epsilon 0.9839233371386163, time 724.0, rides 128\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 18, reward 259.0, memory_length 2000, epsilon 0.9830378061351915, time 730.0, rides 139\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 19, reward -302.0, memory_length 2000, epsilon 0.9821530721096698, time 727.0, rides 139\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 20, reward -343.0, memory_length 2000, epsilon 0.9812691343447711, time 725.0, rides 118\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 21, reward 46.0, memory_length 2000, epsilon 0.9803859921238608, time 733.0, rides 134\n",
      "Initial State is  [3, 6, 3]\n",
      "episode 22, reward -440.0, memory_length 2000, epsilon 0.9795036447309493, time 743.0, rides 124\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 23, reward -355.0, memory_length 2000, epsilon 0.9786220914506915, time 729.0, rides 129\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 24, reward -560.0, memory_length 2000, epsilon 0.9777413315683858, time 737.0, rides 136\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 25, reward -130.0, memory_length 2000, epsilon 0.9768613643699743, time 728.0, rides 131\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 26, reward -328.0, memory_length 2000, epsilon 0.9759821891420413, time 730.0, rides 141\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 27, reward -388.0, memory_length 2000, epsilon 0.9751038051718134, time 725.0, rides 116\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 28, reward -212.0, memory_length 2000, epsilon 0.9742262117471587, time 728.0, rides 134\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 29, reward -143.0, memory_length 2000, epsilon 0.9733494081565863, time 726.0, rides 127\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 30, reward -393.0, memory_length 2000, epsilon 0.9724733936892453, time 730.0, rides 132\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 31, reward -425.0, memory_length 2000, epsilon 0.971598167634925, time 744.0, rides 126\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 32, reward -496.0, memory_length 2000, epsilon 0.9707237292840536, time 723.0, rides 133\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 33, reward -565.0, memory_length 2000, epsilon 0.969850077927698, time 733.0, rides 129\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 34, reward -314.0, memory_length 2000, epsilon 0.968977212857563, time 730.0, rides 144\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 35, reward -329.0, memory_length 2000, epsilon 0.9681051333659911, time 732.0, rides 115\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 36, reward -398.0, memory_length 2000, epsilon 0.9672338387459617, time 733.0, rides 131\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 37, reward -126.0, memory_length 2000, epsilon 0.9663633282910903, time 727.0, rides 137\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 38, reward -145.0, memory_length 2000, epsilon 0.9654936012956283, time 726.0, rides 127\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 39, reward -316.0, memory_length 2000, epsilon 0.9646246570544622, time 735.0, rides 121\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 40, reward -213.0, memory_length 2000, epsilon 0.9637564948631132, time 725.0, rides 125\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 41, reward -357.0, memory_length 2000, epsilon 0.9628891140177364, time 726.0, rides 126\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 42, reward -160.0, memory_length 2000, epsilon 0.9620225138151204, time 726.0, rides 119\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 43, reward -443.0, memory_length 2000, epsilon 0.9611566935526867, time 737.0, rides 115\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 44, reward -69.0, memory_length 2000, epsilon 0.9602916525284894, time 722.0, rides 117\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 45, reward -434.0, memory_length 2000, epsilon 0.9594273900412137, time 728.0, rides 129\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 46, reward -227.0, memory_length 2000, epsilon 0.9585639053901766, time 724.0, rides 134\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 47, reward 168.0, memory_length 2000, epsilon 0.9577011978753254, time 721.0, rides 119\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 48, reward -189.0, memory_length 2000, epsilon 0.9568392667972375, time 727.0, rides 128\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 49, reward -430.0, memory_length 2000, epsilon 0.95597811145712, time 731.0, rides 113\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 50, reward -541.0, memory_length 2000, epsilon 0.9551177311568085, time 733.0, rides 127\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 51, reward -362.0, memory_length 2000, epsilon 0.9542581251987674, time 729.0, rides 144\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 52, reward -184.0, memory_length 2000, epsilon 0.9533992928860885, time 740.0, rides 116\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 53, reward -263.0, memory_length 2000, epsilon 0.952541233522491, time 729.0, rides 132\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 54, reward -410.0, memory_length 2000, epsilon 0.9516839464123207, time 728.0, rides 119\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 55, reward 28.0, memory_length 2000, epsilon 0.9508274308605495, time 733.0, rides 144\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 56, reward -180.0, memory_length 2000, epsilon 0.949971686172775, time 723.0, rides 117\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 57, reward -428.0, memory_length 2000, epsilon 0.9491167116552195, time 738.0, rides 137\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 58, reward -175.0, memory_length 2000, epsilon 0.9482625066147298, time 734.0, rides 125\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 59, reward -328.0, memory_length 2000, epsilon 0.9474090703587765, time 724.0, rides 117\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 60, reward -358.0, memory_length 2000, epsilon 0.9465564021954537, time 725.0, rides 123\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 61, reward -433.0, memory_length 2000, epsilon 0.9457045014334777, time 725.0, rides 132\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 62, reward -345.0, memory_length 2000, epsilon 0.9448533673821876, time 729.0, rides 123\n",
      "Initial State is  [1, 7, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 63, reward -31.0, memory_length 2000, epsilon 0.9440029993515436, time 726.0, rides 123\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 64, reward -275.0, memory_length 2000, epsilon 0.9431533966521273, time 733.0, rides 124\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 65, reward 78.0, memory_length 2000, epsilon 0.9423045585951404, time 725.0, rides 127\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 66, reward 166.0, memory_length 2000, epsilon 0.9414564844924047, time 734.0, rides 120\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 67, reward -218.0, memory_length 2000, epsilon 0.9406091736563615, time 728.0, rides 124\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 68, reward -344.0, memory_length 2000, epsilon 0.9397626254000708, time 729.0, rides 119\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 69, reward -162.0, memory_length 2000, epsilon 0.9389168390372108, time 728.0, rides 123\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 70, reward -64.0, memory_length 2000, epsilon 0.9380718138820773, time 724.0, rides 127\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 71, reward -360.0, memory_length 2000, epsilon 0.9372275492495834, time 733.0, rides 114\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 72, reward -158.0, memory_length 2000, epsilon 0.9363840444552588, time 731.0, rides 123\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 73, reward -413.0, memory_length 2000, epsilon 0.9355412988152491, time 739.0, rides 139\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 74, reward -39.0, memory_length 2000, epsilon 0.9346993116463154, time 728.0, rides 126\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 75, reward -291.0, memory_length 2000, epsilon 0.9338580822658337, time 730.0, rides 144\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 76, reward -39.0, memory_length 2000, epsilon 0.9330176099917944, time 729.0, rides 129\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 77, reward -192.0, memory_length 2000, epsilon 0.9321778941428017, time 722.0, rides 137\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 78, reward -333.0, memory_length 2000, epsilon 0.9313389340380732, time 725.0, rides 132\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 79, reward -336.0, memory_length 2000, epsilon 0.930500728997439, time 727.0, rides 128\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 80, reward -274.0, memory_length 2000, epsilon 0.9296632783413412, time 729.0, rides 121\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 81, reward -241.0, memory_length 2000, epsilon 0.928826581390834, time 721.0, rides 115\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 82, reward -296.0, memory_length 2000, epsilon 0.9279906374675821, time 728.0, rides 127\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 83, reward -498.0, memory_length 2000, epsilon 0.9271554458938613, time 736.0, rides 116\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 84, reward -129.0, memory_length 2000, epsilon 0.9263210059925568, time 738.0, rides 134\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 85, reward -281.0, memory_length 2000, epsilon 0.9254873170871636, time 730.0, rides 118\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 86, reward -288.0, memory_length 2000, epsilon 0.9246543785017851, time 722.0, rides 142\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 87, reward -324.0, memory_length 2000, epsilon 0.9238221895611335, time 736.0, rides 132\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 88, reward -10.0, memory_length 2000, epsilon 0.9229907495905284, time 728.0, rides 126\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 89, reward -147.0, memory_length 2000, epsilon 0.922160057915897, time 724.0, rides 134\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 90, reward -156.0, memory_length 2000, epsilon 0.9213301138637726, time 723.0, rides 120\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 91, reward -142.0, memory_length 2000, epsilon 0.9205009167612952, time 733.0, rides 135\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 92, reward -186.0, memory_length 2000, epsilon 0.91967246593621, time 728.0, rides 130\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 93, reward -372.0, memory_length 2000, epsilon 0.9188447607168674, time 726.0, rides 144\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 94, reward -207.0, memory_length 2000, epsilon 0.9180178004322221, time 736.0, rides 139\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 95, reward -413.0, memory_length 2000, epsilon 0.9171915844118331, time 731.0, rides 134\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 96, reward -410.0, memory_length 2000, epsilon 0.9163661119858625, time 737.0, rides 121\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 97, reward -210.0, memory_length 2000, epsilon 0.9155413824850752, time 729.0, rides 124\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 98, reward -455.0, memory_length 2000, epsilon 0.9147173952408386, time 728.0, rides 133\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 99, reward -576.0, memory_length 2000, epsilon 0.9138941495851218, time 729.0, rides 128\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 100, reward -383.0, memory_length 2000, epsilon 0.9130716448504952, time 722.0, rides 122\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 101, reward -365.0, memory_length 2000, epsilon 0.9122498803701298, time 725.0, rides 131\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 102, reward -90.0, memory_length 2000, epsilon 0.9114288554777966, time 729.0, rides 118\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 103, reward -225.0, memory_length 2000, epsilon 0.9106085695078666, time 729.0, rides 141\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 104, reward -264.0, memory_length 2000, epsilon 0.9097890217953095, time 734.0, rides 123\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 105, reward -321.0, memory_length 2000, epsilon 0.9089702116756937, time 728.0, rides 135\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 106, reward -444.0, memory_length 2000, epsilon 0.9081521384851856, time 738.0, rides 142\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 107, reward -23.0, memory_length 2000, epsilon 0.9073348015605489, time 736.0, rides 126\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 108, reward -208.0, memory_length 2000, epsilon 0.9065182002391444, time 724.0, rides 125\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 109, reward -81.0, memory_length 2000, epsilon 0.9057023338589292, time 729.0, rides 122\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 110, reward -233.0, memory_length 2000, epsilon 0.9048872017584562, time 727.0, rides 120\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 111, reward -75.0, memory_length 2000, epsilon 0.9040728032768736, time 728.0, rides 120\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 112, reward -495.0, memory_length 2000, epsilon 0.9032591377539244, time 729.0, rides 128\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 113, reward -627.0, memory_length 2000, epsilon 0.9024462045299458, time 733.0, rides 122\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 114, reward -214.0, memory_length 2000, epsilon 0.9016340029458689, time 738.0, rides 128\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 115, reward -200.0, memory_length 2000, epsilon 0.9008225323432176, time 724.0, rides 126\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 116, reward 3.0, memory_length 2000, epsilon 0.9000117920641088, time 723.0, rides 130\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 117, reward -337.0, memory_length 2000, epsilon 0.8992017814512511, time 732.0, rides 132\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 118, reward -153.0, memory_length 2000, epsilon 0.8983924998479449, time 731.0, rides 119\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 119, reward 114.0, memory_length 2000, epsilon 0.8975839465980817, time 732.0, rides 133\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 120, reward -197.0, memory_length 2000, epsilon 0.8967761210461435, time 730.0, rides 132\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 121, reward -496.0, memory_length 2000, epsilon 0.8959690225372019, time 732.0, rides 141\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 122, reward -161.0, memory_length 2000, epsilon 0.8951626504169184, time 732.0, rides 125\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 123, reward -209.0, memory_length 2000, epsilon 0.8943570040315432, time 746.0, rides 126\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 124, reward -109.0, memory_length 2000, epsilon 0.8935520827279148, time 737.0, rides 120\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 125, reward -178.0, memory_length 2000, epsilon 0.8927478858534597, time 725.0, rides 110\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 126, reward -206.0, memory_length 2000, epsilon 0.8919444127561915, time 736.0, rides 141\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 127, reward -328.0, memory_length 2000, epsilon 0.891141662784711, time 741.0, rides 126\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 128, reward -105.0, memory_length 2000, epsilon 0.8903396352882047, time 733.0, rides 123\n",
      "Initial State is  [2, 10, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 129, reward -348.0, memory_length 2000, epsilon 0.8895383296164453, time 728.0, rides 120\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 130, reward -303.0, memory_length 2000, epsilon 0.8887377451197905, time 733.0, rides 124\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 131, reward -199.0, memory_length 2000, epsilon 0.8879378811491827, time 724.0, rides 134\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 132, reward 14.0, memory_length 2000, epsilon 0.8871387370561484, time 729.0, rides 112\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 133, reward -449.0, memory_length 2000, epsilon 0.8863403121927979, time 739.0, rides 118\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 134, reward -280.0, memory_length 2000, epsilon 0.8855426059118243, time 728.0, rides 144\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 135, reward -250.0, memory_length 2000, epsilon 0.8847456175665037, time 729.0, rides 132\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 136, reward -193.0, memory_length 2000, epsilon 0.8839493465106939, time 733.0, rides 128\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 137, reward 27.0, memory_length 2000, epsilon 0.8831537920988343, time 739.0, rides 120\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 138, reward -346.0, memory_length 2000, epsilon 0.8823589536859453, time 734.0, rides 121\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 139, reward -130.0, memory_length 2000, epsilon 0.8815648306276279, time 729.0, rides 105\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 140, reward -316.0, memory_length 2000, epsilon 0.880771422280063, time 726.0, rides 129\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 141, reward -292.0, memory_length 2000, epsilon 0.879978728000011, time 730.0, rides 141\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 142, reward -230.0, memory_length 2000, epsilon 0.8791867471448109, time 733.0, rides 119\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 143, reward -42.0, memory_length 2000, epsilon 0.8783954790723806, time 727.0, rides 136\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 144, reward -327.0, memory_length 2000, epsilon 0.8776049231412154, time 728.0, rides 119\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 145, reward -239.0, memory_length 2000, epsilon 0.8768150787103883, time 724.0, rides 110\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 146, reward -100.0, memory_length 2000, epsilon 0.876025945139549, time 733.0, rides 117\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 147, reward -348.0, memory_length 2000, epsilon 0.8752375217889234, time 734.0, rides 133\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 148, reward -137.0, memory_length 2000, epsilon 0.8744498080193134, time 733.0, rides 120\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 149, reward -51.0, memory_length 2000, epsilon 0.873662803192096, time 740.0, rides 114\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 150, reward -432.0, memory_length 2000, epsilon 0.8728765066692231, time 724.0, rides 127\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 151, reward -625.0, memory_length 2000, epsilon 0.8720909178132208, time 730.0, rides 136\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 152, reward 52.0, memory_length 2000, epsilon 0.8713060359871889, time 730.0, rides 119\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 153, reward -292.0, memory_length 2000, epsilon 0.8705218605548004, time 727.0, rides 136\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 154, reward -281.0, memory_length 2000, epsilon 0.869738390880301, time 734.0, rides 123\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 155, reward -353.0, memory_length 2000, epsilon 0.8689556263285088, time 730.0, rides 137\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 156, reward -159.0, memory_length 2000, epsilon 0.8681735662648131, time 724.0, rides 127\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 157, reward -235.0, memory_length 2000, epsilon 0.8673922100551748, time 739.0, rides 125\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 158, reward -433.0, memory_length 2000, epsilon 0.8666115570661251, time 727.0, rides 144\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 159, reward -395.0, memory_length 2000, epsilon 0.8658316066647657, time 737.0, rides 134\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 160, reward 58.0, memory_length 2000, epsilon 0.8650523582187674, time 733.0, rides 125\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 161, reward -100.0, memory_length 2000, epsilon 0.8642738110963705, time 728.0, rides 126\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 162, reward -133.0, memory_length 2000, epsilon 0.8634959646663837, time 724.0, rides 126\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 163, reward -133.0, memory_length 2000, epsilon 0.8627188182981839, time 730.0, rides 131\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 164, reward -227.0, memory_length 2000, epsilon 0.8619423713617155, time 724.0, rides 129\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 165, reward -425.0, memory_length 2000, epsilon 0.8611666232274899, time 734.0, rides 128\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 166, reward -341.0, memory_length 2000, epsilon 0.8603915732665852, time 727.0, rides 146\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 167, reward -320.0, memory_length 2000, epsilon 0.8596172208506453, time 727.0, rides 131\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 168, reward -489.0, memory_length 2000, epsilon 0.8588435653518797, time 723.0, rides 135\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 169, reward -330.0, memory_length 2000, epsilon 0.858070606143063, time 735.0, rides 126\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 170, reward -299.0, memory_length 2000, epsilon 0.8572983425975342, time 738.0, rides 132\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 171, reward -359.0, memory_length 2000, epsilon 0.8565267740891964, time 729.0, rides 126\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 172, reward -209.0, memory_length 2000, epsilon 0.8557558999925161, time 732.0, rides 137\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 173, reward 70.0, memory_length 2000, epsilon 0.8549857196825228, time 735.0, rides 136\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 174, reward -278.0, memory_length 2000, epsilon 0.8542162325348085, time 728.0, rides 123\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 175, reward -384.0, memory_length 2000, epsilon 0.8534474379255271, time 737.0, rides 128\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 176, reward -26.0, memory_length 2000, epsilon 0.8526793352313942, time 734.0, rides 127\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 177, reward -93.0, memory_length 2000, epsilon 0.851911923829686, time 722.0, rides 128\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 178, reward 117.0, memory_length 2000, epsilon 0.8511452030982393, time 723.0, rides 119\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 179, reward -92.0, memory_length 2000, epsilon 0.8503791724154508, time 724.0, rides 119\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 180, reward -325.0, memory_length 2000, epsilon 0.8496138311602769, time 735.0, rides 126\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 181, reward -226.0, memory_length 2000, epsilon 0.8488491787122326, time 732.0, rides 141\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 182, reward -526.0, memory_length 2000, epsilon 0.8480852144513916, time 722.0, rides 136\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 183, reward -273.0, memory_length 2000, epsilon 0.8473219377583854, time 722.0, rides 131\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 184, reward -235.0, memory_length 2000, epsilon 0.8465593480144028, time 737.0, rides 117\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 185, reward -332.0, memory_length 2000, epsilon 0.8457974446011898, time 729.0, rides 134\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 186, reward -128.0, memory_length 2000, epsilon 0.8450362269010487, time 731.0, rides 121\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 187, reward -348.0, memory_length 2000, epsilon 0.8442756942968378, time 728.0, rides 117\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 188, reward -178.0, memory_length 2000, epsilon 0.8435158461719706, time 731.0, rides 134\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 189, reward -306.0, memory_length 2000, epsilon 0.8427566819104159, time 725.0, rides 120\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 190, reward -569.0, memory_length 2000, epsilon 0.8419982008966965, time 732.0, rides 126\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 191, reward -307.0, memory_length 2000, epsilon 0.8412404025158895, time 731.0, rides 123\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 192, reward -21.0, memory_length 2000, epsilon 0.8404832861536252, time 732.0, rides 136\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 193, reward 54.0, memory_length 2000, epsilon 0.8397268511960869, time 723.0, rides 122\n",
      "Initial State is  [0, 11, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 194, reward -23.0, memory_length 2000, epsilon 0.8389710970300104, time 737.0, rides 119\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 195, reward 186.0, memory_length 2000, epsilon 0.8382160230426834, time 742.0, rides 123\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 196, reward -6.0, memory_length 2000, epsilon 0.837461628621945, time 722.0, rides 130\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 197, reward 245.0, memory_length 2000, epsilon 0.8367079131561852, time 724.0, rides 123\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 198, reward -376.0, memory_length 2000, epsilon 0.8359548760343446, time 723.0, rides 127\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 199, reward -189.0, memory_length 2000, epsilon 0.8352025166459137, time 741.0, rides 124\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 200, reward -113.0, memory_length 2000, epsilon 0.8344508343809324, time 723.0, rides 132\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 201, reward -144.0, memory_length 2000, epsilon 0.8336998286299895, time 726.0, rides 125\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 202, reward -91.0, memory_length 2000, epsilon 0.8329494987842225, time 728.0, rides 132\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 203, reward -278.0, memory_length 2000, epsilon 0.8321998442353167, time 734.0, rides 120\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 204, reward 154.0, memory_length 2000, epsilon 0.8314508643755049, time 726.0, rides 116\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 205, reward -115.0, memory_length 2000, epsilon 0.8307025585975669, time 737.0, rides 126\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 206, reward -287.0, memory_length 2000, epsilon 0.8299549262948291, time 729.0, rides 119\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 207, reward -382.0, memory_length 2000, epsilon 0.8292079668611638, time 730.0, rides 126\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 208, reward -27.0, memory_length 2000, epsilon 0.8284616796909887, time 735.0, rides 127\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 209, reward -262.0, memory_length 2000, epsilon 0.8277160641792668, time 727.0, rides 130\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 210, reward -17.0, memory_length 2000, epsilon 0.8269711197215055, time 722.0, rides 129\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 211, reward -135.0, memory_length 2000, epsilon 0.8262268457137562, time 739.0, rides 123\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 212, reward -394.0, memory_length 2000, epsilon 0.8254832415526138, time 726.0, rides 132\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 213, reward -103.0, memory_length 2000, epsilon 0.8247403066352164, time 729.0, rides 131\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 214, reward -313.0, memory_length 2000, epsilon 0.8239980403592446, time 732.0, rides 126\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 215, reward -313.0, memory_length 2000, epsilon 0.8232564421229213, time 727.0, rides 134\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 216, reward -200.0, memory_length 2000, epsilon 0.8225155113250107, time 737.0, rides 144\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 217, reward -1.0, memory_length 2000, epsilon 0.8217752473648181, time 731.0, rides 134\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 218, reward -241.0, memory_length 2000, epsilon 0.8210356496421898, time 726.0, rides 141\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 219, reward -172.0, memory_length 2000, epsilon 0.8202967175575118, time 725.0, rides 127\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 220, reward -312.0, memory_length 2000, epsilon 0.81955845051171, time 728.0, rides 137\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 221, reward -94.0, memory_length 2000, epsilon 0.8188208479062494, time 746.0, rides 136\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 222, reward -219.0, memory_length 2000, epsilon 0.8180839091431338, time 728.0, rides 122\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 223, reward -197.0, memory_length 2000, epsilon 0.8173476336249049, time 728.0, rides 125\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 224, reward -8.0, memory_length 2000, epsilon 0.8166120207546425, time 724.0, rides 128\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 225, reward -150.0, memory_length 2000, epsilon 0.8158770699359633, time 728.0, rides 125\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 226, reward -214.0, memory_length 2000, epsilon 0.8151427805730209, time 742.0, rides 136\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 227, reward -51.0, memory_length 2000, epsilon 0.8144091520705052, time 730.0, rides 135\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 228, reward -142.0, memory_length 2000, epsilon 0.8136761838336418, time 741.0, rides 132\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 229, reward -43.0, memory_length 2000, epsilon 0.8129438752681916, time 730.0, rides 116\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 230, reward -235.0, memory_length 2000, epsilon 0.8122122257804502, time 736.0, rides 132\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 231, reward -375.0, memory_length 2000, epsilon 0.8114812347772478, time 733.0, rides 131\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 232, reward -253.0, memory_length 2000, epsilon 0.8107509016659482, time 739.0, rides 139\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 233, reward -208.0, memory_length 2000, epsilon 0.8100212258544488, time 741.0, rides 139\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 234, reward -182.0, memory_length 2000, epsilon 0.8092922067511797, time 723.0, rides 115\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 235, reward -83.0, memory_length 2000, epsilon 0.8085638437651037, time 732.0, rides 117\n",
      "Initial State is  [4, 8, 1]\n",
      "episode 236, reward -181.0, memory_length 2000, epsilon 0.8078361363057152, time 726.0, rides 142\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 237, reward -347.0, memory_length 2000, epsilon 0.80710908378304, time 735.0, rides 134\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 238, reward -230.0, memory_length 2000, epsilon 0.8063826856076353, time 721.0, rides 129\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 239, reward -222.0, memory_length 2000, epsilon 0.8056569411905884, time 728.0, rides 139\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 240, reward -78.0, memory_length 2000, epsilon 0.8049318499435169, time 726.0, rides 129\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 241, reward 157.0, memory_length 2000, epsilon 0.8042074112785677, time 732.0, rides 116\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 242, reward -25.0, memory_length 2000, epsilon 0.8034836246084169, time 726.0, rides 124\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 243, reward 90.0, memory_length 2000, epsilon 0.8027604893462693, time 725.0, rides 117\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 244, reward -275.0, memory_length 2000, epsilon 0.8020380049058576, time 733.0, rides 130\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 245, reward -38.0, memory_length 2000, epsilon 0.8013161707014423, time 725.0, rides 121\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 246, reward 55.0, memory_length 2000, epsilon 0.800594986147811, time 730.0, rides 122\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 247, reward -186.0, memory_length 2000, epsilon 0.799874450660278, time 726.0, rides 127\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 248, reward -163.0, memory_length 2000, epsilon 0.7991545636546837, time 721.0, rides 131\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 249, reward 35.0, memory_length 2000, epsilon 0.7984353245473945, time 732.0, rides 126\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 250, reward -171.0, memory_length 2000, epsilon 0.7977167327553019, time 724.0, rides 134\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 251, reward -5.0, memory_length 2000, epsilon 0.7969987876958221, time 730.0, rides 124\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 252, reward -166.0, memory_length 2000, epsilon 0.7962814887868959, time 727.0, rides 136\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 253, reward -176.0, memory_length 2000, epsilon 0.7955648354469876, time 728.0, rides 131\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 254, reward -243.0, memory_length 2000, epsilon 0.7948488270950853, time 731.0, rides 135\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 255, reward -51.0, memory_length 2000, epsilon 0.7941334631506998, time 733.0, rides 130\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 256, reward 241.0, memory_length 2000, epsilon 0.7934187430338642, time 733.0, rides 136\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 257, reward -201.0, memory_length 2000, epsilon 0.7927046661651337, time 726.0, rides 124\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 258, reward -61.0, memory_length 2000, epsilon 0.7919912319655851, time 723.0, rides 139\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 259, reward -358.0, memory_length 2000, epsilon 0.791278439856816, time 734.0, rides 129\n",
      "Initial State is  [3, 7, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 260, reward -39.0, memory_length 2000, epsilon 0.7905662892609449, time 726.0, rides 125\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 261, reward -153.0, memory_length 2000, epsilon 0.78985477960061, time 722.0, rides 126\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 262, reward -118.0, memory_length 2000, epsilon 0.7891439102989695, time 740.0, rides 126\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 263, reward -39.0, memory_length 2000, epsilon 0.7884336807797003, time 731.0, rides 124\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 264, reward -112.0, memory_length 2000, epsilon 0.7877240904669985, time 740.0, rides 127\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 265, reward 83.0, memory_length 2000, epsilon 0.7870151387855783, time 732.0, rides 122\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 266, reward -344.0, memory_length 2000, epsilon 0.7863068251606713, time 733.0, rides 118\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 267, reward 7.0, memory_length 2000, epsilon 0.7855991490180266, time 728.0, rides 119\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 268, reward -88.0, memory_length 2000, epsilon 0.7848921097839104, time 728.0, rides 139\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 269, reward 86.0, memory_length 2000, epsilon 0.7841857068851049, time 737.0, rides 136\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 270, reward -321.0, memory_length 2000, epsilon 0.7834799397489083, time 732.0, rides 117\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 271, reward -199.0, memory_length 2000, epsilon 0.7827748078031342, time 724.0, rides 107\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 272, reward 25.0, memory_length 2000, epsilon 0.7820703104761114, time 732.0, rides 124\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 273, reward -117.0, memory_length 2000, epsilon 0.7813664471966829, time 729.0, rides 120\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 274, reward 67.0, memory_length 2000, epsilon 0.7806632173942059, time 731.0, rides 132\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 275, reward 46.0, memory_length 2000, epsilon 0.7799606204985511, time 729.0, rides 134\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 276, reward -430.0, memory_length 2000, epsilon 0.7792586559401024, time 727.0, rides 117\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 277, reward -343.0, memory_length 2000, epsilon 0.7785573231497562, time 726.0, rides 138\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 278, reward 314.0, memory_length 2000, epsilon 0.7778566215589214, time 729.0, rides 127\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 279, reward -301.0, memory_length 2000, epsilon 0.7771565505995184, time 726.0, rides 129\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 280, reward 20.0, memory_length 2000, epsilon 0.7764571097039789, time 741.0, rides 124\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 281, reward 79.0, memory_length 2000, epsilon 0.7757582983052452, time 730.0, rides 133\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 282, reward -17.0, memory_length 2000, epsilon 0.7750601158367705, time 727.0, rides 128\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 283, reward 9.0, memory_length 2000, epsilon 0.7743625617325174, time 729.0, rides 130\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 284, reward -215.0, memory_length 2000, epsilon 0.7736656354269581, time 721.0, rides 120\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 285, reward -67.0, memory_length 2000, epsilon 0.7729693363550738, time 725.0, rides 117\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 286, reward -188.0, memory_length 2000, epsilon 0.7722736639523542, time 728.0, rides 121\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 287, reward -229.0, memory_length 2000, epsilon 0.7715786176547971, time 729.0, rides 133\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 288, reward -273.0, memory_length 2000, epsilon 0.7708841968989077, time 733.0, rides 127\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 289, reward -59.0, memory_length 2000, epsilon 0.7701904011216987, time 730.0, rides 136\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 290, reward 148.0, memory_length 2000, epsilon 0.7694972297606891, time 728.0, rides 133\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 291, reward -450.0, memory_length 2000, epsilon 0.7688046822539045, time 735.0, rides 137\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 292, reward -332.0, memory_length 2000, epsilon 0.768112758039876, time 727.0, rides 130\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 293, reward -103.0, memory_length 2000, epsilon 0.76742145655764, time 732.0, rides 136\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 294, reward 212.0, memory_length 2000, epsilon 0.7667307772467382, time 727.0, rides 121\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 295, reward -294.0, memory_length 2000, epsilon 0.7660407195472161, time 741.0, rides 132\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 296, reward 228.0, memory_length 2000, epsilon 0.7653512828996236, time 740.0, rides 133\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 297, reward -70.0, memory_length 2000, epsilon 0.7646624667450139, time 734.0, rides 112\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 298, reward -357.0, memory_length 2000, epsilon 0.7639742705249434, time 734.0, rides 125\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 299, reward 0.0, memory_length 2000, epsilon 0.763286693681471, time 735.0, rides 128\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 300, reward -99.0, memory_length 2000, epsilon 0.7625997356571577, time 729.0, rides 122\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 301, reward -161.0, memory_length 2000, epsilon 0.7619133958950662, time 721.0, rides 127\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 302, reward -305.0, memory_length 2000, epsilon 0.7612276738387607, time 723.0, rides 138\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 303, reward -89.0, memory_length 2000, epsilon 0.7605425689323058, time 731.0, rides 130\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 304, reward 108.0, memory_length 2000, epsilon 0.7598580806202667, time 736.0, rides 122\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 305, reward 147.0, memory_length 2000, epsilon 0.7591742083477084, time 723.0, rides 131\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 306, reward -438.0, memory_length 2000, epsilon 0.7584909515601955, time 726.0, rides 125\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 307, reward -4.0, memory_length 2000, epsilon 0.7578083097037913, time 732.0, rides 130\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 308, reward -430.0, memory_length 2000, epsilon 0.7571262822250578, time 722.0, rides 121\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 309, reward -294.0, memory_length 2000, epsilon 0.7564448685710553, time 733.0, rides 132\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 310, reward -6.0, memory_length 2000, epsilon 0.7557640681893414, time 722.0, rides 139\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 311, reward -57.0, memory_length 2000, epsilon 0.7550838805279709, time 732.0, rides 129\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 312, reward -111.0, memory_length 2000, epsilon 0.7544043050354957, time 726.0, rides 117\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 313, reward 277.0, memory_length 2000, epsilon 0.7537253411609638, time 722.0, rides 132\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 314, reward -247.0, memory_length 2000, epsilon 0.7530469883539189, time 736.0, rides 131\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 315, reward -77.0, memory_length 2000, epsilon 0.7523692460644004, time 728.0, rides 123\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 316, reward -254.0, memory_length 2000, epsilon 0.7516921137429424, time 724.0, rides 119\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 317, reward 50.0, memory_length 2000, epsilon 0.7510155908405738, time 723.0, rides 140\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 318, reward -59.0, memory_length 2000, epsilon 0.7503396768088173, time 730.0, rides 122\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 319, reward 10.0, memory_length 2000, epsilon 0.7496643710996893, time 734.0, rides 123\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 320, reward 146.0, memory_length 2000, epsilon 0.7489896731656995, time 728.0, rides 123\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 321, reward -265.0, memory_length 2000, epsilon 0.7483155824598504, time 732.0, rides 135\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 322, reward -135.0, memory_length 2000, epsilon 0.7476420984356366, time 723.0, rides 110\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 323, reward -25.0, memory_length 2000, epsilon 0.7469692205470445, time 734.0, rides 116\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 324, reward -94.0, memory_length 2000, epsilon 0.7462969482485522, time 724.0, rides 117\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 325, reward -293.0, memory_length 2000, epsilon 0.7456252809951285, time 725.0, rides 144\n",
      "Initial State is  [2, 0, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 326, reward -34.0, memory_length 2000, epsilon 0.7449542182422328, time 733.0, rides 130\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 327, reward -16.0, memory_length 2000, epsilon 0.7442837594458148, time 727.0, rides 117\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 328, reward -123.0, memory_length 2000, epsilon 0.7436139040623135, time 741.0, rides 122\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 329, reward -267.0, memory_length 2000, epsilon 0.7429446515486574, time 731.0, rides 116\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 330, reward -65.0, memory_length 2000, epsilon 0.7422760013622636, time 734.0, rides 124\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 331, reward -308.0, memory_length 2000, epsilon 0.7416079529610375, time 722.0, rides 129\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 332, reward 116.0, memory_length 2000, epsilon 0.7409405058033726, time 725.0, rides 125\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 333, reward -120.0, memory_length 2000, epsilon 0.7402736593481495, time 723.0, rides 137\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 334, reward 24.0, memory_length 2000, epsilon 0.7396074130547361, time 729.0, rides 119\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 335, reward -52.0, memory_length 2000, epsilon 0.7389417663829868, time 734.0, rides 127\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 336, reward -104.0, memory_length 2000, epsilon 0.7382767187932421, time 729.0, rides 138\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 337, reward -278.0, memory_length 2000, epsilon 0.7376122697463282, time 723.0, rides 116\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 338, reward -2.0, memory_length 2000, epsilon 0.7369484187035565, time 721.0, rides 127\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 339, reward 69.0, memory_length 2000, epsilon 0.7362851651267234, time 728.0, rides 125\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 340, reward -115.0, memory_length 2000, epsilon 0.7356225084781093, time 724.0, rides 127\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 341, reward 35.0, memory_length 2000, epsilon 0.734960448220479, time 725.0, rides 151\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 342, reward -134.0, memory_length 2000, epsilon 0.7342989838170806, time 723.0, rides 127\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 343, reward 4.0, memory_length 2000, epsilon 0.7336381147316452, time 738.0, rides 132\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 344, reward -262.0, memory_length 2000, epsilon 0.7329778404283868, time 730.0, rides 144\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 345, reward -147.0, memory_length 2000, epsilon 0.7323181603720013, time 734.0, rides 133\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 346, reward -148.0, memory_length 2000, epsilon 0.7316590740276665, time 735.0, rides 136\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 347, reward -14.0, memory_length 2000, epsilon 0.7310005808610416, time 734.0, rides 113\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 348, reward -253.0, memory_length 2000, epsilon 0.7303426803382667, time 726.0, rides 124\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 349, reward -184.0, memory_length 2000, epsilon 0.7296853719259622, time 736.0, rides 128\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 350, reward -224.0, memory_length 2000, epsilon 0.7290286550912288, time 726.0, rides 120\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 351, reward 186.0, memory_length 2000, epsilon 0.7283725293016468, time 723.0, rides 137\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 352, reward 215.0, memory_length 2000, epsilon 0.7277169940252752, time 732.0, rides 148\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 353, reward -267.0, memory_length 2000, epsilon 0.7270620487306525, time 734.0, rides 120\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 354, reward -27.0, memory_length 2000, epsilon 0.7264076928867949, time 726.0, rides 139\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 355, reward -171.0, memory_length 2000, epsilon 0.7257539259631968, time 730.0, rides 119\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 356, reward 5.0, memory_length 2000, epsilon 0.7251007474298299, time 733.0, rides 131\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 357, reward 168.0, memory_length 2000, epsilon 0.724448156757143, time 729.0, rides 116\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 358, reward -167.0, memory_length 2000, epsilon 0.7237961534160616, time 730.0, rides 131\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 359, reward 62.0, memory_length 2000, epsilon 0.7231447368779872, time 727.0, rides 117\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 360, reward 70.0, memory_length 2000, epsilon 0.722493906614797, time 729.0, rides 121\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 361, reward -227.0, memory_length 2000, epsilon 0.7218436620988437, time 732.0, rides 123\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 362, reward -144.0, memory_length 2000, epsilon 0.7211940028029546, time 723.0, rides 121\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 363, reward 23.0, memory_length 2000, epsilon 0.720544928200432, time 725.0, rides 123\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 364, reward 4.0, memory_length 2000, epsilon 0.7198964377650516, time 731.0, rides 130\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 365, reward -52.0, memory_length 2000, epsilon 0.7192485309710631, time 731.0, rides 120\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 366, reward -106.0, memory_length 2000, epsilon 0.7186012072931891, time 728.0, rides 108\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 367, reward 90.0, memory_length 2000, epsilon 0.7179544662066252, time 732.0, rides 119\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 368, reward -66.0, memory_length 2000, epsilon 0.7173083071870392, time 731.0, rides 126\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 369, reward -4.0, memory_length 2000, epsilon 0.7166627297105709, time 731.0, rides 152\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 370, reward 82.0, memory_length 2000, epsilon 0.7160177332538313, time 724.0, rides 147\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 371, reward 162.0, memory_length 2000, epsilon 0.7153733172939029, time 733.0, rides 128\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 372, reward 19.0, memory_length 2000, epsilon 0.7147294813083384, time 727.0, rides 143\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 373, reward -73.0, memory_length 2000, epsilon 0.7140862247751608, time 733.0, rides 136\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 374, reward -163.0, memory_length 2000, epsilon 0.7134435471728632, time 728.0, rides 135\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 375, reward -186.0, memory_length 2000, epsilon 0.7128014479804076, time 726.0, rides 127\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 376, reward -157.0, memory_length 2000, epsilon 0.7121599266772252, time 735.0, rides 133\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 377, reward 56.0, memory_length 2000, epsilon 0.7115189827432157, time 722.0, rides 126\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 378, reward -109.0, memory_length 2000, epsilon 0.7108786156587468, time 733.0, rides 133\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 379, reward -196.0, memory_length 2000, epsilon 0.7102388249046538, time 728.0, rides 127\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 380, reward -84.0, memory_length 2000, epsilon 0.7095996099622397, time 734.0, rides 124\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 381, reward -209.0, memory_length 2000, epsilon 0.7089609703132737, time 740.0, rides 124\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 382, reward -184.0, memory_length 2000, epsilon 0.7083229054399917, time 734.0, rides 129\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 383, reward 239.0, memory_length 2000, epsilon 0.7076854148250956, time 721.0, rides 125\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 384, reward -46.0, memory_length 2000, epsilon 0.7070484979517531, time 727.0, rides 138\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 385, reward 87.0, memory_length 2000, epsilon 0.7064121543035965, time 733.0, rides 121\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 386, reward 91.0, memory_length 2000, epsilon 0.7057763833647233, time 735.0, rides 126\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 387, reward 94.0, memory_length 2000, epsilon 0.705141184619695, time 737.0, rides 120\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 388, reward -88.0, memory_length 2000, epsilon 0.7045065575535373, time 735.0, rides 145\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 389, reward -20.0, memory_length 2000, epsilon 0.7038725016517391, time 730.0, rides 120\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 390, reward -182.0, memory_length 2000, epsilon 0.7032390164002525, time 728.0, rides 132\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 391, reward -298.0, memory_length 2000, epsilon 0.7026061012854923, time 727.0, rides 127\n",
      "Initial State is  [1, 13, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 392, reward -211.0, memory_length 2000, epsilon 0.7019737557943353, time 737.0, rides 135\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 393, reward -136.0, memory_length 2000, epsilon 0.7013419794141204, time 733.0, rides 130\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 394, reward 139.0, memory_length 2000, epsilon 0.7007107716326476, time 730.0, rides 140\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 395, reward -232.0, memory_length 2000, epsilon 0.7000801319381782, time 729.0, rides 124\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 396, reward -192.0, memory_length 2000, epsilon 0.6994500598194339, time 735.0, rides 129\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 397, reward -46.0, memory_length 2000, epsilon 0.6988205547655963, time 735.0, rides 132\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 398, reward -402.0, memory_length 2000, epsilon 0.6981916162663073, time 728.0, rides 136\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 399, reward -332.0, memory_length 2000, epsilon 0.6975632438116677, time 726.0, rides 133\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 400, reward 22.0, memory_length 2000, epsilon 0.6969354368922371, time 726.0, rides 126\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 401, reward 177.0, memory_length 2000, epsilon 0.6963081949990341, time 732.0, rides 145\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 402, reward -209.0, memory_length 2000, epsilon 0.6956815176235349, time 721.0, rides 127\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 403, reward -278.0, memory_length 2000, epsilon 0.6950554042576738, time 727.0, rides 117\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 404, reward -270.0, memory_length 2000, epsilon 0.6944298543938419, time 725.0, rides 129\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 405, reward -487.0, memory_length 2000, epsilon 0.6938048675248873, time 725.0, rides 143\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 406, reward -319.0, memory_length 2000, epsilon 0.693180443144115, time 733.0, rides 123\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 407, reward -34.0, memory_length 2000, epsilon 0.6925565807452853, time 728.0, rides 126\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 408, reward 23.0, memory_length 2000, epsilon 0.6919332798226145, time 737.0, rides 123\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 409, reward 78.0, memory_length 2000, epsilon 0.6913105398707742, time 726.0, rides 125\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 410, reward -42.0, memory_length 2000, epsilon 0.6906883603848905, time 729.0, rides 137\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 411, reward 130.0, memory_length 2000, epsilon 0.6900667408605441, time 729.0, rides 139\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 412, reward -371.0, memory_length 2000, epsilon 0.6894456807937696, time 721.0, rides 120\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 413, reward -118.0, memory_length 2000, epsilon 0.6888251796810552, time 740.0, rides 119\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 414, reward 146.0, memory_length 2000, epsilon 0.6882052370193422, time 726.0, rides 118\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 415, reward 73.0, memory_length 2000, epsilon 0.6875858523060248, time 727.0, rides 120\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 416, reward -105.0, memory_length 2000, epsilon 0.6869670250389494, time 725.0, rides 124\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 417, reward -319.0, memory_length 2000, epsilon 0.6863487547164143, time 722.0, rides 133\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 418, reward 137.0, memory_length 2000, epsilon 0.6857310408371695, time 723.0, rides 136\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 419, reward -86.0, memory_length 2000, epsilon 0.6851138829004161, time 728.0, rides 112\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 420, reward 196.0, memory_length 2000, epsilon 0.6844972804058057, time 734.0, rides 116\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 421, reward 52.0, memory_length 2000, epsilon 0.6838812328534405, time 731.0, rides 116\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 422, reward 25.0, memory_length 2000, epsilon 0.6832657397438724, time 735.0, rides 118\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 423, reward -3.0, memory_length 2000, epsilon 0.6826508005781029, time 731.0, rides 126\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 424, reward -279.0, memory_length 2000, epsilon 0.6820364148575826, time 731.0, rides 123\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 425, reward -30.0, memory_length 2000, epsilon 0.6814225820842108, time 734.0, rides 124\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 426, reward 64.0, memory_length 2000, epsilon 0.680809301760335, time 729.0, rides 125\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 427, reward -55.0, memory_length 2000, epsilon 0.6801965733887507, time 732.0, rides 127\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 428, reward -8.0, memory_length 2000, epsilon 0.6795843964727009, time 725.0, rides 120\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 429, reward -44.0, memory_length 2000, epsilon 0.6789727705158755, time 726.0, rides 135\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 430, reward -111.0, memory_length 2000, epsilon 0.6783616950224112, time 734.0, rides 123\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 431, reward -23.0, memory_length 2000, epsilon 0.677751169496891, time 725.0, rides 126\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 432, reward 40.0, memory_length 2000, epsilon 0.6771411934443438, time 726.0, rides 122\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 433, reward -271.0, memory_length 2000, epsilon 0.6765317663702438, time 724.0, rides 131\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 434, reward -11.0, memory_length 2000, epsilon 0.6759228877805106, time 724.0, rides 126\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 435, reward 94.0, memory_length 2000, epsilon 0.6753145571815081, time 724.0, rides 125\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 436, reward 103.0, memory_length 2000, epsilon 0.6747067740800448, time 731.0, rides 127\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 437, reward 60.0, memory_length 2000, epsilon 0.6740995379833727, time 726.0, rides 136\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 438, reward 176.0, memory_length 2000, epsilon 0.6734928483991877, time 733.0, rides 141\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 439, reward 219.0, memory_length 2000, epsilon 0.6728867048356284, time 736.0, rides 134\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 440, reward -148.0, memory_length 2000, epsilon 0.6722811068012763, time 732.0, rides 128\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 441, reward -192.0, memory_length 2000, epsilon 0.6716760538051552, time 726.0, rides 118\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 442, reward 51.0, memory_length 2000, epsilon 0.6710715453567305, time 732.0, rides 121\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 443, reward 119.0, memory_length 2000, epsilon 0.6704675809659094, time 723.0, rides 124\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 444, reward -92.0, memory_length 2000, epsilon 0.6698641601430401, time 732.0, rides 130\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 445, reward -20.0, memory_length 2000, epsilon 0.6692612823989114, time 732.0, rides 126\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 446, reward -50.0, memory_length 2000, epsilon 0.6686589472447524, time 731.0, rides 131\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 447, reward 109.0, memory_length 2000, epsilon 0.6680571541922321, time 729.0, rides 124\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 448, reward -276.0, memory_length 2000, epsilon 0.6674559027534591, time 729.0, rides 113\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 449, reward -95.0, memory_length 2000, epsilon 0.666855192440981, time 725.0, rides 139\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 450, reward -158.0, memory_length 2000, epsilon 0.6662550227677841, time 725.0, rides 116\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 451, reward 34.0, memory_length 2000, epsilon 0.6656553932472932, time 732.0, rides 122\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 452, reward -344.0, memory_length 2000, epsilon 0.6650563033933706, time 732.0, rides 117\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 453, reward -94.0, memory_length 2000, epsilon 0.6644577527203166, time 722.0, rides 123\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 454, reward 42.0, memory_length 2000, epsilon 0.6638597407428684, time 738.0, rides 137\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 455, reward 106.0, memory_length 2000, epsilon 0.6632622669761997, time 725.0, rides 141\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 456, reward -284.0, memory_length 2000, epsilon 0.6626653309359212, time 728.0, rides 116\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 457, reward 185.0, memory_length 2000, epsilon 0.6620689321380788, time 737.0, rides 133\n",
      "Initial State is  [1, 21, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 458, reward 120.0, memory_length 2000, epsilon 0.6614730700991546, time 726.0, rides 137\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 459, reward -157.0, memory_length 2000, epsilon 0.6608777443360653, time 722.0, rides 117\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 460, reward 124.0, memory_length 2000, epsilon 0.6602829543661628, time 732.0, rides 124\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 461, reward -68.0, memory_length 2000, epsilon 0.6596886997072332, time 727.0, rides 133\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 462, reward -237.0, memory_length 2000, epsilon 0.6590949798774967, time 731.0, rides 142\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 463, reward -74.0, memory_length 2000, epsilon 0.6585017943956069, time 725.0, rides 119\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 464, reward -19.0, memory_length 2000, epsilon 0.6579091427806508, time 730.0, rides 135\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 465, reward 148.0, memory_length 2000, epsilon 0.6573170245521482, time 726.0, rides 130\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 466, reward 81.0, memory_length 2000, epsilon 0.6567254392300513, time 727.0, rides 145\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 467, reward -242.0, memory_length 2000, epsilon 0.6561343863347443, time 739.0, rides 122\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 468, reward 125.0, memory_length 2000, epsilon 0.655543865387043, time 727.0, rides 145\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 469, reward 51.0, memory_length 2000, epsilon 0.6549538759081946, time 732.0, rides 133\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 470, reward 299.0, memory_length 2000, epsilon 0.6543644174198773, time 723.0, rides 141\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 471, reward -202.0, memory_length 2000, epsilon 0.6537754894441994, time 731.0, rides 137\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 472, reward 59.0, memory_length 2000, epsilon 0.6531870915036996, time 725.0, rides 115\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 473, reward 6.0, memory_length 2000, epsilon 0.6525992231213462, time 727.0, rides 128\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 474, reward -373.0, memory_length 2000, epsilon 0.652011883820537, time 723.0, rides 134\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 475, reward -139.0, memory_length 2000, epsilon 0.6514250731250985, time 730.0, rides 117\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 476, reward -121.0, memory_length 2000, epsilon 0.6508387905592858, time 733.0, rides 141\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 477, reward 291.0, memory_length 2000, epsilon 0.6502530356477825, time 731.0, rides 129\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 478, reward 114.0, memory_length 2000, epsilon 0.6496678079156994, time 731.0, rides 117\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 479, reward 127.0, memory_length 2000, epsilon 0.6490831068885753, time 727.0, rides 126\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 480, reward 245.0, memory_length 2000, epsilon 0.6484989320923755, time 727.0, rides 137\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 481, reward 32.0, memory_length 2000, epsilon 0.6479152830534923, time 731.0, rides 128\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 482, reward -216.0, memory_length 2000, epsilon 0.6473321592987442, time 737.0, rides 120\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 483, reward 83.0, memory_length 2000, epsilon 0.6467495603553753, time 722.0, rides 132\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 484, reward 83.0, memory_length 2000, epsilon 0.6461674857510555, time 728.0, rides 116\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 485, reward 240.0, memory_length 2000, epsilon 0.6455859350138796, time 726.0, rides 125\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 486, reward -126.0, memory_length 2000, epsilon 0.6450049076723671, time 730.0, rides 119\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 487, reward -69.0, memory_length 2000, epsilon 0.6444244032554619, time 736.0, rides 127\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 488, reward 387.0, memory_length 2000, epsilon 0.6438444212925319, time 726.0, rides 130\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 489, reward 114.0, memory_length 2000, epsilon 0.6432649613133686, time 730.0, rides 136\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 490, reward -285.0, memory_length 2000, epsilon 0.6426860228481865, time 730.0, rides 119\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 491, reward -59.0, memory_length 2000, epsilon 0.6421076054276231, time 729.0, rides 128\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 492, reward 94.0, memory_length 2000, epsilon 0.6415297085827383, time 727.0, rides 128\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 493, reward -12.0, memory_length 2000, epsilon 0.6409523318450138, time 724.0, rides 123\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 494, reward -60.0, memory_length 2000, epsilon 0.6403754747463533, time 735.0, rides 123\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 495, reward 2.0, memory_length 2000, epsilon 0.6397991368190815, time 728.0, rides 132\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 496, reward 183.0, memory_length 2000, epsilon 0.6392233175959443, time 724.0, rides 120\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 497, reward -193.0, memory_length 2000, epsilon 0.638648016610108, time 727.0, rides 148\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 498, reward -150.0, memory_length 2000, epsilon 0.6380732333951589, time 729.0, rides 115\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 499, reward 161.0, memory_length 2000, epsilon 0.6374989674851033, time 726.0, rides 123\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 500, reward 155.0, memory_length 2000, epsilon 0.6369252184143667, time 730.0, rides 125\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 501, reward 51.0, memory_length 2000, epsilon 0.6363519857177937, time 725.0, rides 129\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 502, reward 114.0, memory_length 2000, epsilon 0.6357792689306477, time 730.0, rides 133\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 503, reward 102.0, memory_length 2000, epsilon 0.6352070675886101, time 730.0, rides 137\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 504, reward 310.0, memory_length 2000, epsilon 0.6346353812277804, time 726.0, rides 137\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 505, reward 319.0, memory_length 2000, epsilon 0.6340642093846754, time 726.0, rides 132\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 506, reward 35.0, memory_length 2000, epsilon 0.6334935515962292, time 724.0, rides 147\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 507, reward -86.0, memory_length 2000, epsilon 0.6329234073997926, time 733.0, rides 131\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 508, reward -113.0, memory_length 2000, epsilon 0.6323537763331327, time 735.0, rides 135\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 509, reward 173.0, memory_length 2000, epsilon 0.631784657934433, time 725.0, rides 130\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 510, reward 188.0, memory_length 2000, epsilon 0.6312160517422919, time 734.0, rides 118\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 511, reward 73.0, memory_length 2000, epsilon 0.6306479572957239, time 726.0, rides 125\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 512, reward -19.0, memory_length 2000, epsilon 0.6300803741341577, time 735.0, rides 133\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 513, reward 4.0, memory_length 2000, epsilon 0.629513301797437, time 727.0, rides 137\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 514, reward 30.0, memory_length 2000, epsilon 0.6289467398258193, time 727.0, rides 127\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 515, reward -248.0, memory_length 2000, epsilon 0.628380687759976, time 723.0, rides 152\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 516, reward 238.0, memory_length 2000, epsilon 0.627815145140992, time 742.0, rides 130\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 517, reward 10.0, memory_length 2000, epsilon 0.6272501115103651, time 730.0, rides 128\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 518, reward 32.0, memory_length 2000, epsilon 0.6266855864100058, time 724.0, rides 133\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 519, reward 90.0, memory_length 2000, epsilon 0.6261215693822368, time 741.0, rides 118\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 520, reward 54.0, memory_length 2000, epsilon 0.6255580599697929, time 728.0, rides 125\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 521, reward 232.0, memory_length 2000, epsilon 0.6249950577158201, time 736.0, rides 127\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 522, reward 105.0, memory_length 2000, epsilon 0.6244325621638759, time 729.0, rides 131\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 523, reward -223.0, memory_length 2000, epsilon 0.6238705728579284, time 735.0, rides 126\n",
      "Initial State is  [4, 18, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 524, reward -193.0, memory_length 2000, epsilon 0.6233090893423562, time 728.0, rides 128\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 525, reward 257.0, memory_length 2000, epsilon 0.6227481111619481, time 729.0, rides 146\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 526, reward 145.0, memory_length 2000, epsilon 0.6221876378619023, time 723.0, rides 124\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 527, reward 161.0, memory_length 2000, epsilon 0.6216276689878266, time 723.0, rides 125\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 528, reward -54.0, memory_length 2000, epsilon 0.6210682040857376, time 731.0, rides 142\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 529, reward -29.0, memory_length 2000, epsilon 0.6205092427020604, time 720.0, rides 135\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 530, reward -93.0, memory_length 2000, epsilon 0.6199507843836286, time 728.0, rides 124\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 531, reward 163.0, memory_length 2000, epsilon 0.6193928286776833, time 744.0, rides 120\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 532, reward 266.0, memory_length 2000, epsilon 0.6188353751318734, time 734.0, rides 136\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 533, reward -109.0, memory_length 2000, epsilon 0.6182784232942546, time 723.0, rides 135\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 534, reward 220.0, memory_length 2000, epsilon 0.6177219727132898, time 731.0, rides 127\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 535, reward -255.0, memory_length 2000, epsilon 0.6171660229378478, time 734.0, rides 136\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 536, reward -188.0, memory_length 2000, epsilon 0.6166105735172037, time 725.0, rides 123\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 537, reward 37.0, memory_length 2000, epsilon 0.6160556240010382, time 735.0, rides 129\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 538, reward -27.0, memory_length 2000, epsilon 0.6155011739394373, time 723.0, rides 131\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 539, reward -66.0, memory_length 2000, epsilon 0.6149472228828917, time 726.0, rides 124\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 540, reward 422.0, memory_length 2000, epsilon 0.6143937703822971, time 729.0, rides 121\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 541, reward 235.0, memory_length 2000, epsilon 0.613840815988953, time 724.0, rides 124\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 542, reward 202.0, memory_length 2000, epsilon 0.6132883592545629, time 720.0, rides 133\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 543, reward -67.0, memory_length 2000, epsilon 0.6127363997312338, time 741.0, rides 133\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 544, reward 287.0, memory_length 2000, epsilon 0.6121849369714757, time 722.0, rides 129\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 545, reward -2.0, memory_length 2000, epsilon 0.6116339705282013, time 728.0, rides 126\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 546, reward 298.0, memory_length 2000, epsilon 0.611083499954726, time 730.0, rides 129\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 547, reward 267.0, memory_length 2000, epsilon 0.6105335248047667, time 734.0, rides 127\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 548, reward 22.0, memory_length 2000, epsilon 0.6099840446324425, time 736.0, rides 121\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 549, reward 102.0, memory_length 2000, epsilon 0.6094350589922732, time 735.0, rides 124\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 550, reward -101.0, memory_length 2000, epsilon 0.6088865674391802, time 734.0, rides 124\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 551, reward 122.0, memory_length 2000, epsilon 0.608338569528485, time 733.0, rides 123\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 552, reward -150.0, memory_length 2000, epsilon 0.6077910648159093, time 727.0, rides 129\n",
      "Initial State is  [0, 12, 2]\n",
      "episode 553, reward 118.0, memory_length 2000, epsilon 0.607244052857575, time 728.0, rides 133\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 554, reward 220.0, memory_length 2000, epsilon 0.6066975332100032, time 730.0, rides 119\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 555, reward 42.0, memory_length 2000, epsilon 0.6061515054301142, time 728.0, rides 127\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 556, reward 198.0, memory_length 2000, epsilon 0.6056059690752271, time 729.0, rides 121\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 557, reward 304.0, memory_length 2000, epsilon 0.6050609237030594, time 724.0, rides 123\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 558, reward -97.0, memory_length 2000, epsilon 0.6045163688717267, time 732.0, rides 122\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 559, reward -128.0, memory_length 2000, epsilon 0.6039723041397421, time 729.0, rides 124\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 560, reward -159.0, memory_length 2000, epsilon 0.6034287290660163, time 730.0, rides 154\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 561, reward -63.0, memory_length 2000, epsilon 0.6028856432098568, time 726.0, rides 125\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 562, reward -219.0, memory_length 2000, epsilon 0.6023430461309679, time 730.0, rides 134\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 563, reward -39.0, memory_length 2000, epsilon 0.6018009373894501, time 732.0, rides 134\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 564, reward 537.0, memory_length 2000, epsilon 0.6012593165457996, time 742.0, rides 139\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 565, reward 183.0, memory_length 2000, epsilon 0.6007181831609083, time 732.0, rides 121\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 566, reward 119.0, memory_length 2000, epsilon 0.6001775367960634, time 738.0, rides 129\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 567, reward 68.0, memory_length 2000, epsilon 0.599637377012947, time 732.0, rides 134\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 568, reward 382.0, memory_length 2000, epsilon 0.5990977033736353, time 729.0, rides 119\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 569, reward -11.0, memory_length 2000, epsilon 0.598558515440599, time 729.0, rides 134\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 570, reward 0.0, memory_length 2000, epsilon 0.5980198127767025, time 722.0, rides 137\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 571, reward 164.0, memory_length 2000, epsilon 0.5974815949452035, time 733.0, rides 129\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 572, reward -73.0, memory_length 2000, epsilon 0.5969438615097528, time 733.0, rides 129\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 573, reward 136.0, memory_length 2000, epsilon 0.5964066120343939, time 728.0, rides 126\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 574, reward 34.0, memory_length 2000, epsilon 0.595869846083563, time 730.0, rides 132\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 575, reward 158.0, memory_length 2000, epsilon 0.5953335632220877, time 726.0, rides 129\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 576, reward 116.0, memory_length 2000, epsilon 0.5947977630151878, time 722.0, rides 106\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 577, reward 274.0, memory_length 2000, epsilon 0.5942624450284741, time 735.0, rides 121\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 578, reward 112.0, memory_length 2000, epsilon 0.5937276088279485, time 727.0, rides 124\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 579, reward -174.0, memory_length 2000, epsilon 0.5931932539800033, time 734.0, rides 129\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 580, reward 224.0, memory_length 2000, epsilon 0.5926593800514213, time 736.0, rides 128\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 581, reward -5.0, memory_length 2000, epsilon 0.592125986609375, time 729.0, rides 128\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 582, reward 264.0, memory_length 2000, epsilon 0.5915930732214265, time 723.0, rides 138\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 583, reward -33.0, memory_length 2000, epsilon 0.5910606394555272, time 733.0, rides 134\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 584, reward -149.0, memory_length 2000, epsilon 0.5905286848800172, time 731.0, rides 131\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 585, reward -116.0, memory_length 2000, epsilon 0.5899972090636252, time 734.0, rides 116\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 586, reward -210.0, memory_length 2000, epsilon 0.589466211575468, time 734.0, rides 122\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 587, reward 1.0, memory_length 2000, epsilon 0.58893569198505, time 727.0, rides 134\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 588, reward 4.0, memory_length 2000, epsilon 0.5884056498622635, time 735.0, rides 128\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 589, reward 49.0, memory_length 2000, epsilon 0.5878760847773875, time 722.0, rides 128\n",
      "Initial State is  [1, 12, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 590, reward 72.0, memory_length 2000, epsilon 0.5873469963010879, time 728.0, rides 137\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 591, reward 78.0, memory_length 2000, epsilon 0.5868183840044169, time 724.0, rides 131\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 592, reward 255.0, memory_length 2000, epsilon 0.5862902474588129, time 734.0, rides 127\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 593, reward 79.0, memory_length 2000, epsilon 0.5857625862360999, time 725.0, rides 125\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 594, reward 193.0, memory_length 2000, epsilon 0.5852353999084874, time 725.0, rides 119\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 595, reward 150.0, memory_length 2000, epsilon 0.5847086880485698, time 729.0, rides 127\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 596, reward 131.0, memory_length 2000, epsilon 0.584182450229326, time 728.0, rides 133\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 597, reward 386.0, memory_length 2000, epsilon 0.5836566860241196, time 729.0, rides 116\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 598, reward -119.0, memory_length 2000, epsilon 0.5831313950066979, time 735.0, rides 134\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 599, reward 226.0, memory_length 2000, epsilon 0.5826065767511919, time 728.0, rides 131\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 600, reward 119.0, memory_length 2000, epsilon 0.5820822308321157, time 728.0, rides 124\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 601, reward 7.0, memory_length 2000, epsilon 0.5815583568243669, time 726.0, rides 139\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 602, reward 101.0, memory_length 2000, epsilon 0.5810349543032249, time 723.0, rides 133\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 603, reward 189.0, memory_length 2000, epsilon 0.580512022844352, time 724.0, rides 128\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 604, reward 131.0, memory_length 2000, epsilon 0.579989562023792, time 740.0, rides 141\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 605, reward -68.0, memory_length 2000, epsilon 0.5794675714179707, time 727.0, rides 139\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 606, reward 244.0, memory_length 2000, epsilon 0.5789460506036945, time 732.0, rides 133\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 607, reward 105.0, memory_length 2000, epsilon 0.5784249991581512, time 728.0, rides 140\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 608, reward 9.0, memory_length 2000, epsilon 0.5779044166589088, time 733.0, rides 135\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 609, reward 282.0, memory_length 2000, epsilon 0.5773843026839157, time 731.0, rides 143\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 610, reward -87.0, memory_length 2000, epsilon 0.5768646568115002, time 728.0, rides 135\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 611, reward 45.0, memory_length 2000, epsilon 0.5763454786203699, time 731.0, rides 131\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 612, reward 177.0, memory_length 2000, epsilon 0.5758267676896115, time 729.0, rides 140\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 613, reward -28.0, memory_length 2000, epsilon 0.5753085235986909, time 727.0, rides 122\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 614, reward 69.0, memory_length 2000, epsilon 0.574790745927452, time 721.0, rides 134\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 615, reward -116.0, memory_length 2000, epsilon 0.5742734342561173, time 726.0, rides 133\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 616, reward -25.0, memory_length 2000, epsilon 0.5737565881652869, time 724.0, rides 126\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 617, reward 158.0, memory_length 2000, epsilon 0.573240207235938, time 733.0, rides 139\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 618, reward 291.0, memory_length 2000, epsilon 0.5727242910494257, time 721.0, rides 116\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 619, reward -35.0, memory_length 2000, epsilon 0.5722088391874812, time 738.0, rides 140\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 620, reward 393.0, memory_length 2000, epsilon 0.5716938512322125, time 730.0, rides 122\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 621, reward -156.0, memory_length 2000, epsilon 0.5711793267661035, time 723.0, rides 140\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 622, reward 152.0, memory_length 2000, epsilon 0.570665265372014, time 725.0, rides 143\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 623, reward 11.0, memory_length 2000, epsilon 0.5701516666331792, time 729.0, rides 144\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 624, reward -425.0, memory_length 2000, epsilon 0.5696385301332093, time 725.0, rides 135\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 625, reward 176.0, memory_length 2000, epsilon 0.5691258554560894, time 729.0, rides 140\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 626, reward 323.0, memory_length 2000, epsilon 0.5686136421861789, time 731.0, rides 141\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 627, reward -21.0, memory_length 2000, epsilon 0.5681018899082114, time 722.0, rides 142\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 628, reward 179.0, memory_length 2000, epsilon 0.5675905982072941, time 729.0, rides 132\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 629, reward 267.0, memory_length 2000, epsilon 0.5670797666689075, time 730.0, rides 123\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 630, reward 31.0, memory_length 2000, epsilon 0.5665693948789055, time 731.0, rides 131\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 631, reward -188.0, memory_length 2000, epsilon 0.5660594824235144, time 732.0, rides 126\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 632, reward -140.0, memory_length 2000, epsilon 0.5655500288893333, time 734.0, rides 134\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 633, reward 81.0, memory_length 2000, epsilon 0.5650410338633328, time 738.0, rides 133\n",
      "Initial State is  [1, 21, 4]\n",
      "episode 634, reward 111.0, memory_length 2000, epsilon 0.5645324969328558, time 724.0, rides 126\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 635, reward 157.0, memory_length 2000, epsilon 0.5640244176856162, time 728.0, rides 128\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 636, reward -86.0, memory_length 2000, epsilon 0.5635167957096991, time 724.0, rides 136\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 637, reward 84.0, memory_length 2000, epsilon 0.5630096305935605, time 738.0, rides 121\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 638, reward 74.0, memory_length 2000, epsilon 0.5625029219260262, time 732.0, rides 127\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 639, reward 246.0, memory_length 2000, epsilon 0.5619966692962928, time 729.0, rides 137\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 640, reward 79.0, memory_length 2000, epsilon 0.5614908722939261, time 725.0, rides 142\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 641, reward -217.0, memory_length 2000, epsilon 0.5609855305088616, time 737.0, rides 137\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 642, reward 97.0, memory_length 2000, epsilon 0.5604806435314036, time 722.0, rides 119\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 643, reward 278.0, memory_length 2000, epsilon 0.5599762109522253, time 724.0, rides 130\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 644, reward -181.0, memory_length 2000, epsilon 0.5594722323623683, time 729.0, rides 120\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 645, reward 99.0, memory_length 2000, epsilon 0.5589687073532422, time 732.0, rides 131\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 646, reward 263.0, memory_length 2000, epsilon 0.5584656355166243, time 725.0, rides 130\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 647, reward 121.0, memory_length 2000, epsilon 0.5579630164446594, time 724.0, rides 134\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 648, reward 6.0, memory_length 2000, epsilon 0.5574608497298592, time 731.0, rides 118\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 649, reward 9.0, memory_length 2000, epsilon 0.5569591349651023, time 728.0, rides 137\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 650, reward 72.0, memory_length 2000, epsilon 0.5564578717436337, time 725.0, rides 131\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 651, reward 456.0, memory_length 2000, epsilon 0.5559570596590644, time 724.0, rides 134\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 652, reward -137.0, memory_length 2000, epsilon 0.5554566983053713, time 731.0, rides 132\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 653, reward 14.0, memory_length 2000, epsilon 0.5549567872768965, time 726.0, rides 119\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 654, reward 135.0, memory_length 2000, epsilon 0.5544573261683472, time 728.0, rides 139\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 655, reward -34.0, memory_length 2000, epsilon 0.5539583145747957, time 730.0, rides 141\n",
      "Initial State is  [1, 4, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 656, reward 7.0, memory_length 2000, epsilon 0.5534597520916784, time 725.0, rides 135\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 657, reward 151.0, memory_length 2000, epsilon 0.552961638314796, time 729.0, rides 135\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 658, reward -39.0, memory_length 2000, epsilon 0.5524639728403126, time 739.0, rides 136\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 659, reward 184.0, memory_length 2000, epsilon 0.5519667552647562, time 729.0, rides 131\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 660, reward 86.0, memory_length 2000, epsilon 0.551469985185018, time 737.0, rides 125\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 661, reward -152.0, memory_length 2000, epsilon 0.5509736621983514, time 732.0, rides 122\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 662, reward 360.0, memory_length 2000, epsilon 0.550477785902373, time 723.0, rides 114\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 663, reward 83.0, memory_length 2000, epsilon 0.5499823558950608, time 724.0, rides 147\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 664, reward 269.0, memory_length 2000, epsilon 0.5494873717747553, time 732.0, rides 117\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 665, reward 17.0, memory_length 2000, epsilon 0.548992833140158, time 732.0, rides 127\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 666, reward 250.0, memory_length 2000, epsilon 0.5484987395903319, time 729.0, rides 125\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 667, reward 93.0, memory_length 2000, epsilon 0.5480050907247006, time 737.0, rides 127\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 668, reward 95.0, memory_length 2000, epsilon 0.5475118861430484, time 727.0, rides 141\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 669, reward 252.0, memory_length 2000, epsilon 0.5470191254455196, time 733.0, rides 132\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 670, reward 322.0, memory_length 2000, epsilon 0.5465268082326186, time 721.0, rides 132\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 671, reward 42.0, memory_length 2000, epsilon 0.5460349341052092, time 727.0, rides 137\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 672, reward 117.0, memory_length 2000, epsilon 0.5455435026645145, time 737.0, rides 162\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 673, reward -28.0, memory_length 2000, epsilon 0.5450525135121164, time 733.0, rides 150\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 674, reward 170.0, memory_length 2000, epsilon 0.5445619662499555, time 725.0, rides 128\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 675, reward 55.0, memory_length 2000, epsilon 0.5440718604803305, time 727.0, rides 126\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 676, reward 119.0, memory_length 2000, epsilon 0.5435821958058982, time 725.0, rides 143\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 677, reward 282.0, memory_length 2000, epsilon 0.5430929718296729, time 732.0, rides 141\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 678, reward 269.0, memory_length 2000, epsilon 0.5426041881550262, time 729.0, rides 120\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 679, reward 173.0, memory_length 2000, epsilon 0.5421158443856867, time 726.0, rides 127\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 680, reward 177.0, memory_length 2000, epsilon 0.5416279401257396, time 729.0, rides 135\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 681, reward 80.0, memory_length 2000, epsilon 0.5411404749796264, time 730.0, rides 121\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 682, reward 184.0, memory_length 2000, epsilon 0.5406534485521447, time 725.0, rides 112\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 683, reward 131.0, memory_length 2000, epsilon 0.5401668604484477, time 726.0, rides 123\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 684, reward 237.0, memory_length 2000, epsilon 0.5396807102740442, time 727.0, rides 131\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 685, reward 314.0, memory_length 2000, epsilon 0.5391949976347975, time 725.0, rides 126\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 686, reward 113.0, memory_length 2000, epsilon 0.5387097221369261, time 731.0, rides 125\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 687, reward -197.0, memory_length 2000, epsilon 0.5382248833870029, time 728.0, rides 117\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 688, reward -77.0, memory_length 2000, epsilon 0.5377404809919546, time 726.0, rides 125\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 689, reward -55.0, memory_length 2000, epsilon 0.5372565145590619, time 724.0, rides 124\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 690, reward -150.0, memory_length 2000, epsilon 0.5367729836959587, time 728.0, rides 128\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 691, reward -164.0, memory_length 2000, epsilon 0.5362898880106324, time 733.0, rides 133\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 692, reward 533.0, memory_length 2000, epsilon 0.5358072271114228, time 726.0, rides 132\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 693, reward 259.0, memory_length 2000, epsilon 0.5353250006070225, time 738.0, rides 116\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 694, reward 108.0, memory_length 2000, epsilon 0.5348432081064761, time 725.0, rides 122\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 695, reward 165.0, memory_length 2000, epsilon 0.5343618492191803, time 726.0, rides 129\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 696, reward 278.0, memory_length 2000, epsilon 0.533880923554883, time 728.0, rides 118\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 697, reward 241.0, memory_length 2000, epsilon 0.5334004307236836, time 724.0, rides 124\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 698, reward 224.0, memory_length 2000, epsilon 0.5329203703360322, time 734.0, rides 121\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 699, reward 25.0, memory_length 2000, epsilon 0.5324407420027298, time 727.0, rides 128\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 700, reward 259.0, memory_length 2000, epsilon 0.5319615453349273, time 724.0, rides 120\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 701, reward 52.0, memory_length 2000, epsilon 0.5314827799441258, time 721.0, rides 129\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 702, reward 225.0, memory_length 2000, epsilon 0.5310044454421762, time 722.0, rides 146\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 703, reward 208.0, memory_length 2000, epsilon 0.5305265414412782, time 728.0, rides 134\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 704, reward 245.0, memory_length 2000, epsilon 0.5300490675539811, time 729.0, rides 134\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 705, reward 500.0, memory_length 2000, epsilon 0.5295720233931824, time 725.0, rides 127\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 706, reward 4.0, memory_length 2000, epsilon 0.5290954085721286, time 728.0, rides 135\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 707, reward 204.0, memory_length 2000, epsilon 0.5286192227044136, time 731.0, rides 158\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 708, reward 160.0, memory_length 2000, epsilon 0.5281434654039796, time 724.0, rides 137\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 709, reward 260.0, memory_length 2000, epsilon 0.527668136285116, time 731.0, rides 129\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 710, reward -12.0, memory_length 2000, epsilon 0.5271932349624594, time 725.0, rides 118\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 711, reward 92.0, memory_length 2000, epsilon 0.5267187610509931, time 729.0, rides 132\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 712, reward 176.0, memory_length 2000, epsilon 0.5262447141660472, time 740.0, rides 134\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 713, reward 242.0, memory_length 2000, epsilon 0.5257710939232978, time 748.0, rides 118\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 714, reward 197.0, memory_length 2000, epsilon 0.5252978999387669, time 733.0, rides 121\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 715, reward 187.0, memory_length 2000, epsilon 0.524825131828822, time 734.0, rides 130\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 716, reward -238.0, memory_length 2000, epsilon 0.524352789210176, time 733.0, rides 132\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 717, reward 360.0, memory_length 2000, epsilon 0.5238808716998868, time 730.0, rides 140\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 718, reward 469.0, memory_length 2000, epsilon 0.5234093789153569, time 721.0, rides 133\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 719, reward 190.0, memory_length 2000, epsilon 0.522938310474333, time 725.0, rides 127\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 720, reward -55.0, memory_length 2000, epsilon 0.5224676659949061, time 728.0, rides 142\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 721, reward 385.0, memory_length 2000, epsilon 0.5219974450955107, time 732.0, rides 132\n",
      "Initial State is  [4, 21, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 722, reward 368.0, memory_length 2000, epsilon 0.5215276473949247, time 738.0, rides 130\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 723, reward 168.0, memory_length 2000, epsilon 0.5210582725122693, time 725.0, rides 135\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 724, reward 123.0, memory_length 2000, epsilon 0.5205893200670083, time 734.0, rides 136\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 725, reward 27.0, memory_length 2000, epsilon 0.520120789678948, time 740.0, rides 135\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 726, reward 208.0, memory_length 2000, epsilon 0.519652680968237, time 723.0, rides 128\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 727, reward 195.0, memory_length 2000, epsilon 0.5191849935553656, time 733.0, rides 130\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 728, reward 332.0, memory_length 2000, epsilon 0.5187177270611658, time 723.0, rides 131\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 729, reward 353.0, memory_length 2000, epsilon 0.5182508811068107, time 726.0, rides 132\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 730, reward 282.0, memory_length 2000, epsilon 0.5177844553138146, time 727.0, rides 133\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 731, reward 312.0, memory_length 2000, epsilon 0.5173184493040321, time 723.0, rides 145\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 732, reward 181.0, memory_length 2000, epsilon 0.5168528626996585, time 730.0, rides 128\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 733, reward 61.0, memory_length 2000, epsilon 0.5163876951232288, time 730.0, rides 120\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 734, reward 118.0, memory_length 2000, epsilon 0.5159229461976179, time 735.0, rides 118\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 735, reward 140.0, memory_length 2000, epsilon 0.51545861554604, time 727.0, rides 122\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 736, reward -24.0, memory_length 2000, epsilon 0.5149947027920485, time 732.0, rides 132\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 737, reward -67.0, memory_length 2000, epsilon 0.5145312075595356, time 726.0, rides 140\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 738, reward 271.0, memory_length 2000, epsilon 0.5140681294727321, time 740.0, rides 126\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 739, reward 70.0, memory_length 2000, epsilon 0.5136054681562066, time 728.0, rides 125\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 740, reward 208.0, memory_length 2000, epsilon 0.5131432232348659, time 724.0, rides 131\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 741, reward 251.0, memory_length 2000, epsilon 0.5126813943339545, time 728.0, rides 127\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 742, reward 32.0, memory_length 2000, epsilon 0.512219981079054, time 741.0, rides 123\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 743, reward -102.0, memory_length 2000, epsilon 0.5117589830960828, time 736.0, rides 127\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 744, reward 253.0, memory_length 2000, epsilon 0.5112984000112963, time 728.0, rides 129\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 745, reward 13.0, memory_length 2000, epsilon 0.5108382314512862, time 731.0, rides 118\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 746, reward -158.0, memory_length 2000, epsilon 0.51037847704298, time 724.0, rides 116\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 747, reward 52.0, memory_length 2000, epsilon 0.5099191364136413, time 736.0, rides 129\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 748, reward 254.0, memory_length 2000, epsilon 0.509460209190869, time 725.0, rides 123\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 749, reward 116.0, memory_length 2000, epsilon 0.5090016950025972, time 731.0, rides 117\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 750, reward 349.0, memory_length 2000, epsilon 0.5085435934770949, time 733.0, rides 141\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 751, reward 435.0, memory_length 2000, epsilon 0.5080859042429655, time 730.0, rides 150\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 752, reward 442.0, memory_length 2000, epsilon 0.5076286269291468, time 723.0, rides 130\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 753, reward 251.0, memory_length 2000, epsilon 0.5071717611649106, time 726.0, rides 121\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 754, reward -131.0, memory_length 2000, epsilon 0.5067153065798622, time 729.0, rides 131\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 755, reward -99.0, memory_length 2000, epsilon 0.5062592628039403, time 730.0, rides 118\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 756, reward 371.0, memory_length 2000, epsilon 0.5058036294674167, time 734.0, rides 125\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 757, reward 87.0, memory_length 2000, epsilon 0.505348406200896, time 734.0, rides 131\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 758, reward 59.0, memory_length 2000, epsilon 0.5048935926353152, time 723.0, rides 124\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 759, reward 162.0, memory_length 2000, epsilon 0.5044391884019434, time 729.0, rides 136\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 760, reward 86.0, memory_length 2000, epsilon 0.5039851931323815, time 730.0, rides 138\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 761, reward 159.0, memory_length 2000, epsilon 0.5035316064585624, time 723.0, rides 134\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 762, reward 67.0, memory_length 2000, epsilon 0.5030784280127497, time 726.0, rides 115\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 763, reward 188.0, memory_length 2000, epsilon 0.5026256574275383, time 736.0, rides 126\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 764, reward 243.0, memory_length 2000, epsilon 0.5021732943358534, time 725.0, rides 117\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 765, reward 39.0, memory_length 2000, epsilon 0.5017213383709511, time 729.0, rides 119\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 766, reward 408.0, memory_length 2000, epsilon 0.5012697891664173, time 734.0, rides 122\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 767, reward 366.0, memory_length 2000, epsilon 0.5008186463561675, time 728.0, rides 117\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 768, reward 350.0, memory_length 2000, epsilon 0.5003679095744469, time 743.0, rides 128\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 769, reward 206.0, memory_length 2000, epsilon 0.49991757845582985, time 728.0, rides 131\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 770, reward 309.0, memory_length 2000, epsilon 0.4994676526352196, time 732.0, rides 146\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 771, reward 231.0, memory_length 2000, epsilon 0.49901813174784787, time 726.0, rides 130\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 772, reward 57.0, memory_length 2000, epsilon 0.4985690154292748, time 737.0, rides 125\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 773, reward 357.0, memory_length 2000, epsilon 0.4981203033153884, time 733.0, rides 124\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 774, reward 350.0, memory_length 2000, epsilon 0.49767199504240456, time 734.0, rides 127\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 775, reward 417.0, memory_length 2000, epsilon 0.49722409024686637, time 735.0, rides 121\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 776, reward 462.0, memory_length 2000, epsilon 0.49677658856564416, time 737.0, rides 129\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 777, reward 44.0, memory_length 2000, epsilon 0.4963294896359351, time 733.0, rides 139\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 778, reward 550.0, memory_length 2000, epsilon 0.49588279309526273, time 723.0, rides 123\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 779, reward 255.0, memory_length 2000, epsilon 0.495436498581477, time 725.0, rides 145\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 780, reward 426.0, memory_length 2000, epsilon 0.49499060573275366, time 729.0, rides 133\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 781, reward -47.0, memory_length 2000, epsilon 0.49454511418759417, time 726.0, rides 130\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 782, reward 37.0, memory_length 2000, epsilon 0.49410002358482535, time 732.0, rides 116\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 783, reward 465.0, memory_length 2000, epsilon 0.49365533356359903, time 724.0, rides 116\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 784, reward 83.0, memory_length 2000, epsilon 0.4932110437633918, time 734.0, rides 131\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 785, reward 23.0, memory_length 2000, epsilon 0.49276715382400477, time 731.0, rides 127\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 786, reward 197.0, memory_length 2000, epsilon 0.49232366338556316, time 724.0, rides 121\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 787, reward -4.0, memory_length 2000, epsilon 0.49188057208851615, time 722.0, rides 131\n",
      "Initial State is  [1, 0, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 788, reward 350.0, memory_length 2000, epsilon 0.4914378795736365, time 735.0, rides 130\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 789, reward 129.0, memory_length 2000, epsilon 0.4909955854820202, time 728.0, rides 130\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 790, reward -122.0, memory_length 2000, epsilon 0.4905536894550864, time 726.0, rides 114\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 791, reward 195.0, memory_length 2000, epsilon 0.49011219113457677, time 729.0, rides 150\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 792, reward 111.0, memory_length 2000, epsilon 0.48967109016255567, time 721.0, rides 113\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 793, reward 59.0, memory_length 2000, epsilon 0.48923038618140935, time 728.0, rides 134\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 794, reward -115.0, memory_length 2000, epsilon 0.48879007883384606, time 722.0, rides 133\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 795, reward 303.0, memory_length 2000, epsilon 0.4883501677628956, time 722.0, rides 115\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 796, reward 314.0, memory_length 2000, epsilon 0.48791065261190897, time 729.0, rides 133\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 797, reward 274.0, memory_length 2000, epsilon 0.48747153302455826, time 724.0, rides 124\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 798, reward 149.0, memory_length 2000, epsilon 0.48703280864483617, time 737.0, rides 131\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 799, reward 120.0, memory_length 2000, epsilon 0.48659447911705583, time 727.0, rides 124\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 800, reward 273.0, memory_length 2000, epsilon 0.48615654408585046, time 732.0, rides 131\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 801, reward -61.0, memory_length 2000, epsilon 0.48571900319617317, time 731.0, rides 128\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 802, reward 229.0, memory_length 2000, epsilon 0.4852818560932966, time 724.0, rides 133\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 803, reward 445.0, memory_length 2000, epsilon 0.48484510242281265, time 730.0, rides 131\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 804, reward 391.0, memory_length 2000, epsilon 0.4844087418306321, time 733.0, rides 128\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 805, reward 83.0, memory_length 2000, epsilon 0.4839727739629845, time 736.0, rides 135\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 806, reward 279.0, memory_length 2000, epsilon 0.4835371984664178, time 725.0, rides 115\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 807, reward 382.0, memory_length 2000, epsilon 0.48310201498779803, time 723.0, rides 139\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 808, reward 390.0, memory_length 2000, epsilon 0.482667223174309, time 726.0, rides 113\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 809, reward -83.0, memory_length 2000, epsilon 0.48223282267345213, time 730.0, rides 135\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 810, reward 142.0, memory_length 2000, epsilon 0.481798813133046, time 727.0, rides 122\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 811, reward 326.0, memory_length 2000, epsilon 0.48136519420122625, time 722.0, rides 151\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 812, reward 363.0, memory_length 2000, epsilon 0.4809319655264451, time 735.0, rides 132\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 813, reward 516.0, memory_length 2000, epsilon 0.4804991267574713, time 723.0, rides 124\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 814, reward 21.0, memory_length 2000, epsilon 0.48006667754338955, time 730.0, rides 127\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 815, reward 148.0, memory_length 2000, epsilon 0.4796346175336005, time 723.0, rides 125\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 816, reward 165.0, memory_length 2000, epsilon 0.47920294637782024, time 737.0, rides 131\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 817, reward 303.0, memory_length 2000, epsilon 0.4787716637260802, time 731.0, rides 129\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 818, reward 185.0, memory_length 2000, epsilon 0.47834076922872676, time 722.0, rides 127\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 819, reward 322.0, memory_length 2000, epsilon 0.4779102625364209, time 736.0, rides 137\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 820, reward 94.0, memory_length 2000, epsilon 0.4774801433001381, time 730.0, rides 148\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 821, reward 332.0, memory_length 2000, epsilon 0.477050411171168, time 727.0, rides 133\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 822, reward -2.0, memory_length 2000, epsilon 0.4766210658011139, time 729.0, rides 122\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 823, reward 289.0, memory_length 2000, epsilon 0.4761921068418929, time 727.0, rides 125\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 824, reward 249.0, memory_length 2000, epsilon 0.4757635339457352, time 724.0, rides 126\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 825, reward 151.0, memory_length 2000, epsilon 0.475335346765184, time 723.0, rides 140\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 826, reward 372.0, memory_length 2000, epsilon 0.47490754495309534, time 732.0, rides 132\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 827, reward 702.0, memory_length 2000, epsilon 0.47448012816263757, time 731.0, rides 134\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 828, reward -38.0, memory_length 2000, epsilon 0.4740530960472912, time 725.0, rides 128\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 829, reward 312.0, memory_length 2000, epsilon 0.4736264482608486, time 727.0, rides 120\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 830, reward 196.0, memory_length 2000, epsilon 0.47320018445741385, time 723.0, rides 124\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 831, reward 125.0, memory_length 2000, epsilon 0.47277430429140216, time 738.0, rides 127\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 832, reward 41.0, memory_length 2000, epsilon 0.4723488074175399, time 729.0, rides 109\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 833, reward 257.0, memory_length 2000, epsilon 0.4719236934908641, time 728.0, rides 133\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 834, reward 68.0, memory_length 2000, epsilon 0.4714989621667223, time 730.0, rides 133\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 835, reward -9.0, memory_length 2000, epsilon 0.47107461310077225, time 730.0, rides 135\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 836, reward 245.0, memory_length 2000, epsilon 0.47065064594898154, time 727.0, rides 138\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 837, reward 246.0, memory_length 2000, epsilon 0.47022706036762746, time 732.0, rides 106\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 838, reward 246.0, memory_length 2000, epsilon 0.4698038560132966, time 726.0, rides 124\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 839, reward 138.0, memory_length 2000, epsilon 0.46938103254288466, time 726.0, rides 134\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 840, reward 105.0, memory_length 2000, epsilon 0.46895858961359604, time 729.0, rides 126\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 841, reward 605.0, memory_length 2000, epsilon 0.4685365268829438, time 735.0, rides 128\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 842, reward 245.0, memory_length 2000, epsilon 0.46811484400874914, time 732.0, rides 123\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 843, reward 357.0, memory_length 2000, epsilon 0.46769354064914126, time 730.0, rides 141\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 844, reward 93.0, memory_length 2000, epsilon 0.467272616462557, time 728.0, rides 122\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 845, reward 412.0, memory_length 2000, epsilon 0.46685207110774074, time 736.0, rides 132\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 846, reward 300.0, memory_length 2000, epsilon 0.46643190424374376, time 729.0, rides 125\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 847, reward 218.0, memory_length 2000, epsilon 0.4660121155299244, time 727.0, rides 128\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 848, reward 142.0, memory_length 2000, epsilon 0.46559270462594743, time 725.0, rides 147\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 849, reward 208.0, memory_length 2000, epsilon 0.46517367119178404, time 727.0, rides 146\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 850, reward 690.0, memory_length 2000, epsilon 0.4647550148877114, time 736.0, rides 134\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 851, reward 325.0, memory_length 2000, epsilon 0.46433673537431247, time 736.0, rides 137\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 852, reward 355.0, memory_length 2000, epsilon 0.4639188323124756, time 733.0, rides 125\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 853, reward 253.0, memory_length 2000, epsilon 0.46350130536339434, time 723.0, rides 129\n",
      "Initial State is  [0, 18, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 854, reward 150.0, memory_length 2000, epsilon 0.4630841541885673, time 727.0, rides 129\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 855, reward 201.0, memory_length 2000, epsilon 0.4626673784497976, time 726.0, rides 134\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 856, reward 191.0, memory_length 2000, epsilon 0.4622509778091928, time 726.0, rides 115\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 857, reward 295.0, memory_length 2000, epsilon 0.4618349519291645, time 732.0, rides 135\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 858, reward 179.0, memory_length 2000, epsilon 0.46141930047242824, time 724.0, rides 123\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 859, reward 168.0, memory_length 2000, epsilon 0.46100402310200306, time 726.0, rides 121\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 860, reward 326.0, memory_length 2000, epsilon 0.46058911948121123, time 728.0, rides 128\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 861, reward -65.0, memory_length 2000, epsilon 0.46017458927367816, time 725.0, rides 121\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 862, reward 264.0, memory_length 2000, epsilon 0.45976043214333184, time 728.0, rides 135\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 863, reward 48.0, memory_length 2000, epsilon 0.4593466477544028, time 725.0, rides 141\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 864, reward 273.0, memory_length 2000, epsilon 0.45893323577142386, time 728.0, rides 123\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 865, reward 324.0, memory_length 2000, epsilon 0.4585201958592296, time 729.0, rides 129\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 866, reward 227.0, memory_length 2000, epsilon 0.45810752768295626, time 722.0, rides 123\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 867, reward 215.0, memory_length 2000, epsilon 0.4576952309080416, time 731.0, rides 124\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 868, reward 130.0, memory_length 2000, epsilon 0.45728330520022437, time 733.0, rides 140\n",
      "Initial State is  [3, 8, 5]\n",
      "episode 869, reward 570.0, memory_length 2000, epsilon 0.45687175022554416, time 727.0, rides 127\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 870, reward 475.0, memory_length 2000, epsilon 0.45646056565034115, time 734.0, rides 127\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 871, reward 439.0, memory_length 2000, epsilon 0.45604975114125584, time 732.0, rides 127\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 872, reward 271.0, memory_length 2000, epsilon 0.4556393063652287, time 729.0, rides 119\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 873, reward -31.0, memory_length 2000, epsilon 0.45522923098949997, time 728.0, rides 127\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 874, reward 394.0, memory_length 2000, epsilon 0.4548195246816094, time 728.0, rides 141\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 875, reward 323.0, memory_length 2000, epsilon 0.45441018710939596, time 724.0, rides 129\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 876, reward 74.0, memory_length 2000, epsilon 0.4540012179409975, time 726.0, rides 134\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 877, reward 495.0, memory_length 2000, epsilon 0.45359261684485064, time 733.0, rides 126\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 878, reward 205.0, memory_length 2000, epsilon 0.45318438348969026, time 736.0, rides 129\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 879, reward 181.0, memory_length 2000, epsilon 0.4527765175445495, time 724.0, rides 135\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 880, reward 371.0, memory_length 2000, epsilon 0.4523690186787594, time 727.0, rides 123\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 881, reward 204.0, memory_length 2000, epsilon 0.4519618865619485, time 730.0, rides 123\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 882, reward 293.0, memory_length 2000, epsilon 0.45155512086404276, time 734.0, rides 122\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 883, reward -78.0, memory_length 2000, epsilon 0.4511487212552651, time 728.0, rides 118\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 884, reward 159.0, memory_length 2000, epsilon 0.4507426874061354, time 728.0, rides 143\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 885, reward 324.0, memory_length 2000, epsilon 0.45033701898746986, time 724.0, rides 117\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 886, reward 175.0, memory_length 2000, epsilon 0.44993171567038115, time 726.0, rides 124\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 887, reward 187.0, memory_length 2000, epsilon 0.4495267771262778, time 727.0, rides 127\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 888, reward -57.0, memory_length 2000, epsilon 0.4491222030268642, time 726.0, rides 119\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 889, reward -19.0, memory_length 2000, epsilon 0.44871799304414, time 726.0, rides 121\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 890, reward 97.0, memory_length 2000, epsilon 0.4483141468504003, time 735.0, rides 134\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 891, reward 239.0, memory_length 2000, epsilon 0.44791066411823494, time 735.0, rides 134\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 892, reward 69.0, memory_length 2000, epsilon 0.44750754452052854, time 727.0, rides 124\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 893, reward 196.0, memory_length 2000, epsilon 0.4471047877304601, time 729.0, rides 133\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 894, reward 173.0, memory_length 2000, epsilon 0.44670239342150264, time 736.0, rides 143\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 895, reward 16.0, memory_length 2000, epsilon 0.4463003612674233, time 727.0, rides 124\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 896, reward 54.0, memory_length 2000, epsilon 0.44589869094228257, time 733.0, rides 118\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 897, reward 114.0, memory_length 2000, epsilon 0.4454973821204345, time 729.0, rides 130\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 898, reward 312.0, memory_length 2000, epsilon 0.4450964344765261, time 729.0, rides 131\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 899, reward 384.0, memory_length 2000, epsilon 0.4446958476854972, time 724.0, rides 127\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 900, reward 333.0, memory_length 2000, epsilon 0.4442956214225802, time 735.0, rides 123\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 901, reward 106.0, memory_length 2000, epsilon 0.4438957553632999, time 733.0, rides 110\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 902, reward 267.0, memory_length 2000, epsilon 0.4434962491834729, time 730.0, rides 135\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 903, reward 156.0, memory_length 2000, epsilon 0.44309710255920776, time 728.0, rides 132\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 904, reward 490.0, memory_length 2000, epsilon 0.44269831516690444, time 727.0, rides 128\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 905, reward 129.0, memory_length 2000, epsilon 0.4422998866832542, time 730.0, rides 108\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 906, reward 281.0, memory_length 2000, epsilon 0.44190181678523927, time 727.0, rides 122\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 907, reward 119.0, memory_length 2000, epsilon 0.44150410515013255, time 730.0, rides 151\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 908, reward 230.0, memory_length 2000, epsilon 0.44110675145549744, time 730.0, rides 135\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 909, reward 660.0, memory_length 2000, epsilon 0.4407097553791875, time 729.0, rides 137\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 910, reward 307.0, memory_length 2000, epsilon 0.44031311659934624, time 729.0, rides 129\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 911, reward 812.0, memory_length 2000, epsilon 0.43991683479440685, time 729.0, rides 138\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 912, reward 214.0, memory_length 2000, epsilon 0.4395209096430919, time 733.0, rides 140\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 913, reward 323.0, memory_length 2000, epsilon 0.43912534082441307, time 730.0, rides 139\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 914, reward 334.0, memory_length 2000, epsilon 0.4387301280176711, time 728.0, rides 136\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 915, reward -57.0, memory_length 2000, epsilon 0.43833527090245517, time 729.0, rides 137\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 916, reward 177.0, memory_length 2000, epsilon 0.43794076915864294, time 727.0, rides 129\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 917, reward 370.0, memory_length 2000, epsilon 0.43754662246640014, time 734.0, rides 130\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 918, reward 208.0, memory_length 2000, epsilon 0.43715283050618037, time 724.0, rides 126\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 919, reward 174.0, memory_length 2000, epsilon 0.4367593929587248, time 733.0, rides 128\n",
      "Initial State is  [4, 20, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 920, reward 143.0, memory_length 2000, epsilon 0.43636630950506194, time 727.0, rides 124\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 921, reward 417.0, memory_length 2000, epsilon 0.43597357982650736, time 734.0, rides 127\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 922, reward 195.0, memory_length 2000, epsilon 0.4355812036046635, time 734.0, rides 124\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 923, reward 538.0, memory_length 2000, epsilon 0.4351891805214193, time 726.0, rides 141\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 924, reward 267.0, memory_length 2000, epsilon 0.43479751025895, time 737.0, rides 133\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 925, reward 270.0, memory_length 2000, epsilon 0.43440619249971696, time 735.0, rides 112\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 926, reward 150.0, memory_length 2000, epsilon 0.4340152269264672, time 732.0, rides 129\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 927, reward 184.0, memory_length 2000, epsilon 0.4336246132222334, time 731.0, rides 153\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 928, reward 451.0, memory_length 2000, epsilon 0.43323435107033337, time 720.0, rides 140\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 929, reward 377.0, memory_length 2000, epsilon 0.43284444015437007, time 726.0, rides 141\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 930, reward 282.0, memory_length 2000, epsilon 0.43245488015823114, time 732.0, rides 121\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 931, reward 428.0, memory_length 2000, epsilon 0.4320656707660887, time 721.0, rides 142\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 932, reward 170.0, memory_length 2000, epsilon 0.43167681166239924, time 727.0, rides 133\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 933, reward 363.0, memory_length 2000, epsilon 0.4312883025319031, time 732.0, rides 129\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 934, reward 168.0, memory_length 2000, epsilon 0.4309001430596244, time 733.0, rides 130\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 935, reward 167.0, memory_length 2000, epsilon 0.4305123329308707, time 729.0, rides 138\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 936, reward 175.0, memory_length 2000, epsilon 0.43012487183123294, time 726.0, rides 112\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 937, reward 258.0, memory_length 2000, epsilon 0.4297377594465848, time 730.0, rides 136\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 938, reward 156.0, memory_length 2000, epsilon 0.42935099546308286, time 724.0, rides 131\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 939, reward 328.0, memory_length 2000, epsilon 0.4289645795671661, time 728.0, rides 129\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 940, reward 379.0, memory_length 2000, epsilon 0.42857851144555564, time 727.0, rides 129\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 941, reward 414.0, memory_length 2000, epsilon 0.42819279078525463, time 724.0, rides 147\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 942, reward 307.0, memory_length 2000, epsilon 0.4278074172735479, time 730.0, rides 128\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 943, reward 294.0, memory_length 2000, epsilon 0.4274223905980017, time 724.0, rides 137\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 944, reward 134.0, memory_length 2000, epsilon 0.4270377104464635, time 745.0, rides 127\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 945, reward -53.0, memory_length 2000, epsilon 0.42665337650706164, time 727.0, rides 125\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 946, reward 58.0, memory_length 2000, epsilon 0.4262693884682053, time 722.0, rides 127\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 947, reward 477.0, memory_length 2000, epsilon 0.42588574601858387, time 730.0, rides 123\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 948, reward 152.0, memory_length 2000, epsilon 0.42550244884716715, time 728.0, rides 113\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 949, reward 336.0, memory_length 2000, epsilon 0.4251194966432047, time 733.0, rides 121\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 950, reward 319.0, memory_length 2000, epsilon 0.4247368890962258, time 728.0, rides 127\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 951, reward 149.0, memory_length 2000, epsilon 0.42435462589603923, time 734.0, rides 122\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 952, reward 656.0, memory_length 2000, epsilon 0.4239727067327328, time 730.0, rides 120\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 953, reward 344.0, memory_length 2000, epsilon 0.4235911312966733, time 736.0, rides 139\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 954, reward -79.0, memory_length 2000, epsilon 0.4232098992785063, time 728.0, rides 131\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 955, reward 80.0, memory_length 2000, epsilon 0.4228290103691556, time 726.0, rides 119\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 956, reward 412.0, memory_length 2000, epsilon 0.42244846425982335, time 721.0, rides 124\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 957, reward 313.0, memory_length 2000, epsilon 0.4220682606419895, time 729.0, rides 131\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 958, reward 426.0, memory_length 2000, epsilon 0.4216883992074117, time 727.0, rides 131\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 959, reward 190.0, memory_length 2000, epsilon 0.421308879648125, time 731.0, rides 144\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 960, reward 458.0, memory_length 2000, epsilon 0.4209297016564417, time 727.0, rides 128\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 961, reward 273.0, memory_length 2000, epsilon 0.4205508649249509, time 726.0, rides 117\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 962, reward 371.0, memory_length 2000, epsilon 0.42017236914651845, time 729.0, rides 132\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 963, reward 357.0, memory_length 2000, epsilon 0.41979421401428657, time 726.0, rides 134\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 964, reward 122.0, memory_length 2000, epsilon 0.41941639922167373, time 727.0, rides 120\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 965, reward 99.0, memory_length 2000, epsilon 0.41903892446237423, time 721.0, rides 119\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 966, reward 236.0, memory_length 2000, epsilon 0.4186617894303581, time 723.0, rides 118\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 967, reward 321.0, memory_length 2000, epsilon 0.4182849938198708, time 733.0, rides 114\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 968, reward 587.0, memory_length 2000, epsilon 0.41790853732543287, time 731.0, rides 122\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 969, reward 101.0, memory_length 2000, epsilon 0.41753241964183996, time 733.0, rides 133\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 970, reward 608.0, memory_length 2000, epsilon 0.4171566404641623, time 727.0, rides 130\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 971, reward 385.0, memory_length 2000, epsilon 0.41678119948774456, time 732.0, rides 134\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 972, reward 478.0, memory_length 2000, epsilon 0.4164060964082056, time 727.0, rides 124\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 973, reward 311.0, memory_length 2000, epsilon 0.4160313309214382, time 733.0, rides 121\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 974, reward 226.0, memory_length 2000, epsilon 0.4156569027236089, time 728.0, rides 138\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 975, reward 93.0, memory_length 2000, epsilon 0.41528281151115765, time 724.0, rides 129\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 976, reward 542.0, memory_length 2000, epsilon 0.4149090569807976, time 724.0, rides 132\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 977, reward 320.0, memory_length 2000, epsilon 0.4145356388295149, time 732.0, rides 116\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 978, reward 409.0, memory_length 2000, epsilon 0.4141625567545683, time 729.0, rides 122\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 979, reward 114.0, memory_length 2000, epsilon 0.41378981045348917, time 727.0, rides 130\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 980, reward 302.0, memory_length 2000, epsilon 0.41341739962408103, time 726.0, rides 133\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 981, reward 63.0, memory_length 2000, epsilon 0.41304532396441934, time 726.0, rides 132\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 982, reward 244.0, memory_length 2000, epsilon 0.41267358317285135, time 731.0, rides 136\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 983, reward 316.0, memory_length 2000, epsilon 0.41230217694799576, time 725.0, rides 129\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 984, reward 389.0, memory_length 2000, epsilon 0.4119311049887426, time 725.0, rides 127\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 985, reward 358.0, memory_length 2000, epsilon 0.4115603669942527, time 724.0, rides 121\n",
      "Initial State is  [0, 10, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 986, reward 327.0, memory_length 2000, epsilon 0.41118996266395785, time 728.0, rides 124\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 987, reward 273.0, memory_length 2000, epsilon 0.4108198916975603, time 732.0, rides 133\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 988, reward -29.0, memory_length 2000, epsilon 0.41045015379503247, time 736.0, rides 132\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 989, reward 395.0, memory_length 2000, epsilon 0.4100807486566169, time 743.0, rides 126\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 990, reward 168.0, memory_length 2000, epsilon 0.409711675982826, time 729.0, rides 141\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 991, reward 275.0, memory_length 2000, epsilon 0.40934293547444145, time 726.0, rides 120\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 992, reward 259.0, memory_length 2000, epsilon 0.40897452683251445, time 730.0, rides 131\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 993, reward 168.0, memory_length 2000, epsilon 0.4086064497583652, time 740.0, rides 128\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 994, reward 366.0, memory_length 2000, epsilon 0.40823870395358264, time 730.0, rides 124\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 995, reward 198.0, memory_length 2000, epsilon 0.4078712891200244, time 727.0, rides 149\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 996, reward 357.0, memory_length 2000, epsilon 0.4075042049598164, time 728.0, rides 135\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 997, reward -101.0, memory_length 2000, epsilon 0.40713745117535255, time 730.0, rides 138\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 998, reward 165.0, memory_length 2000, epsilon 0.4067710274692947, time 723.0, rides 141\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 999, reward 279.0, memory_length 2000, epsilon 0.40640493354457236, time 724.0, rides 129\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 1000, reward 373.0, memory_length 2000, epsilon 0.40603916910438226, time 721.0, rides 139\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 1001, reward 36.0, memory_length 2000, epsilon 0.4056737338521883, time 737.0, rides 125\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 1002, reward 668.0, memory_length 2000, epsilon 0.4053086274917213, time 721.0, rides 128\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 1003, reward 338.0, memory_length 2000, epsilon 0.4049438497269788, time 734.0, rides 126\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 1004, reward 37.0, memory_length 2000, epsilon 0.4045794002622245, time 729.0, rides 122\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 1005, reward 299.0, memory_length 2000, epsilon 0.4042152788019885, time 730.0, rides 137\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 1006, reward 302.0, memory_length 2000, epsilon 0.4038514850510667, time 729.0, rides 134\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 1007, reward 167.0, memory_length 2000, epsilon 0.4034880187145207, time 730.0, rides 136\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 1008, reward 226.0, memory_length 2000, epsilon 0.40312487949767767, time 726.0, rides 145\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 1009, reward 604.0, memory_length 2000, epsilon 0.4027620671061298, time 732.0, rides 118\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 1010, reward 196.0, memory_length 2000, epsilon 0.4023995812457343, time 727.0, rides 133\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 1011, reward 225.0, memory_length 2000, epsilon 0.40203742162261313, time 727.0, rides 137\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 1012, reward 480.0, memory_length 2000, epsilon 0.4016755879431528, time 728.0, rides 131\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 1013, reward 397.0, memory_length 2000, epsilon 0.4013140799140039, time 730.0, rides 133\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1014, reward 182.0, memory_length 2000, epsilon 0.40095289724208133, time 731.0, rides 135\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 1015, reward 432.0, memory_length 2000, epsilon 0.40059203963456347, time 731.0, rides 115\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 1016, reward 150.0, memory_length 2000, epsilon 0.40023150679889236, time 732.0, rides 139\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1017, reward 517.0, memory_length 2000, epsilon 0.39987129844277336, time 725.0, rides 132\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 1018, reward 295.0, memory_length 2000, epsilon 0.39951141427417486, time 731.0, rides 129\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 1019, reward 74.0, memory_length 2000, epsilon 0.3991518540013281, time 732.0, rides 131\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 1020, reward 490.0, memory_length 2000, epsilon 0.39879261733272686, time 728.0, rides 139\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 1021, reward 251.0, memory_length 2000, epsilon 0.3984337039771274, time 731.0, rides 120\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1022, reward 288.0, memory_length 2000, epsilon 0.398075113643548, time 723.0, rides 133\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 1023, reward 218.0, memory_length 2000, epsilon 0.3977168460412688, time 721.0, rides 142\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 1024, reward -57.0, memory_length 2000, epsilon 0.3973589008798316, time 726.0, rides 129\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 1025, reward 94.0, memory_length 2000, epsilon 0.39700127786903977, time 730.0, rides 129\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 1026, reward 361.0, memory_length 2000, epsilon 0.3966439767189576, time 723.0, rides 128\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 1027, reward 171.0, memory_length 2000, epsilon 0.39628699713991056, time 729.0, rides 137\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1028, reward 239.0, memory_length 2000, epsilon 0.39593033884248463, time 724.0, rides 115\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 1029, reward 540.0, memory_length 2000, epsilon 0.39557400153752637, time 723.0, rides 132\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1030, reward 191.0, memory_length 2000, epsilon 0.3952179849361426, time 730.0, rides 133\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 1031, reward 491.0, memory_length 2000, epsilon 0.39486228874970003, time 724.0, rides 122\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1032, reward 198.0, memory_length 2000, epsilon 0.3945069126898253, time 729.0, rides 126\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1033, reward 229.0, memory_length 2000, epsilon 0.39415185646840445, time 727.0, rides 144\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 1034, reward 319.0, memory_length 2000, epsilon 0.39379711979758286, time 732.0, rides 127\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 1035, reward 284.0, memory_length 2000, epsilon 0.39344270238976503, time 725.0, rides 123\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1036, reward 265.0, memory_length 2000, epsilon 0.39308860395761425, time 733.0, rides 111\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1037, reward 161.0, memory_length 2000, epsilon 0.39273482421405237, time 723.0, rides 130\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 1038, reward 251.0, memory_length 2000, epsilon 0.3923813628722597, time 728.0, rides 131\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 1039, reward 516.0, memory_length 2000, epsilon 0.3920282196456747, time 733.0, rides 134\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 1040, reward -61.0, memory_length 2000, epsilon 0.3916753942479936, time 723.0, rides 132\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 1041, reward 381.0, memory_length 2000, epsilon 0.39132288639317037, time 734.0, rides 135\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 1042, reward 259.0, memory_length 2000, epsilon 0.3909706957954165, time 727.0, rides 129\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 1043, reward 315.0, memory_length 2000, epsilon 0.39061882216920063, time 728.0, rides 124\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1044, reward 177.0, memory_length 2000, epsilon 0.3902672652292484, time 730.0, rides 123\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 1045, reward 340.0, memory_length 2000, epsilon 0.38991602469054204, time 724.0, rides 127\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 1046, reward 702.0, memory_length 2000, epsilon 0.38956510026832053, time 729.0, rides 132\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 1047, reward 557.0, memory_length 2000, epsilon 0.38921449167807903, time 736.0, rides 122\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 1048, reward 393.0, memory_length 2000, epsilon 0.38886419863556876, time 741.0, rides 133\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1049, reward 331.0, memory_length 2000, epsilon 0.38851422085679677, time 734.0, rides 144\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 1050, reward 229.0, memory_length 2000, epsilon 0.38816455805802563, time 728.0, rides 135\n",
      "Initial State is  [3, 6, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1051, reward 252.0, memory_length 2000, epsilon 0.3878152099557734, time 727.0, rides 129\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 1052, reward 321.0, memory_length 2000, epsilon 0.3874661762668132, time 722.0, rides 130\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 1053, reward 341.0, memory_length 2000, epsilon 0.38711745670817305, time 726.0, rides 131\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 1054, reward 339.0, memory_length 2000, epsilon 0.3867690509971357, time 726.0, rides 128\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 1055, reward 272.0, memory_length 2000, epsilon 0.3864209588512383, time 727.0, rides 121\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1056, reward 241.0, memory_length 2000, epsilon 0.3860731799882722, time 734.0, rides 145\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1057, reward 334.0, memory_length 2000, epsilon 0.3857257141262827, time 727.0, rides 129\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 1058, reward 457.0, memory_length 2000, epsilon 0.38537856098356904, time 728.0, rides 117\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 1059, reward 255.0, memory_length 2000, epsilon 0.3850317202786838, time 734.0, rides 131\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 1060, reward 139.0, memory_length 2000, epsilon 0.384685191730433, time 728.0, rides 132\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1061, reward 357.0, memory_length 2000, epsilon 0.38433897505787556, time 739.0, rides 124\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 1062, reward 204.0, memory_length 2000, epsilon 0.38399306998032345, time 730.0, rides 120\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 1063, reward 611.0, memory_length 2000, epsilon 0.38364747621734113, time 729.0, rides 142\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 1064, reward 473.0, memory_length 2000, epsilon 0.3833021934887455, time 731.0, rides 124\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 1065, reward 497.0, memory_length 2000, epsilon 0.38295722151460565, time 728.0, rides 130\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 1066, reward 285.0, memory_length 2000, epsilon 0.3826125600152425, time 735.0, rides 132\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1067, reward 363.0, memory_length 2000, epsilon 0.3822682087112288, time 724.0, rides 133\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 1068, reward 255.0, memory_length 2000, epsilon 0.3819241673233887, time 728.0, rides 128\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 1069, reward 302.0, memory_length 2000, epsilon 0.38158043557279764, time 737.0, rides 123\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 1070, reward 232.0, memory_length 2000, epsilon 0.3812370131807821, time 727.0, rides 141\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 1071, reward 551.0, memory_length 2000, epsilon 0.3808938998689194, time 724.0, rides 120\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 1072, reward 291.0, memory_length 2000, epsilon 0.38055109535903736, time 733.0, rides 114\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 1073, reward 99.0, memory_length 2000, epsilon 0.3802085993732142, time 732.0, rides 131\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1074, reward 276.0, memory_length 2000, epsilon 0.3798664116337783, time 727.0, rides 133\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1075, reward 435.0, memory_length 2000, epsilon 0.3795245318633079, time 726.0, rides 121\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 1076, reward 378.0, memory_length 2000, epsilon 0.37918295978463096, time 730.0, rides 132\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1077, reward 401.0, memory_length 2000, epsilon 0.3788416951208248, time 729.0, rides 121\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 1078, reward 445.0, memory_length 2000, epsilon 0.378500737595216, time 733.0, rides 143\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 1079, reward 430.0, memory_length 2000, epsilon 0.3781600869313803, time 733.0, rides 132\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 1080, reward 434.0, memory_length 2000, epsilon 0.37781974285314207, time 725.0, rides 142\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 1081, reward 356.0, memory_length 2000, epsilon 0.37747970508457424, time 731.0, rides 134\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 1082, reward 141.0, memory_length 2000, epsilon 0.3771399733499981, time 733.0, rides 144\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 1083, reward 465.0, memory_length 2000, epsilon 0.3768005473739831, time 726.0, rides 126\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 1084, reward 341.0, memory_length 2000, epsilon 0.37646142688134654, time 727.0, rides 125\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1085, reward 315.0, memory_length 2000, epsilon 0.3761226115971533, time 731.0, rides 131\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 1086, reward 605.0, memory_length 2000, epsilon 0.3757841012467159, time 722.0, rides 137\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 1087, reward 336.0, memory_length 2000, epsilon 0.3754458955555939, time 724.0, rides 137\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1088, reward 326.0, memory_length 2000, epsilon 0.37510799424959385, time 727.0, rides 157\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 1089, reward 197.0, memory_length 2000, epsilon 0.3747703970547692, time 732.0, rides 142\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 1090, reward 339.0, memory_length 2000, epsilon 0.37443310369741994, time 729.0, rides 127\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 1091, reward 492.0, memory_length 2000, epsilon 0.37409611390409225, time 730.0, rides 127\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 1092, reward 411.0, memory_length 2000, epsilon 0.3737594274015786, time 729.0, rides 123\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 1093, reward 346.0, memory_length 2000, epsilon 0.37342304391691716, time 728.0, rides 127\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 1094, reward 302.0, memory_length 2000, epsilon 0.37308696317739193, time 727.0, rides 134\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 1095, reward 405.0, memory_length 2000, epsilon 0.3727511849105323, time 730.0, rides 129\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 1096, reward 432.0, memory_length 2000, epsilon 0.3724157088441128, time 726.0, rides 129\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 1097, reward 348.0, memory_length 2000, epsilon 0.3720805347061531, time 727.0, rides 128\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 1098, reward 402.0, memory_length 2000, epsilon 0.37174566222491756, time 732.0, rides 129\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 1099, reward 367.0, memory_length 2000, epsilon 0.3714110911289151, time 733.0, rides 129\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1100, reward -54.0, memory_length 2000, epsilon 0.3710768211468991, time 724.0, rides 109\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 1101, reward 163.0, memory_length 2000, epsilon 0.3707428520078669, time 725.0, rides 118\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 1102, reward 478.0, memory_length 2000, epsilon 0.3704091834410598, time 727.0, rides 128\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 1103, reward 528.0, memory_length 2000, epsilon 0.37007581517596283, time 744.0, rides 139\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 1104, reward 417.0, memory_length 2000, epsilon 0.36974274694230447, time 724.0, rides 131\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 1105, reward 603.0, memory_length 2000, epsilon 0.3694099784700564, time 733.0, rides 119\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1106, reward 332.0, memory_length 2000, epsilon 0.36907750948943335, time 730.0, rides 127\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 1107, reward 182.0, memory_length 2000, epsilon 0.36874533973089285, time 741.0, rides 148\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 1108, reward 263.0, memory_length 2000, epsilon 0.36841346892513505, time 734.0, rides 142\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 1109, reward 59.0, memory_length 2000, epsilon 0.36808189680310244, time 741.0, rides 130\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 1110, reward 300.0, memory_length 2000, epsilon 0.36775062309597967, time 736.0, rides 127\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1111, reward 191.0, memory_length 2000, epsilon 0.3674196475351933, time 727.0, rides 131\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 1112, reward 432.0, memory_length 2000, epsilon 0.36708896985241163, time 733.0, rides 134\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 1113, reward 337.0, memory_length 2000, epsilon 0.36675858977954445, time 725.0, rides 145\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 1114, reward 282.0, memory_length 2000, epsilon 0.36642850704874286, time 730.0, rides 130\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 1115, reward 448.0, memory_length 2000, epsilon 0.36609872139239896, time 733.0, rides 135\n",
      "Initial State is  [3, 14, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1116, reward -146.0, memory_length 2000, epsilon 0.3657692325431458, time 739.0, rides 141\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 1117, reward 365.0, memory_length 2000, epsilon 0.36544004023385696, time 734.0, rides 128\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 1118, reward 310.0, memory_length 2000, epsilon 0.36511114419764645, time 727.0, rides 125\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 1119, reward 453.0, memory_length 2000, epsilon 0.3647825441678686, time 736.0, rides 131\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 1120, reward 672.0, memory_length 2000, epsilon 0.3644542398781175, time 727.0, rides 134\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1121, reward 424.0, memory_length 2000, epsilon 0.3641262310622272, time 726.0, rides 132\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 1122, reward 552.0, memory_length 2000, epsilon 0.36379851745427116, time 739.0, rides 139\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 1123, reward 522.0, memory_length 2000, epsilon 0.3634710987885623, time 727.0, rides 127\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 1124, reward 413.0, memory_length 2000, epsilon 0.3631439747996526, time 730.0, rides 125\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 1125, reward 284.0, memory_length 2000, epsilon 0.36281714522233294, time 732.0, rides 136\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 1126, reward 360.0, memory_length 2000, epsilon 0.36249060979163283, time 721.0, rides 116\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 1127, reward 393.0, memory_length 2000, epsilon 0.36216436824282033, time 723.0, rides 143\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1128, reward 555.0, memory_length 2000, epsilon 0.3618384203114018, time 729.0, rides 120\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 1129, reward 99.0, memory_length 2000, epsilon 0.3615127657331215, time 725.0, rides 147\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 1130, reward 369.0, memory_length 2000, epsilon 0.36118740424396173, time 736.0, rides 136\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1131, reward 358.0, memory_length 2000, epsilon 0.36086233558014214, time 734.0, rides 134\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 1132, reward 571.0, memory_length 2000, epsilon 0.36053755947812, time 725.0, rides 128\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 1133, reward 240.0, memory_length 2000, epsilon 0.36021307567458966, time 731.0, rides 118\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 1134, reward 498.0, memory_length 2000, epsilon 0.3598888839064825, time 733.0, rides 117\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1135, reward 615.0, memory_length 2000, epsilon 0.35956498391096664, time 726.0, rides 121\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 1136, reward 514.0, memory_length 2000, epsilon 0.35924137542544676, time 729.0, rides 138\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 1137, reward 383.0, memory_length 2000, epsilon 0.35891805818756384, time 726.0, rides 123\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 1138, reward 488.0, memory_length 2000, epsilon 0.35859503193519504, time 724.0, rides 115\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 1139, reward 469.0, memory_length 2000, epsilon 0.35827229640645336, time 728.0, rides 131\n",
      "Initial State is  [1, 16, 2]\n",
      "episode 1140, reward 503.0, memory_length 2000, epsilon 0.35794985133968754, time 732.0, rides 131\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1141, reward 434.0, memory_length 2000, epsilon 0.3576276964734818, time 731.0, rides 130\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 1142, reward 178.0, memory_length 2000, epsilon 0.3573058315466557, time 729.0, rides 133\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 1143, reward 462.0, memory_length 2000, epsilon 0.3569842562982637, time 727.0, rides 130\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 1144, reward 504.0, memory_length 2000, epsilon 0.35666297046759526, time 733.0, rides 132\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 1145, reward 298.0, memory_length 2000, epsilon 0.3563419737941744, time 726.0, rides 124\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 1146, reward 202.0, memory_length 2000, epsilon 0.3560212660177597, time 727.0, rides 132\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1147, reward 628.0, memory_length 2000, epsilon 0.3557008468783437, time 725.0, rides 123\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 1148, reward 292.0, memory_length 2000, epsilon 0.3553807161161532, time 724.0, rides 124\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 1149, reward 297.0, memory_length 2000, epsilon 0.35506087347164866, time 729.0, rides 124\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1150, reward 363.0, memory_length 2000, epsilon 0.35474131868552417, time 724.0, rides 124\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 1151, reward 127.0, memory_length 2000, epsilon 0.3544220514987072, time 735.0, rides 143\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1152, reward 418.0, memory_length 2000, epsilon 0.3541030716523584, time 740.0, rides 127\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 1153, reward 732.0, memory_length 2000, epsilon 0.35378437888787123, time 733.0, rides 140\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 1154, reward 202.0, memory_length 2000, epsilon 0.35346597294687215, time 722.0, rides 128\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 1155, reward 417.0, memory_length 2000, epsilon 0.35314785357121997, time 735.0, rides 122\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 1156, reward 147.0, memory_length 2000, epsilon 0.3528300205030059, time 735.0, rides 129\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 1157, reward 434.0, memory_length 2000, epsilon 0.35251247348455317, time 729.0, rides 121\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1158, reward 296.0, memory_length 2000, epsilon 0.35219521225841705, time 734.0, rides 126\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 1159, reward 505.0, memory_length 2000, epsilon 0.3518782365673845, time 725.0, rides 121\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 1160, reward 389.0, memory_length 2000, epsilon 0.3515615461544738, time 727.0, rides 129\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 1161, reward 568.0, memory_length 2000, epsilon 0.3512451407629348, time 725.0, rides 125\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 1162, reward 260.0, memory_length 2000, epsilon 0.35092902013624816, time 725.0, rides 124\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 1163, reward 205.0, memory_length 2000, epsilon 0.3506131840181255, time 725.0, rides 130\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1164, reward 666.0, memory_length 2000, epsilon 0.3502976321525092, time 734.0, rides 124\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 1165, reward 390.0, memory_length 2000, epsilon 0.34998236428357193, time 734.0, rides 126\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 1166, reward 268.0, memory_length 2000, epsilon 0.3496673801557167, time 725.0, rides 112\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1167, reward 52.0, memory_length 2000, epsilon 0.3493526795135765, time 728.0, rides 115\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 1168, reward 424.0, memory_length 2000, epsilon 0.3490382621020143, time 724.0, rides 143\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 1169, reward 631.0, memory_length 2000, epsilon 0.34872412766612243, time 736.0, rides 122\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 1170, reward 385.0, memory_length 2000, epsilon 0.3484102759512229, time 733.0, rides 135\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 1171, reward 296.0, memory_length 2000, epsilon 0.3480967067028668, time 723.0, rides 120\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1172, reward 266.0, memory_length 2000, epsilon 0.34778341966683424, time 728.0, rides 142\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 1173, reward 671.0, memory_length 2000, epsilon 0.3474704145891341, time 729.0, rides 119\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 1174, reward 602.0, memory_length 2000, epsilon 0.34715769121600387, time 738.0, rides 118\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1175, reward 325.0, memory_length 2000, epsilon 0.3468452492939095, time 734.0, rides 131\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 1176, reward 306.0, memory_length 2000, epsilon 0.34653308856954496, time 723.0, rides 142\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 1177, reward 403.0, memory_length 2000, epsilon 0.34622120878983237, time 727.0, rides 127\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 1178, reward 421.0, memory_length 2000, epsilon 0.3459096097019215, time 729.0, rides 127\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1179, reward 460.0, memory_length 2000, epsilon 0.3455982910531898, time 736.0, rides 129\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 1180, reward 594.0, memory_length 2000, epsilon 0.34528725259124193, time 724.0, rides 133\n",
      "Initial State is  [4, 17, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1181, reward 483.0, memory_length 2000, epsilon 0.3449764940639098, time 727.0, rides 129\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 1182, reward 208.0, memory_length 2000, epsilon 0.3446660152192523, time 725.0, rides 143\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 1183, reward 331.0, memory_length 2000, epsilon 0.34435581580555497, time 732.0, rides 125\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1184, reward 307.0, memory_length 2000, epsilon 0.34404589557133, time 723.0, rides 154\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 1185, reward 479.0, memory_length 2000, epsilon 0.3437362542653158, time 729.0, rides 131\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 1186, reward 333.0, memory_length 2000, epsilon 0.343426891636477, time 729.0, rides 149\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 1187, reward 118.0, memory_length 2000, epsilon 0.34311780743400416, time 730.0, rides 119\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1188, reward 619.0, memory_length 2000, epsilon 0.3428090014073136, time 728.0, rides 130\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 1189, reward 356.0, memory_length 2000, epsilon 0.342500473306047, time 726.0, rides 127\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 1190, reward 522.0, memory_length 2000, epsilon 0.34219222288007156, time 727.0, rides 115\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1191, reward 407.0, memory_length 2000, epsilon 0.3418842498794795, time 727.0, rides 127\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 1192, reward 459.0, memory_length 2000, epsilon 0.341576554054588, time 736.0, rides 134\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1193, reward 480.0, memory_length 2000, epsilon 0.3412691351559388, time 730.0, rides 136\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 1194, reward 338.0, memory_length 2000, epsilon 0.34096199293429846, time 737.0, rides 121\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 1195, reward 656.0, memory_length 2000, epsilon 0.3406551271406576, time 724.0, rides 144\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1196, reward 297.0, memory_length 2000, epsilon 0.340348537526231, time 724.0, rides 126\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 1197, reward 698.0, memory_length 2000, epsilon 0.3400422238424574, time 723.0, rides 144\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 1198, reward 758.0, memory_length 2000, epsilon 0.3397361858409992, time 727.0, rides 138\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 1199, reward 726.0, memory_length 2000, epsilon 0.3394304232737423, time 726.0, rides 142\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1200, reward 579.0, memory_length 2000, epsilon 0.3391249358927959, time 731.0, rides 125\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 1201, reward 386.0, memory_length 2000, epsilon 0.3388197234504924, time 735.0, rides 123\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 1202, reward 116.0, memory_length 2000, epsilon 0.3385147856993869, time 731.0, rides 121\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 1203, reward 294.0, memory_length 2000, epsilon 0.33821012239225745, time 743.0, rides 133\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 1204, reward 499.0, memory_length 2000, epsilon 0.33790573328210444, time 735.0, rides 123\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 1205, reward 149.0, memory_length 2000, epsilon 0.33760161812215056, time 730.0, rides 122\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1206, reward 349.0, memory_length 2000, epsilon 0.3372977766658406, time 723.0, rides 125\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 1207, reward 549.0, memory_length 2000, epsilon 0.3369942086668413, time 730.0, rides 130\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 1208, reward -93.0, memory_length 2000, epsilon 0.33669091387904115, time 736.0, rides 139\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 1209, reward 181.0, memory_length 2000, epsilon 0.33638789205655, time 736.0, rides 140\n",
      "Initial State is  [4, 13, 3]\n",
      "episode 1210, reward 292.0, memory_length 2000, epsilon 0.3360851429536991, time 727.0, rides 126\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1211, reward 243.0, memory_length 2000, epsilon 0.3357826663250408, time 729.0, rides 142\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 1212, reward 224.0, memory_length 2000, epsilon 0.33548046192534825, time 739.0, rides 136\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 1213, reward 278.0, memory_length 2000, epsilon 0.3351785295096154, time 733.0, rides 138\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 1214, reward 658.0, memory_length 2000, epsilon 0.33487686883305673, time 725.0, rides 132\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1215, reward 452.0, memory_length 2000, epsilon 0.33457547965110696, time 726.0, rides 133\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1216, reward 284.0, memory_length 2000, epsilon 0.33427436171942093, time 726.0, rides 141\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 1217, reward 420.0, memory_length 2000, epsilon 0.33397351479387344, time 737.0, rides 137\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1218, reward 164.0, memory_length 2000, epsilon 0.33367293863055897, time 727.0, rides 122\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 1219, reward 166.0, memory_length 2000, epsilon 0.33337263298579145, time 732.0, rides 135\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 1220, reward 312.0, memory_length 2000, epsilon 0.33307259761610425, time 723.0, rides 112\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 1221, reward 506.0, memory_length 2000, epsilon 0.3327728322782498, time 734.0, rides 140\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 1222, reward 695.0, memory_length 2000, epsilon 0.3324733367291993, time 726.0, rides 121\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 1223, reward 467.0, memory_length 2000, epsilon 0.33217411072614306, time 727.0, rides 129\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1224, reward 354.0, memory_length 2000, epsilon 0.33187515402648954, time 733.0, rides 128\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 1225, reward 468.0, memory_length 2000, epsilon 0.3315764663878657, time 726.0, rides 150\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 1226, reward 584.0, memory_length 2000, epsilon 0.33127804756811663, time 731.0, rides 129\n",
      "Initial State is  [3, 18, 6]\n",
      "episode 1227, reward 237.0, memory_length 2000, epsilon 0.3309798973253053, time 727.0, rides 132\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1228, reward 266.0, memory_length 2000, epsilon 0.3306820154177125, time 732.0, rides 126\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 1229, reward 362.0, memory_length 2000, epsilon 0.3303844016038366, time 732.0, rides 131\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 1230, reward 399.0, memory_length 2000, epsilon 0.33008705564239316, time 730.0, rides 125\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1231, reward 416.0, memory_length 2000, epsilon 0.329789977292315, time 734.0, rides 144\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1232, reward 229.0, memory_length 2000, epsilon 0.32949316631275194, time 730.0, rides 130\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 1233, reward 226.0, memory_length 2000, epsilon 0.32919662246307047, time 724.0, rides 123\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 1234, reward 405.0, memory_length 2000, epsilon 0.3289003455028537, time 733.0, rides 129\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1235, reward 395.0, memory_length 2000, epsilon 0.32860433519190113, time 728.0, rides 129\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 1236, reward 305.0, memory_length 2000, epsilon 0.32830859129022844, time 732.0, rides 140\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 1237, reward 407.0, memory_length 2000, epsilon 0.3280131135580672, time 733.0, rides 137\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 1238, reward 350.0, memory_length 2000, epsilon 0.32771790175586496, time 722.0, rides 137\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1239, reward 486.0, memory_length 2000, epsilon 0.3274229556442847, time 730.0, rides 134\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 1240, reward 538.0, memory_length 2000, epsilon 0.3271282749842048, time 724.0, rides 133\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 1241, reward 503.0, memory_length 2000, epsilon 0.32683385953671906, time 734.0, rides 126\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 1242, reward 84.0, memory_length 2000, epsilon 0.326539709063136, time 733.0, rides 136\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 1243, reward 386.0, memory_length 2000, epsilon 0.3262458233249792, time 735.0, rides 132\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 1244, reward 414.0, memory_length 2000, epsilon 0.32595220208398673, time 723.0, rides 123\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 1245, reward 554.0, memory_length 2000, epsilon 0.3256588451021111, time 726.0, rides 140\n",
      "Initial State is  [2, 19, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1246, reward 620.0, memory_length 2000, epsilon 0.32536575214151925, time 731.0, rides 137\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 1247, reward 170.0, memory_length 2000, epsilon 0.3250729229645919, time 730.0, rides 122\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 1248, reward 446.0, memory_length 2000, epsilon 0.32478035733392374, time 734.0, rides 116\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 1249, reward 414.0, memory_length 2000, epsilon 0.3244880550123232, time 726.0, rides 130\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 1250, reward 304.0, memory_length 2000, epsilon 0.3241960157628121, time 724.0, rides 129\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 1251, reward 361.0, memory_length 2000, epsilon 0.32390423934862556, time 726.0, rides 133\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 1252, reward 264.0, memory_length 2000, epsilon 0.3236127255332118, time 727.0, rides 139\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 1253, reward 272.0, memory_length 2000, epsilon 0.32332147408023193, time 735.0, rides 146\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 1254, reward 181.0, memory_length 2000, epsilon 0.3230304847535597, time 737.0, rides 116\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 1255, reward 461.0, memory_length 2000, epsilon 0.3227397573172815, time 722.0, rides 139\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 1256, reward 345.0, memory_length 2000, epsilon 0.32244929153569596, time 729.0, rides 119\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1257, reward 251.0, memory_length 2000, epsilon 0.3221590871733138, time 726.0, rides 131\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 1258, reward 858.0, memory_length 2000, epsilon 0.32186914399485783, time 729.0, rides 133\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 1259, reward 443.0, memory_length 2000, epsilon 0.3215794617652625, time 727.0, rides 132\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 1260, reward 347.0, memory_length 2000, epsilon 0.3212900402496737, time 727.0, rides 134\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1261, reward 495.0, memory_length 2000, epsilon 0.321000879213449, time 726.0, rides 119\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 1262, reward 471.0, memory_length 2000, epsilon 0.3207119784221569, time 724.0, rides 118\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 1263, reward 487.0, memory_length 2000, epsilon 0.32042333764157693, time 728.0, rides 142\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 1264, reward 153.0, memory_length 2000, epsilon 0.32013495663769953, time 724.0, rides 139\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 1265, reward 323.0, memory_length 2000, epsilon 0.3198468351767256, time 733.0, rides 138\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 1266, reward 215.0, memory_length 2000, epsilon 0.31955897302506653, time 728.0, rides 136\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 1267, reward 105.0, memory_length 2000, epsilon 0.31927136994934396, time 727.0, rides 139\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 1268, reward 275.0, memory_length 2000, epsilon 0.31898402571638956, time 733.0, rides 133\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 1269, reward 463.0, memory_length 2000, epsilon 0.3186969400932448, time 729.0, rides 131\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 1270, reward 237.0, memory_length 2000, epsilon 0.31841011284716086, time 721.0, rides 133\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 1271, reward 603.0, memory_length 2000, epsilon 0.3181235437455984, time 731.0, rides 122\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 1272, reward 308.0, memory_length 2000, epsilon 0.31783723255622737, time 725.0, rides 127\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1273, reward 387.0, memory_length 2000, epsilon 0.31755117904692676, time 726.0, rides 136\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 1274, reward 590.0, memory_length 2000, epsilon 0.3172653829857845, time 733.0, rides 133\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 1275, reward 110.0, memory_length 2000, epsilon 0.3169798441410973, time 734.0, rides 134\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 1276, reward 424.0, memory_length 2000, epsilon 0.31669456228137033, time 731.0, rides 134\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 1277, reward 814.0, memory_length 2000, epsilon 0.3164095371753171, time 729.0, rides 137\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 1278, reward 415.0, memory_length 2000, epsilon 0.31612476859185934, time 725.0, rides 120\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 1279, reward 423.0, memory_length 2000, epsilon 0.3158402563001267, time 741.0, rides 145\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 1280, reward 348.0, memory_length 2000, epsilon 0.31555600006945655, time 731.0, rides 126\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 1281, reward 429.0, memory_length 2000, epsilon 0.31527199966939407, time 730.0, rides 138\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 1282, reward 546.0, memory_length 2000, epsilon 0.3149882548696916, time 728.0, rides 135\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 1283, reward 189.0, memory_length 2000, epsilon 0.3147047654403089, time 730.0, rides 133\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 1284, reward 615.0, memory_length 2000, epsilon 0.31442153115141264, time 722.0, rides 118\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 1285, reward 426.0, memory_length 2000, epsilon 0.31413855177337635, time 728.0, rides 134\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 1286, reward 370.0, memory_length 2000, epsilon 0.31385582707678034, time 735.0, rides 143\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 1287, reward 507.0, memory_length 2000, epsilon 0.3135733568324112, time 730.0, rides 131\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 1288, reward 448.0, memory_length 2000, epsilon 0.313291140811262, time 728.0, rides 139\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 1289, reward 812.0, memory_length 2000, epsilon 0.3130091787845319, time 724.0, rides 138\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 1290, reward 364.0, memory_length 2000, epsilon 0.3127274705236258, time 727.0, rides 130\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 1291, reward 685.0, memory_length 2000, epsilon 0.3124460158001546, time 726.0, rides 131\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1292, reward 678.0, memory_length 2000, epsilon 0.3121648143859344, time 731.0, rides 141\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1293, reward 1000.0, memory_length 2000, epsilon 0.31188386605298707, time 739.0, rides 119\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 1294, reward 373.0, memory_length 2000, epsilon 0.31160317057353937, time 724.0, rides 130\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 1295, reward 479.0, memory_length 2000, epsilon 0.3113227277200232, time 737.0, rides 125\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 1296, reward 520.0, memory_length 2000, epsilon 0.31104253726507514, time 734.0, rides 132\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 1297, reward 444.0, memory_length 2000, epsilon 0.3107625989815366, time 730.0, rides 152\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 1298, reward 380.0, memory_length 2000, epsilon 0.3104829126424532, time 735.0, rides 136\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 1299, reward 471.0, memory_length 2000, epsilon 0.310203478021075, time 730.0, rides 129\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 1300, reward 504.0, memory_length 2000, epsilon 0.30992429489085604, time 733.0, rides 122\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 1301, reward 513.0, memory_length 2000, epsilon 0.30964536302545426, time 730.0, rides 130\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 1302, reward 548.0, memory_length 2000, epsilon 0.30936668219873137, time 730.0, rides 125\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 1303, reward 424.0, memory_length 2000, epsilon 0.3090882521847525, time 733.0, rides 121\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1304, reward 427.0, memory_length 2000, epsilon 0.3088100727577862, time 728.0, rides 122\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 1305, reward 150.0, memory_length 2000, epsilon 0.3085321436923042, time 732.0, rides 148\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 1306, reward 625.0, memory_length 2000, epsilon 0.30825446476298113, time 736.0, rides 127\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 1307, reward 409.0, memory_length 2000, epsilon 0.3079770357446944, time 734.0, rides 137\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1308, reward 741.0, memory_length 2000, epsilon 0.3076998564125242, time 728.0, rides 133\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 1309, reward 401.0, memory_length 2000, epsilon 0.3074229265417529, time 724.0, rides 122\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 1310, reward 439.0, memory_length 2000, epsilon 0.3071462459078653, time 731.0, rides 131\n",
      "Initial State is  [2, 14, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1311, reward 294.0, memory_length 2000, epsilon 0.30686981428654825, time 734.0, rides 134\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 1312, reward 237.0, memory_length 2000, epsilon 0.30659363145369034, time 736.0, rides 131\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 1313, reward 426.0, memory_length 2000, epsilon 0.30631769718538204, time 723.0, rides 139\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 1314, reward 285.0, memory_length 2000, epsilon 0.3060420112579152, time 727.0, rides 133\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 1315, reward 211.0, memory_length 2000, epsilon 0.3057665734477831, time 728.0, rides 117\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1316, reward 436.0, memory_length 2000, epsilon 0.30549138353168004, time 724.0, rides 129\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 1317, reward 295.0, memory_length 2000, epsilon 0.30521644128650155, time 734.0, rides 119\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1318, reward 645.0, memory_length 2000, epsilon 0.3049417464893437, time 725.0, rides 118\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1319, reward 574.0, memory_length 2000, epsilon 0.30466729891750327, time 727.0, rides 137\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 1320, reward 510.0, memory_length 2000, epsilon 0.3043930983484775, time 726.0, rides 142\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 1321, reward 539.0, memory_length 2000, epsilon 0.30411914455996386, time 733.0, rides 135\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 1322, reward 484.0, memory_length 2000, epsilon 0.3038454373298599, time 724.0, rides 130\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 1323, reward 376.0, memory_length 2000, epsilon 0.30357197643626305, time 729.0, rides 129\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1324, reward 454.0, memory_length 2000, epsilon 0.3032987616574704, time 730.0, rides 133\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 1325, reward 331.0, memory_length 2000, epsilon 0.30302579277197866, time 730.0, rides 134\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 1326, reward 458.0, memory_length 2000, epsilon 0.30275306955848386, time 739.0, rides 133\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1327, reward 304.0, memory_length 2000, epsilon 0.3024805917958812, time 733.0, rides 129\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1328, reward 613.0, memory_length 2000, epsilon 0.3022083592632649, time 732.0, rides 130\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 1329, reward 627.0, memory_length 2000, epsilon 0.30193637173992793, time 731.0, rides 139\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 1330, reward 510.0, memory_length 2000, epsilon 0.301664629005362, time 733.0, rides 113\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 1331, reward 83.0, memory_length 2000, epsilon 0.3013931308392572, time 726.0, rides 130\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1332, reward 178.0, memory_length 2000, epsilon 0.30112187702150184, time 727.0, rides 124\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1333, reward 555.0, memory_length 2000, epsilon 0.30085086733218247, time 730.0, rides 130\n",
      "Initial State is  [1, 9, 2]\n",
      "episode 1334, reward 418.0, memory_length 2000, epsilon 0.3005801015515835, time 729.0, rides 128\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1335, reward 368.0, memory_length 2000, epsilon 0.30030957946018705, time 731.0, rides 133\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 1336, reward 343.0, memory_length 2000, epsilon 0.30003930083867286, time 724.0, rides 128\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 1337, reward 630.0, memory_length 2000, epsilon 0.29976926546791804, time 726.0, rides 128\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1338, reward 520.0, memory_length 2000, epsilon 0.29949947312899694, time 727.0, rides 127\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1339, reward 517.0, memory_length 2000, epsilon 0.29922992360318085, time 722.0, rides 120\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 1340, reward 294.0, memory_length 2000, epsilon 0.29896061667193796, time 732.0, rides 125\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 1341, reward 332.0, memory_length 2000, epsilon 0.2986915521169332, time 722.0, rides 124\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 1342, reward 275.0, memory_length 2000, epsilon 0.298422729720028, time 725.0, rides 131\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 1343, reward 560.0, memory_length 2000, epsilon 0.29815414926327993, time 725.0, rides 118\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 1344, reward 406.0, memory_length 2000, epsilon 0.297885810528943, time 736.0, rides 126\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 1345, reward 410.0, memory_length 2000, epsilon 0.2976177132994669, time 733.0, rides 131\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 1346, reward 653.0, memory_length 2000, epsilon 0.29734985735749736, time 722.0, rides 132\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 1347, reward 421.0, memory_length 2000, epsilon 0.2970822424858756, time 733.0, rides 128\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 1348, reward 508.0, memory_length 2000, epsilon 0.2968148684676383, time 732.0, rides 129\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 1349, reward 506.0, memory_length 2000, epsilon 0.29654773508601745, time 728.0, rides 122\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 1350, reward 722.0, memory_length 2000, epsilon 0.29628084212444006, time 729.0, rides 141\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1351, reward 572.0, memory_length 2000, epsilon 0.29601418936652807, time 738.0, rides 132\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 1352, reward 320.0, memory_length 2000, epsilon 0.2957477765960982, time 725.0, rides 134\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 1353, reward 482.0, memory_length 2000, epsilon 0.2954816035971617, time 733.0, rides 123\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 1354, reward 399.0, memory_length 2000, epsilon 0.29521567015392425, time 726.0, rides 136\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 1355, reward 420.0, memory_length 2000, epsilon 0.2949499760507857, time 731.0, rides 137\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 1356, reward 215.0, memory_length 2000, epsilon 0.29468452107234, time 731.0, rides 122\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 1357, reward 214.0, memory_length 2000, epsilon 0.2944193050033749, time 735.0, rides 129\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1358, reward 647.0, memory_length 2000, epsilon 0.29415432762887184, time 735.0, rides 123\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 1359, reward 323.0, memory_length 2000, epsilon 0.29388958873400584, time 725.0, rides 139\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 1360, reward 307.0, memory_length 2000, epsilon 0.2936250881041452, time 721.0, rides 132\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 1361, reward 469.0, memory_length 2000, epsilon 0.2933608255248515, time 726.0, rides 124\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1362, reward 578.0, memory_length 2000, epsilon 0.2930968007818791, time 729.0, rides 122\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 1363, reward 416.0, memory_length 2000, epsilon 0.2928330136611754, time 725.0, rides 133\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 1364, reward 708.0, memory_length 2000, epsilon 0.29256946394888034, time 735.0, rides 129\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 1365, reward 549.0, memory_length 2000, epsilon 0.2923061514313263, time 732.0, rides 127\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 1366, reward 487.0, memory_length 2000, epsilon 0.29204307589503814, time 732.0, rides 137\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 1367, reward 282.0, memory_length 2000, epsilon 0.2917802371267326, time 728.0, rides 120\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 1368, reward 520.0, memory_length 2000, epsilon 0.29151763491331856, time 726.0, rides 128\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 1369, reward 529.0, memory_length 2000, epsilon 0.2912552690418966, time 731.0, rides 132\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 1370, reward 341.0, memory_length 2000, epsilon 0.2909931392997589, time 730.0, rides 127\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 1371, reward 448.0, memory_length 2000, epsilon 0.2907312454743891, time 728.0, rides 130\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 1372, reward 304.0, memory_length 2000, epsilon 0.2904695873534622, time 727.0, rides 127\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 1373, reward 564.0, memory_length 2000, epsilon 0.29020816472484406, time 736.0, rides 141\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 1374, reward 760.0, memory_length 2000, epsilon 0.2899469773765917, time 729.0, rides 143\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 1375, reward 364.0, memory_length 2000, epsilon 0.28968602509695274, time 722.0, rides 142\n",
      "Initial State is  [3, 10, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1376, reward 777.0, memory_length 2000, epsilon 0.2894253076743655, time 732.0, rides 134\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 1377, reward 786.0, memory_length 2000, epsilon 0.28916482489745854, time 731.0, rides 135\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 1378, reward 558.0, memory_length 2000, epsilon 0.2889045765550508, time 730.0, rides 133\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 1379, reward 568.0, memory_length 2000, epsilon 0.2886445624361513, time 732.0, rides 136\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 1380, reward 476.0, memory_length 2000, epsilon 0.28838478232995873, time 733.0, rides 133\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 1381, reward 419.0, memory_length 2000, epsilon 0.28812523602586176, time 723.0, rides 125\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 1382, reward 258.0, memory_length 2000, epsilon 0.28786592331343847, time 735.0, rides 142\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 1383, reward 568.0, memory_length 2000, epsilon 0.2876068439824564, time 723.0, rides 132\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 1384, reward 418.0, memory_length 2000, epsilon 0.2873479978228722, time 732.0, rides 128\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 1385, reward 171.0, memory_length 2000, epsilon 0.2870893846248316, time 727.0, rides 140\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 1386, reward 378.0, memory_length 2000, epsilon 0.28683100417866925, time 732.0, rides 155\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 1387, reward 435.0, memory_length 2000, epsilon 0.2865728562749084, time 728.0, rides 134\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 1388, reward 389.0, memory_length 2000, epsilon 0.286314940704261, time 733.0, rides 127\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 1389, reward 477.0, memory_length 2000, epsilon 0.28605725725762715, time 728.0, rides 131\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1390, reward 477.0, memory_length 2000, epsilon 0.28579980572609526, time 730.0, rides 132\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 1391, reward 597.0, memory_length 2000, epsilon 0.2855425859009418, time 728.0, rides 137\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 1392, reward 471.0, memory_length 2000, epsilon 0.2852855975736309, time 727.0, rides 131\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 1393, reward 366.0, memory_length 2000, epsilon 0.2850288405358147, time 736.0, rides 129\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 1394, reward 532.0, memory_length 2000, epsilon 0.28477231457933244, time 731.0, rides 135\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 1395, reward 405.0, memory_length 2000, epsilon 0.284516019496211, time 728.0, rides 155\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 1396, reward 500.0, memory_length 2000, epsilon 0.2842599550786644, time 728.0, rides 135\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 1397, reward 435.0, memory_length 2000, epsilon 0.2840041211190936, time 728.0, rides 123\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 1398, reward 782.0, memory_length 2000, epsilon 0.28374851741008644, time 731.0, rides 142\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 1399, reward 594.0, memory_length 2000, epsilon 0.28349314374441736, time 730.0, rides 133\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 1400, reward 603.0, memory_length 2000, epsilon 0.2832379999150474, time 736.0, rides 160\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 1401, reward 406.0, memory_length 2000, epsilon 0.2829830857151238, time 726.0, rides 140\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 1402, reward 552.0, memory_length 2000, epsilon 0.2827284009379802, time 724.0, rides 135\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1403, reward 603.0, memory_length 2000, epsilon 0.282473945377136, time 737.0, rides 146\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 1404, reward 593.0, memory_length 2000, epsilon 0.2822197188262966, time 724.0, rides 134\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1405, reward 412.0, memory_length 2000, epsilon 0.2819657210793529, time 732.0, rides 135\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 1406, reward 363.0, memory_length 2000, epsilon 0.2817119519303815, time 736.0, rides 131\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 1407, reward 559.0, memory_length 2000, epsilon 0.2814584111736442, time 728.0, rides 131\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 1408, reward 458.0, memory_length 2000, epsilon 0.2812050986035879, time 720.0, rides 141\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 1409, reward 274.0, memory_length 2000, epsilon 0.28095201401484465, time 721.0, rides 129\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 1410, reward 473.0, memory_length 2000, epsilon 0.28069915720223126, time 734.0, rides 129\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1411, reward 380.0, memory_length 2000, epsilon 0.28044652796074926, time 727.0, rides 120\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 1412, reward 499.0, memory_length 2000, epsilon 0.2801941260855846, time 728.0, rides 126\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 1413, reward 255.0, memory_length 2000, epsilon 0.27994195137210753, time 724.0, rides 117\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1414, reward 450.0, memory_length 2000, epsilon 0.27969000361587265, time 734.0, rides 137\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1415, reward 309.0, memory_length 2000, epsilon 0.2794382826126184, time 726.0, rides 132\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 1416, reward 389.0, memory_length 2000, epsilon 0.27918678815826703, time 728.0, rides 123\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 1417, reward 340.0, memory_length 2000, epsilon 0.2789355200489246, time 733.0, rides 131\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 1418, reward 613.0, memory_length 2000, epsilon 0.27868447808088054, time 727.0, rides 135\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 1419, reward 436.0, memory_length 2000, epsilon 0.27843366205060777, time 732.0, rides 133\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 1420, reward 543.0, memory_length 2000, epsilon 0.2781830717547622, time 724.0, rides 126\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 1421, reward 602.0, memory_length 2000, epsilon 0.2779327069901829, time 734.0, rides 139\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 1422, reward 528.0, memory_length 2000, epsilon 0.27768256755389176, time 725.0, rides 124\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 1423, reward 442.0, memory_length 2000, epsilon 0.27743265324309324, time 735.0, rides 129\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 1424, reward 103.0, memory_length 2000, epsilon 0.27718296385517444, time 746.0, rides 126\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 1425, reward 387.0, memory_length 2000, epsilon 0.27693349918770477, time 728.0, rides 141\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1426, reward 519.0, memory_length 2000, epsilon 0.27668425903843585, time 732.0, rides 122\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 1427, reward 640.0, memory_length 2000, epsilon 0.2764352432053013, time 729.0, rides 140\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 1428, reward 810.0, memory_length 2000, epsilon 0.2761864514864165, time 734.0, rides 136\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 1429, reward 477.0, memory_length 2000, epsilon 0.27593788368007877, time 726.0, rides 129\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 1430, reward 458.0, memory_length 2000, epsilon 0.2756895395847667, time 726.0, rides 137\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 1431, reward 263.0, memory_length 2000, epsilon 0.27544141899914043, time 720.0, rides 126\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1432, reward 783.0, memory_length 2000, epsilon 0.2751935217220412, time 738.0, rides 130\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 1433, reward 423.0, memory_length 2000, epsilon 0.27494584755249135, time 726.0, rides 141\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 1434, reward 494.0, memory_length 2000, epsilon 0.2746983962896941, time 727.0, rides 136\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 1435, reward 524.0, memory_length 2000, epsilon 0.2744511677330334, time 726.0, rides 127\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 1436, reward 747.0, memory_length 2000, epsilon 0.2742041616820737, time 730.0, rides 131\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 1437, reward 405.0, memory_length 2000, epsilon 0.27395737793655983, time 732.0, rides 135\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 1438, reward 541.0, memory_length 2000, epsilon 0.2737108162964169, time 725.0, rides 137\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 1439, reward 721.0, memory_length 2000, epsilon 0.27346447656175016, time 734.0, rides 135\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 1440, reward 465.0, memory_length 2000, epsilon 0.2732183585328446, time 731.0, rides 129\n",
      "Initial State is  [3, 21, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1441, reward 730.0, memory_length 2000, epsilon 0.272972462010165, time 726.0, rides 135\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 1442, reward 406.0, memory_length 2000, epsilon 0.27272678679435586, time 730.0, rides 141\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 1443, reward 402.0, memory_length 2000, epsilon 0.2724813326862409, time 739.0, rides 137\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 1444, reward 339.0, memory_length 2000, epsilon 0.2722360994868233, time 730.0, rides 131\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 1445, reward 563.0, memory_length 2000, epsilon 0.2719910869972852, time 725.0, rides 145\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 1446, reward 508.0, memory_length 2000, epsilon 0.2717462950189876, time 729.0, rides 128\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 1447, reward 524.0, memory_length 2000, epsilon 0.2715017233534705, time 731.0, rides 132\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1448, reward 799.0, memory_length 2000, epsilon 0.27125737180245235, time 730.0, rides 133\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 1449, reward 660.0, memory_length 2000, epsilon 0.2710132401678301, time 730.0, rides 147\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 1450, reward 593.0, memory_length 2000, epsilon 0.2707693282516791, time 736.0, rides 133\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 1451, reward 332.0, memory_length 2000, epsilon 0.2705256358562526, time 731.0, rides 133\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 1452, reward 573.0, memory_length 2000, epsilon 0.270282162783982, time 721.0, rides 138\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 1453, reward 250.0, memory_length 2000, epsilon 0.2700389088374764, time 727.0, rides 139\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 1454, reward 811.0, memory_length 2000, epsilon 0.26979587381952264, time 732.0, rides 140\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 1455, reward 606.0, memory_length 2000, epsilon 0.26955305753308506, time 724.0, rides 129\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1456, reward 318.0, memory_length 2000, epsilon 0.2693104597813053, time 733.0, rides 135\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 1457, reward 240.0, memory_length 2000, epsilon 0.2690680803675021, time 722.0, rides 129\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 1458, reward 557.0, memory_length 2000, epsilon 0.2688259190951714, time 723.0, rides 131\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 1459, reward 487.0, memory_length 2000, epsilon 0.26858397576798576, time 731.0, rides 128\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 1460, reward 545.0, memory_length 2000, epsilon 0.2683422501897946, time 734.0, rides 149\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 1461, reward 596.0, memory_length 2000, epsilon 0.26810074216462376, time 730.0, rides 138\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 1462, reward 412.0, memory_length 2000, epsilon 0.2678594514966756, time 725.0, rides 140\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1463, reward 447.0, memory_length 2000, epsilon 0.2676183779903286, time 727.0, rides 136\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 1464, reward 393.0, memory_length 2000, epsilon 0.2673775214501373, time 740.0, rides 116\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1465, reward 452.0, memory_length 2000, epsilon 0.2671368816808322, time 721.0, rides 152\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 1466, reward 629.0, memory_length 2000, epsilon 0.26689645848731947, time 733.0, rides 135\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 1467, reward 365.0, memory_length 2000, epsilon 0.2666562516746809, time 722.0, rides 126\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 1468, reward 559.0, memory_length 2000, epsilon 0.2664162610481737, time 729.0, rides 134\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 1469, reward 545.0, memory_length 2000, epsilon 0.26617648641323033, time 727.0, rides 125\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 1470, reward 658.0, memory_length 2000, epsilon 0.2659369275754584, time 735.0, rides 137\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 1471, reward 386.0, memory_length 2000, epsilon 0.2656975843406405, time 725.0, rides 126\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 1472, reward 610.0, memory_length 2000, epsilon 0.26545845651473393, time 730.0, rides 118\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 1473, reward 441.0, memory_length 2000, epsilon 0.26521954390387065, time 729.0, rides 131\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 1474, reward 569.0, memory_length 2000, epsilon 0.26498084631435714, time 733.0, rides 127\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 1475, reward 500.0, memory_length 2000, epsilon 0.2647423635526742, time 726.0, rides 134\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 1476, reward 425.0, memory_length 2000, epsilon 0.2645040954254768, time 727.0, rides 129\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1477, reward 752.0, memory_length 2000, epsilon 0.2642660417395939, time 736.0, rides 129\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1478, reward 408.0, memory_length 2000, epsilon 0.2640282023020283, time 726.0, rides 125\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 1479, reward 603.0, memory_length 2000, epsilon 0.26379057691995644, time 728.0, rides 126\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 1480, reward 627.0, memory_length 2000, epsilon 0.2635531654007285, time 725.0, rides 138\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 1481, reward 496.0, memory_length 2000, epsilon 0.2633159675518678, time 734.0, rides 130\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 1482, reward 517.0, memory_length 2000, epsilon 0.26307898318107115, time 728.0, rides 135\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 1483, reward 482.0, memory_length 2000, epsilon 0.26284221209620817, time 732.0, rides 138\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 1484, reward 635.0, memory_length 2000, epsilon 0.2626056541053216, time 725.0, rides 146\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 1485, reward 474.0, memory_length 2000, epsilon 0.2623693090166268, time 727.0, rides 120\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1486, reward 425.0, memory_length 2000, epsilon 0.2621331766385118, time 727.0, rides 118\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 1487, reward 702.0, memory_length 2000, epsilon 0.2618972567795372, time 730.0, rides 134\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 1488, reward 687.0, memory_length 2000, epsilon 0.2616615492484356, time 731.0, rides 132\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 1489, reward 603.0, memory_length 2000, epsilon 0.261426053854112, time 731.0, rides 129\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 1490, reward 380.0, memory_length 2000, epsilon 0.2611907704056433, time 725.0, rides 133\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 1491, reward 661.0, memory_length 2000, epsilon 0.26095569871227825, time 732.0, rides 135\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1492, reward 475.0, memory_length 2000, epsilon 0.2607208385834372, time 727.0, rides 129\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 1493, reward 751.0, memory_length 2000, epsilon 0.2604861898287121, time 728.0, rides 128\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 1494, reward 292.0, memory_length 2000, epsilon 0.26025175225786623, time 735.0, rides 149\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1495, reward 709.0, memory_length 2000, epsilon 0.26001752568083414, time 728.0, rides 123\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1496, reward 591.0, memory_length 2000, epsilon 0.25978350990772137, time 733.0, rides 145\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 1497, reward 438.0, memory_length 2000, epsilon 0.2595497047488044, time 730.0, rides 128\n",
      "Initial State is  [0, 12, 0]\n",
      "episode 1498, reward 581.0, memory_length 2000, epsilon 0.2593161100145305, time 732.0, rides 130\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1499, reward 161.0, memory_length 2000, epsilon 0.2590827255155174, time 725.0, rides 131\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 1500, reward 539.0, memory_length 2000, epsilon 0.2588495510625535, time 727.0, rides 131\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 1501, reward 267.0, memory_length 2000, epsilon 0.2586165864665972, time 728.0, rides 117\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1502, reward 467.0, memory_length 2000, epsilon 0.25838383153877725, time 722.0, rides 118\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 1503, reward 851.0, memory_length 2000, epsilon 0.25815128609039234, time 731.0, rides 142\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 1504, reward 542.0, memory_length 2000, epsilon 0.257918949932911, time 729.0, rides 133\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 1505, reward 437.0, memory_length 2000, epsilon 0.25768682287797134, time 731.0, rides 124\n",
      "Initial State is  [3, 18, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1506, reward 486.0, memory_length 2000, epsilon 0.25745490473738114, time 729.0, rides 142\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1507, reward 321.0, memory_length 2000, epsilon 0.2572231953231175, time 722.0, rides 141\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 1508, reward 98.0, memory_length 2000, epsilon 0.25699169444732667, time 725.0, rides 135\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 1509, reward 890.0, memory_length 2000, epsilon 0.2567604019223241, time 729.0, rides 145\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1510, reward 444.0, memory_length 2000, epsilon 0.256529317560594, time 737.0, rides 125\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 1511, reward 426.0, memory_length 2000, epsilon 0.25629844117478945, time 734.0, rides 138\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 1512, reward 471.0, memory_length 2000, epsilon 0.2560677725777321, time 728.0, rides 133\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 1513, reward 321.0, memory_length 2000, epsilon 0.25583731158241213, time 725.0, rides 127\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 1514, reward 397.0, memory_length 2000, epsilon 0.25560705800198796, time 735.0, rides 127\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 1515, reward 417.0, memory_length 2000, epsilon 0.25537701164978616, time 729.0, rides 124\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 1516, reward 587.0, memory_length 2000, epsilon 0.25514717233930134, time 732.0, rides 119\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1517, reward 816.0, memory_length 2000, epsilon 0.25491753988419596, time 723.0, rides 137\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 1518, reward 375.0, memory_length 2000, epsilon 0.25468811409830017, time 731.0, rides 138\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 1519, reward 706.0, memory_length 2000, epsilon 0.2544588947956117, time 723.0, rides 125\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 1520, reward 401.0, memory_length 2000, epsilon 0.2542298817902956, time 737.0, rides 140\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 1521, reward 442.0, memory_length 2000, epsilon 0.25400107489668433, time 734.0, rides 140\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 1522, reward 375.0, memory_length 2000, epsilon 0.2537724739292773, time 722.0, rides 128\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 1523, reward 663.0, memory_length 2000, epsilon 0.253544078702741, time 736.0, rides 136\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 1524, reward 579.0, memory_length 2000, epsilon 0.2533158890319085, time 729.0, rides 134\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1525, reward 547.0, memory_length 2000, epsilon 0.2530879047317798, time 736.0, rides 152\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 1526, reward 392.0, memory_length 2000, epsilon 0.2528601256175212, time 723.0, rides 124\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 1527, reward 511.0, memory_length 2000, epsilon 0.2526325515044654, time 730.0, rides 134\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 1528, reward 535.0, memory_length 2000, epsilon 0.25240518220811137, time 722.0, rides 125\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 1529, reward 190.0, memory_length 2000, epsilon 0.25217801754412406, time 730.0, rides 138\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 1530, reward 487.0, memory_length 2000, epsilon 0.25195105732833434, time 724.0, rides 135\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 1531, reward 601.0, memory_length 2000, epsilon 0.25172430137673885, time 727.0, rides 131\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 1532, reward 641.0, memory_length 2000, epsilon 0.2514977495054998, time 731.0, rides 135\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 1533, reward 378.0, memory_length 2000, epsilon 0.25127140153094485, time 721.0, rides 130\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 1534, reward 546.0, memory_length 2000, epsilon 0.251045257269567, time 729.0, rides 139\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 1535, reward 380.0, memory_length 2000, epsilon 0.25081931653802436, time 726.0, rides 127\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 1536, reward 221.0, memory_length 2000, epsilon 0.2505935791531401, time 727.0, rides 141\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 1537, reward 535.0, memory_length 2000, epsilon 0.2503680449319023, time 727.0, rides 126\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 1538, reward 519.0, memory_length 2000, epsilon 0.25014271369146357, time 724.0, rides 129\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 1539, reward 454.0, memory_length 2000, epsilon 0.24991758524914126, time 738.0, rides 133\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 1540, reward 619.0, memory_length 2000, epsilon 0.24969265942241703, time 728.0, rides 126\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1541, reward 618.0, memory_length 2000, epsilon 0.24946793602893685, time 728.0, rides 139\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 1542, reward 686.0, memory_length 2000, epsilon 0.2492434148865108, time 728.0, rides 135\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 1543, reward 540.0, memory_length 2000, epsilon 0.24901909581311293, time 728.0, rides 138\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 1544, reward 423.0, memory_length 2000, epsilon 0.24879497862688113, time 737.0, rides 127\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 1545, reward 398.0, memory_length 2000, epsilon 0.24857106314611693, time 726.0, rides 139\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1546, reward 544.0, memory_length 2000, epsilon 0.24834734918928542, time 724.0, rides 146\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 1547, reward 631.0, memory_length 2000, epsilon 0.24812383657501505, time 725.0, rides 130\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 1548, reward 657.0, memory_length 2000, epsilon 0.24790052512209754, time 728.0, rides 130\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1549, reward 707.0, memory_length 2000, epsilon 0.24767741464948764, time 727.0, rides 139\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1550, reward 536.0, memory_length 2000, epsilon 0.2474545049763031, time 738.0, rides 140\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 1551, reward 473.0, memory_length 2000, epsilon 0.24723179592182443, time 732.0, rides 142\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1552, reward 402.0, memory_length 2000, epsilon 0.2470092873054948, time 725.0, rides 130\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 1553, reward 596.0, memory_length 2000, epsilon 0.24678697894691984, time 732.0, rides 129\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 1554, reward 320.0, memory_length 2000, epsilon 0.2465648706658676, time 726.0, rides 123\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 1555, reward 778.0, memory_length 2000, epsilon 0.24634296228226832, time 729.0, rides 121\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1556, reward 397.0, memory_length 2000, epsilon 0.24612125361621429, time 728.0, rides 142\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 1557, reward 597.0, memory_length 2000, epsilon 0.2458997444879597, time 724.0, rides 137\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 1558, reward 473.0, memory_length 2000, epsilon 0.24567843471792053, time 728.0, rides 143\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 1559, reward 438.0, memory_length 2000, epsilon 0.2454573241266744, time 725.0, rides 132\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1560, reward 956.0, memory_length 2000, epsilon 0.2452364125349604, time 723.0, rides 121\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 1561, reward 573.0, memory_length 2000, epsilon 0.24501569976367893, time 722.0, rides 123\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 1562, reward 287.0, memory_length 2000, epsilon 0.2447951856338916, time 727.0, rides 136\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 1563, reward 294.0, memory_length 2000, epsilon 0.2445748699668211, time 723.0, rides 121\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 1564, reward 501.0, memory_length 2000, epsilon 0.24435475258385095, time 724.0, rides 126\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 1565, reward 379.0, memory_length 2000, epsilon 0.24413483330652547, time 733.0, rides 112\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 1566, reward 396.0, memory_length 2000, epsilon 0.24391511195654958, time 727.0, rides 135\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 1567, reward 458.0, memory_length 2000, epsilon 0.2436955883557887, time 723.0, rides 142\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 1568, reward 420.0, memory_length 2000, epsilon 0.24347626232626848, time 735.0, rides 117\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1569, reward 375.0, memory_length 2000, epsilon 0.24325713369017485, time 730.0, rides 140\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1570, reward 287.0, memory_length 2000, epsilon 0.24303820226985368, time 723.0, rides 115\n",
      "Initial State is  [4, 5, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1571, reward 530.0, memory_length 2000, epsilon 0.2428194678878108, time 736.0, rides 121\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 1572, reward 380.0, memory_length 2000, epsilon 0.24260093036671176, time 728.0, rides 141\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 1573, reward 322.0, memory_length 2000, epsilon 0.24238258952938171, time 735.0, rides 132\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 1574, reward 492.0, memory_length 2000, epsilon 0.24216444519880526, time 735.0, rides 129\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 1575, reward 456.0, memory_length 2000, epsilon 0.24194649719812633, time 729.0, rides 128\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 1576, reward 300.0, memory_length 2000, epsilon 0.241728745350648, time 728.0, rides 114\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 1577, reward 596.0, memory_length 2000, epsilon 0.2415111894798324, time 721.0, rides 125\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 1578, reward 493.0, memory_length 2000, epsilon 0.24129382940930055, time 729.0, rides 134\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 1579, reward 524.0, memory_length 2000, epsilon 0.24107666496283217, time 726.0, rides 138\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 1580, reward 336.0, memory_length 2000, epsilon 0.24085969596436563, time 734.0, rides 142\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 1581, reward 735.0, memory_length 2000, epsilon 0.2406429222379977, time 725.0, rides 132\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1582, reward 455.0, memory_length 2000, epsilon 0.24042634360798348, time 735.0, rides 136\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 1583, reward 245.0, memory_length 2000, epsilon 0.2402099598987363, time 727.0, rides 142\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 1584, reward 404.0, memory_length 2000, epsilon 0.23999377093482743, time 731.0, rides 114\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 1585, reward 420.0, memory_length 2000, epsilon 0.2397777765409861, time 730.0, rides 137\n",
      "Initial State is  [1, 2, 6]\n",
      "episode 1586, reward 460.0, memory_length 2000, epsilon 0.2395619765420992, time 735.0, rides 127\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 1587, reward 571.0, memory_length 2000, epsilon 0.23934637076321133, time 723.0, rides 133\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 1588, reward 621.0, memory_length 2000, epsilon 0.23913095902952444, time 730.0, rides 135\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 1589, reward 379.0, memory_length 2000, epsilon 0.23891574116639785, time 731.0, rides 132\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1590, reward 693.0, memory_length 2000, epsilon 0.23870071699934808, time 726.0, rides 121\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 1591, reward 340.0, memory_length 2000, epsilon 0.23848588635404866, time 733.0, rides 131\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 1592, reward 576.0, memory_length 2000, epsilon 0.23827124905633001, time 727.0, rides 139\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 1593, reward 749.0, memory_length 2000, epsilon 0.23805680493217932, time 733.0, rides 137\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 1594, reward 527.0, memory_length 2000, epsilon 0.23784255380774036, time 730.0, rides 149\n",
      "Initial State is  [4, 18, 5]\n",
      "episode 1595, reward 538.0, memory_length 2000, epsilon 0.2376284955093134, time 725.0, rides 128\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1596, reward 622.0, memory_length 2000, epsilon 0.23741462986335501, time 729.0, rides 128\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1597, reward 387.0, memory_length 2000, epsilon 0.237200956696478, time 729.0, rides 133\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 1598, reward 594.0, memory_length 2000, epsilon 0.23698747583545118, time 723.0, rides 128\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 1599, reward 702.0, memory_length 2000, epsilon 0.23677418710719927, time 729.0, rides 126\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1600, reward 675.0, memory_length 2000, epsilon 0.2365610903388028, time 732.0, rides 138\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1601, reward 655.0, memory_length 2000, epsilon 0.23634818535749788, time 729.0, rides 137\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 1602, reward 574.0, memory_length 2000, epsilon 0.23613547199067614, time 734.0, rides 136\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 1603, reward 424.0, memory_length 2000, epsilon 0.23592295006588454, time 736.0, rides 142\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 1604, reward 688.0, memory_length 2000, epsilon 0.23571061941082525, time 731.0, rides 132\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 1605, reward 857.0, memory_length 2000, epsilon 0.2354984798533555, time 728.0, rides 139\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 1606, reward 270.0, memory_length 2000, epsilon 0.2352865312214875, time 734.0, rides 128\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 1607, reward 546.0, memory_length 2000, epsilon 0.23507477334338814, time 729.0, rides 129\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 1608, reward 554.0, memory_length 2000, epsilon 0.2348632060473791, time 727.0, rides 133\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1609, reward 477.0, memory_length 2000, epsilon 0.23465182916193644, time 728.0, rides 127\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 1610, reward 120.0, memory_length 2000, epsilon 0.2344406425156907, time 727.0, rides 131\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 1611, reward 466.0, memory_length 2000, epsilon 0.23422964593742657, time 731.0, rides 133\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 1612, reward 267.0, memory_length 2000, epsilon 0.23401883925608288, time 730.0, rides 121\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 1613, reward 285.0, memory_length 2000, epsilon 0.2338082223007524, time 731.0, rides 121\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 1614, reward 518.0, memory_length 2000, epsilon 0.23359779490068172, time 725.0, rides 130\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 1615, reward 552.0, memory_length 2000, epsilon 0.2333875568852711, time 733.0, rides 129\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1616, reward 520.0, memory_length 2000, epsilon 0.23317750808407436, time 731.0, rides 131\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 1617, reward 285.0, memory_length 2000, epsilon 0.2329676483267987, time 725.0, rides 130\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 1618, reward 687.0, memory_length 2000, epsilon 0.23275797744330456, time 739.0, rides 134\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 1619, reward 650.0, memory_length 2000, epsilon 0.23254849526360558, time 721.0, rides 125\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 1620, reward 804.0, memory_length 2000, epsilon 0.23233920161786834, time 725.0, rides 148\n",
      "Initial State is  [4, 4, 4]\n",
      "episode 1621, reward 579.0, memory_length 2000, epsilon 0.23213009633641224, time 732.0, rides 131\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 1622, reward 473.0, memory_length 2000, epsilon 0.23192117924970945, time 731.0, rides 137\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 1623, reward 314.0, memory_length 2000, epsilon 0.2317124501883847, time 738.0, rides 136\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 1624, reward 868.0, memory_length 2000, epsilon 0.23150390898321516, time 731.0, rides 143\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 1625, reward 720.0, memory_length 2000, epsilon 0.23129555546513025, time 727.0, rides 131\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 1626, reward 335.0, memory_length 2000, epsilon 0.23108738946521162, time 731.0, rides 133\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 1627, reward 576.0, memory_length 2000, epsilon 0.23087941081469293, time 730.0, rides 125\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 1628, reward 519.0, memory_length 2000, epsilon 0.2306716193449597, time 724.0, rides 131\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 1629, reward 597.0, memory_length 2000, epsilon 0.23046401488754922, time 726.0, rides 131\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 1630, reward 288.0, memory_length 2000, epsilon 0.23025659727415043, time 730.0, rides 125\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 1631, reward 634.0, memory_length 2000, epsilon 0.2300493663366037, time 725.0, rides 141\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 1632, reward 578.0, memory_length 2000, epsilon 0.22984232190690074, time 729.0, rides 137\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 1633, reward 757.0, memory_length 2000, epsilon 0.22963546381718453, time 725.0, rides 130\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 1634, reward 660.0, memory_length 2000, epsilon 0.22942879189974905, time 727.0, rides 134\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 1635, reward 511.0, memory_length 2000, epsilon 0.22922230598703927, time 727.0, rides 150\n",
      "Initial State is  [1, 21, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1636, reward 715.0, memory_length 2000, epsilon 0.22901600591165094, time 732.0, rides 146\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 1637, reward 527.0, memory_length 2000, epsilon 0.22880989150633047, time 729.0, rides 129\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1638, reward 643.0, memory_length 2000, epsilon 0.22860396260397475, time 730.0, rides 134\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 1639, reward 313.0, memory_length 2000, epsilon 0.22839821903763116, time 734.0, rides 132\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 1640, reward 452.0, memory_length 2000, epsilon 0.22819266064049729, time 732.0, rides 142\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 1641, reward 336.0, memory_length 2000, epsilon 0.22798728724592082, time 729.0, rides 140\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 1642, reward 907.0, memory_length 2000, epsilon 0.2277820986873995, time 727.0, rides 143\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 1643, reward 293.0, memory_length 2000, epsilon 0.22757709479858082, time 723.0, rides 150\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 1644, reward 710.0, memory_length 2000, epsilon 0.2273722754132621, time 729.0, rides 139\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 1645, reward 847.0, memory_length 2000, epsilon 0.22716764036539017, time 734.0, rides 155\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 1646, reward 647.0, memory_length 2000, epsilon 0.2269631894890613, time 726.0, rides 150\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 1647, reward 393.0, memory_length 2000, epsilon 0.22675892261852115, time 741.0, rides 127\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 1648, reward 484.0, memory_length 2000, epsilon 0.22655483958816447, time 733.0, rides 132\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 1649, reward 618.0, memory_length 2000, epsilon 0.22635094023253513, time 726.0, rides 122\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 1650, reward 794.0, memory_length 2000, epsilon 0.22614722438632584, time 725.0, rides 128\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 1651, reward 548.0, memory_length 2000, epsilon 0.22594369188437816, time 734.0, rides 144\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 1652, reward 493.0, memory_length 2000, epsilon 0.2257403425616822, time 731.0, rides 127\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1653, reward 309.0, memory_length 2000, epsilon 0.2255371762533767, time 722.0, rides 135\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1654, reward 520.0, memory_length 2000, epsilon 0.22533419279474864, time 728.0, rides 141\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 1655, reward 577.0, memory_length 2000, epsilon 0.22513139202123336, time 728.0, rides 126\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 1656, reward 441.0, memory_length 2000, epsilon 0.22492877376841425, time 723.0, rides 135\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 1657, reward 503.0, memory_length 2000, epsilon 0.22472633787202267, time 727.0, rides 143\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 1658, reward 569.0, memory_length 2000, epsilon 0.22452408416793784, time 726.0, rides 122\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 1659, reward 517.0, memory_length 2000, epsilon 0.2243220124921867, time 731.0, rides 119\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 1660, reward 448.0, memory_length 2000, epsilon 0.22412012268094372, time 731.0, rides 132\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 1661, reward 186.0, memory_length 2000, epsilon 0.22391841457053088, time 721.0, rides 125\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 1662, reward 444.0, memory_length 2000, epsilon 0.2237168879974174, time 729.0, rides 127\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 1663, reward 497.0, memory_length 2000, epsilon 0.22351554279821972, time 724.0, rides 132\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 1664, reward 620.0, memory_length 2000, epsilon 0.2233143788097013, time 723.0, rides 140\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 1665, reward 413.0, memory_length 2000, epsilon 0.22311339586877257, time 728.0, rides 129\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 1666, reward 501.0, memory_length 2000, epsilon 0.22291259381249068, time 731.0, rides 126\n",
      "Initial State is  [0, 16, 0]\n",
      "episode 1667, reward 466.0, memory_length 2000, epsilon 0.22271197247805943, time 732.0, rides 129\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 1668, reward 383.0, memory_length 2000, epsilon 0.22251153170282917, time 725.0, rides 130\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 1669, reward 297.0, memory_length 2000, epsilon 0.22231127132429662, time 730.0, rides 128\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 1670, reward 764.0, memory_length 2000, epsilon 0.22211119118010475, time 729.0, rides 140\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 1671, reward 552.0, memory_length 2000, epsilon 0.22191129110804264, time 728.0, rides 132\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 1672, reward 558.0, memory_length 2000, epsilon 0.2217115709460454, time 726.0, rides 119\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 1673, reward 356.0, memory_length 2000, epsilon 0.22151203053219395, time 728.0, rides 140\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 1674, reward 440.0, memory_length 2000, epsilon 0.22131266970471497, time 729.0, rides 137\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 1675, reward 573.0, memory_length 2000, epsilon 0.22111348830198072, time 721.0, rides 121\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 1676, reward 713.0, memory_length 2000, epsilon 0.22091448616250892, time 731.0, rides 126\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 1677, reward 338.0, memory_length 2000, epsilon 0.22071566312496266, time 728.0, rides 125\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 1678, reward 543.0, memory_length 2000, epsilon 0.22051701902815019, time 726.0, rides 131\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 1679, reward 420.0, memory_length 2000, epsilon 0.22031855371102485, time 730.0, rides 125\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 1680, reward 495.0, memory_length 2000, epsilon 0.22012026701268492, time 729.0, rides 120\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 1681, reward 435.0, memory_length 2000, epsilon 0.2199221587723735, time 733.0, rides 130\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 1682, reward 299.0, memory_length 2000, epsilon 0.21972422882947837, time 728.0, rides 137\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 1683, reward 661.0, memory_length 2000, epsilon 0.21952647702353184, time 721.0, rides 115\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 1684, reward 563.0, memory_length 2000, epsilon 0.21932890319421067, time 733.0, rides 129\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1685, reward 603.0, memory_length 2000, epsilon 0.21913150718133587, time 738.0, rides 126\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 1686, reward 250.0, memory_length 2000, epsilon 0.21893428882487267, time 728.0, rides 127\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 1687, reward 719.0, memory_length 2000, epsilon 0.2187372479649303, time 730.0, rides 146\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 1688, reward 423.0, memory_length 2000, epsilon 0.21854038444176185, time 722.0, rides 146\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 1689, reward 431.0, memory_length 2000, epsilon 0.21834369809576426, time 737.0, rides 135\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 1690, reward 295.0, memory_length 2000, epsilon 0.21814718876747807, time 724.0, rides 137\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 1691, reward 542.0, memory_length 2000, epsilon 0.21795085629758734, time 726.0, rides 134\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1692, reward 349.0, memory_length 2000, epsilon 0.2177547005269195, time 729.0, rides 130\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 1693, reward 453.0, memory_length 2000, epsilon 0.21755872129644527, time 729.0, rides 124\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 1694, reward 594.0, memory_length 2000, epsilon 0.21736291844727845, time 723.0, rides 126\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 1695, reward 450.0, memory_length 2000, epsilon 0.2171672918206759, time 729.0, rides 134\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 1696, reward 414.0, memory_length 2000, epsilon 0.21697184125803728, time 735.0, rides 126\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1697, reward 423.0, memory_length 2000, epsilon 0.21677656660090505, time 731.0, rides 125\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 1698, reward 443.0, memory_length 2000, epsilon 0.21658146769096423, time 727.0, rides 128\n",
      "Initial State is  [3, 23, 3]\n",
      "episode 1699, reward 136.0, memory_length 2000, epsilon 0.21638654437004237, time 732.0, rides 129\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 1700, reward 180.0, memory_length 2000, epsilon 0.21619179648010933, time 733.0, rides 139\n",
      "Initial State is  [3, 16, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1701, reward 237.0, memory_length 2000, epsilon 0.21599722386327724, time 726.0, rides 155\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 1702, reward 671.0, memory_length 2000, epsilon 0.2158028263618003, time 729.0, rides 136\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 1703, reward 377.0, memory_length 2000, epsilon 0.21560860381807467, time 726.0, rides 132\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 1704, reward 646.0, memory_length 2000, epsilon 0.2154145560746384, time 733.0, rides 127\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 1705, reward 670.0, memory_length 2000, epsilon 0.21522068297417124, time 723.0, rides 150\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 1706, reward 613.0, memory_length 2000, epsilon 0.2150269843594945, time 724.0, rides 132\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 1707, reward 478.0, memory_length 2000, epsilon 0.21483346007357096, time 730.0, rides 146\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 1708, reward 424.0, memory_length 2000, epsilon 0.21464010995950475, time 730.0, rides 132\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 1709, reward 158.0, memory_length 2000, epsilon 0.2144469338605412, time 726.0, rides 125\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 1710, reward 603.0, memory_length 2000, epsilon 0.2142539316200667, time 724.0, rides 132\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 1711, reward 605.0, memory_length 2000, epsilon 0.21406110308160864, time 729.0, rides 153\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 1712, reward 618.0, memory_length 2000, epsilon 0.21386844808883518, time 730.0, rides 156\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 1713, reward 767.0, memory_length 2000, epsilon 0.21367596648555523, time 730.0, rides 138\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 1714, reward 676.0, memory_length 2000, epsilon 0.21348365811571823, time 726.0, rides 128\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 1715, reward 525.0, memory_length 2000, epsilon 0.2132915228234141, time 724.0, rides 132\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 1716, reward 428.0, memory_length 2000, epsilon 0.213099560452873, time 730.0, rides 139\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 1717, reward 385.0, memory_length 2000, epsilon 0.21290777084846543, time 736.0, rides 137\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 1718, reward 644.0, memory_length 2000, epsilon 0.2127161538547018, time 730.0, rides 139\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 1719, reward 464.0, memory_length 2000, epsilon 0.21252470931623257, time 723.0, rides 124\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 1720, reward 559.0, memory_length 2000, epsilon 0.21233343707784796, time 730.0, rides 139\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 1721, reward 687.0, memory_length 2000, epsilon 0.2121423369844779, time 730.0, rides 137\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 1722, reward 602.0, memory_length 2000, epsilon 0.21195140888119188, time 731.0, rides 135\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 1723, reward 604.0, memory_length 2000, epsilon 0.2117606526131988, time 726.0, rides 147\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1724, reward 729.0, memory_length 2000, epsilon 0.2115700680258469, time 729.0, rides 127\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 1725, reward 798.0, memory_length 2000, epsilon 0.21137965496462363, time 732.0, rides 125\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 1726, reward 674.0, memory_length 2000, epsilon 0.21118941327515547, time 725.0, rides 120\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 1727, reward 939.0, memory_length 2000, epsilon 0.21099934280320784, time 734.0, rides 143\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 1728, reward 155.0, memory_length 2000, epsilon 0.21080944339468494, time 729.0, rides 130\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 1729, reward 774.0, memory_length 2000, epsilon 0.21061971489562972, time 732.0, rides 142\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 1730, reward 595.0, memory_length 2000, epsilon 0.21043015715222366, time 725.0, rides 131\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 1731, reward 332.0, memory_length 2000, epsilon 0.21024077001078664, time 727.0, rides 130\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 1732, reward 825.0, memory_length 2000, epsilon 0.21005155331777695, time 732.0, rides 130\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 1733, reward 335.0, memory_length 2000, epsilon 0.20986250691979094, time 734.0, rides 127\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 1734, reward 565.0, memory_length 2000, epsilon 0.20967363066356312, time 725.0, rides 122\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 1735, reward 563.0, memory_length 2000, epsilon 0.20948492439596592, time 728.0, rides 137\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 1736, reward 590.0, memory_length 2000, epsilon 0.20929638796400954, time 727.0, rides 149\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 1737, reward 607.0, memory_length 2000, epsilon 0.20910802121484193, time 734.0, rides 133\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 1738, reward 543.0, memory_length 2000, epsilon 0.20891982399574857, time 728.0, rides 130\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 1739, reward 509.0, memory_length 2000, epsilon 0.20873179615415238, time 735.0, rides 136\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 1740, reward 710.0, memory_length 2000, epsilon 0.20854393753761363, time 729.0, rides 135\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1741, reward 474.0, memory_length 2000, epsilon 0.2083562479938298, time 727.0, rides 134\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 1742, reward 720.0, memory_length 2000, epsilon 0.20816872737063535, time 724.0, rides 131\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 1743, reward 472.0, memory_length 2000, epsilon 0.2079813755160018, time 725.0, rides 147\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 1744, reward 510.0, memory_length 2000, epsilon 0.2077941922780374, time 726.0, rides 129\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 1745, reward 668.0, memory_length 2000, epsilon 0.20760717750498714, time 726.0, rides 129\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 1746, reward 654.0, memory_length 2000, epsilon 0.20742033104523264, time 729.0, rides 141\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 1747, reward 763.0, memory_length 2000, epsilon 0.2072336527472919, time 729.0, rides 139\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 1748, reward 305.0, memory_length 2000, epsilon 0.20704714245981934, time 732.0, rides 144\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 1749, reward 423.0, memory_length 2000, epsilon 0.2068608000316055, time 731.0, rides 127\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 1750, reward 594.0, memory_length 2000, epsilon 0.20667462531157707, time 723.0, rides 145\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 1751, reward 711.0, memory_length 2000, epsilon 0.20648861814879665, time 737.0, rides 134\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 1752, reward 588.0, memory_length 2000, epsilon 0.20630277839246272, time 730.0, rides 139\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 1753, reward 615.0, memory_length 2000, epsilon 0.2061171058919095, time 727.0, rides 149\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 1754, reward 158.0, memory_length 2000, epsilon 0.2059316004966068, time 736.0, rides 130\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 1755, reward 697.0, memory_length 2000, epsilon 0.20574626205615987, time 727.0, rides 137\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 1756, reward 414.0, memory_length 2000, epsilon 0.20556109042030932, time 729.0, rides 124\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 1757, reward 598.0, memory_length 2000, epsilon 0.20537608543893104, time 739.0, rides 132\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 1758, reward 201.0, memory_length 2000, epsilon 0.205191246962036, time 725.0, rides 118\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 1759, reward 555.0, memory_length 2000, epsilon 0.20500657483977017, time 725.0, rides 134\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 1760, reward 200.0, memory_length 2000, epsilon 0.20482206892241436, time 721.0, rides 124\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 1761, reward 577.0, memory_length 2000, epsilon 0.2046377290603842, time 725.0, rides 142\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 1762, reward 891.0, memory_length 2000, epsilon 0.20445355510422983, time 731.0, rides 133\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 1763, reward 416.0, memory_length 2000, epsilon 0.20426954690463603, time 722.0, rides 146\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 1764, reward 541.0, memory_length 2000, epsilon 0.20408570431242185, time 725.0, rides 133\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 1765, reward 393.0, memory_length 2000, epsilon 0.20390202717854067, time 726.0, rides 124\n",
      "Initial State is  [0, 20, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1766, reward 633.0, memory_length 2000, epsilon 0.20371851535407998, time 726.0, rides 125\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 1767, reward 842.0, memory_length 2000, epsilon 0.2035351686902613, time 728.0, rides 131\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 1768, reward 484.0, memory_length 2000, epsilon 0.20335198703844007, time 734.0, rides 129\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 1769, reward 547.0, memory_length 2000, epsilon 0.20316897025010547, time 730.0, rides 127\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 1770, reward 328.0, memory_length 2000, epsilon 0.20298611817688036, time 733.0, rides 139\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 1771, reward 520.0, memory_length 2000, epsilon 0.20280343067052117, time 737.0, rides 138\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 1772, reward 573.0, memory_length 2000, epsilon 0.2026209075829177, time 729.0, rides 137\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 1773, reward 722.0, memory_length 2000, epsilon 0.20243854876609307, time 731.0, rides 135\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 1774, reward 509.0, memory_length 2000, epsilon 0.20225635407220358, time 733.0, rides 128\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 1775, reward 450.0, memory_length 2000, epsilon 0.2020743233535386, time 729.0, rides 131\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 1776, reward 467.0, memory_length 2000, epsilon 0.2018924564625204, time 729.0, rides 129\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 1777, reward 593.0, memory_length 2000, epsilon 0.20171075325170415, time 735.0, rides 141\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 1778, reward 570.0, memory_length 2000, epsilon 0.20152921357377762, time 734.0, rides 149\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 1779, reward 709.0, memory_length 2000, epsilon 0.20134783728156122, time 730.0, rides 139\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 1780, reward 342.0, memory_length 2000, epsilon 0.20116662422800782, time 725.0, rides 135\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 1781, reward 442.0, memory_length 2000, epsilon 0.20098557426620262, time 731.0, rides 129\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 1782, reward 474.0, memory_length 2000, epsilon 0.20080468724936304, time 723.0, rides 134\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 1783, reward 575.0, memory_length 2000, epsilon 0.2006239630308386, time 725.0, rides 146\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 1784, reward 940.0, memory_length 2000, epsilon 0.20044340146411085, time 729.0, rides 153\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 1785, reward 740.0, memory_length 2000, epsilon 0.20026300240279316, time 730.0, rides 129\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 1786, reward 682.0, memory_length 2000, epsilon 0.20008276570063063, time 730.0, rides 133\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 1787, reward 673.0, memory_length 2000, epsilon 0.19990269121150006, time 730.0, rides 137\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 1788, reward 624.0, memory_length 2000, epsilon 0.19972277878940972, time 731.0, rides 125\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1789, reward 437.0, memory_length 2000, epsilon 0.19954302828849926, time 729.0, rides 125\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 1790, reward 830.0, memory_length 2000, epsilon 0.19936343956303962, time 739.0, rides 124\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 1791, reward 712.0, memory_length 2000, epsilon 0.1991840124674329, time 725.0, rides 129\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 1792, reward 515.0, memory_length 2000, epsilon 0.1990047468562122, time 727.0, rides 130\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 1793, reward 564.0, memory_length 2000, epsilon 0.1988256425840416, time 730.0, rides 134\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 1794, reward 594.0, memory_length 2000, epsilon 0.19864669950571595, time 735.0, rides 135\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 1795, reward 943.0, memory_length 2000, epsilon 0.1984679174761608, time 729.0, rides 127\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 1796, reward 737.0, memory_length 2000, epsilon 0.19828929635043224, time 726.0, rides 125\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 1797, reward 378.0, memory_length 2000, epsilon 0.19811083598371684, time 724.0, rides 132\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 1798, reward 669.0, memory_length 2000, epsilon 0.1979325362313315, time 729.0, rides 136\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 1799, reward 469.0, memory_length 2000, epsilon 0.1977543969487233, time 728.0, rides 128\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 1800, reward 477.0, memory_length 2000, epsilon 0.19757641799146944, time 727.0, rides 155\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 1801, reward 850.0, memory_length 2000, epsilon 0.1973985992152771, time 731.0, rides 141\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 1802, reward 584.0, memory_length 2000, epsilon 0.19722094047598335, time 729.0, rides 138\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 1803, reward 580.0, memory_length 2000, epsilon 0.19704344162955495, time 724.0, rides 150\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 1804, reward 461.0, memory_length 2000, epsilon 0.19686610253208836, time 737.0, rides 132\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 1805, reward 366.0, memory_length 2000, epsilon 0.19668892303980948, time 724.0, rides 137\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 1806, reward 670.0, memory_length 2000, epsilon 0.19651190300907365, time 732.0, rides 148\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 1807, reward 549.0, memory_length 2000, epsilon 0.19633504229636548, time 726.0, rides 132\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 1808, reward 637.0, memory_length 2000, epsilon 0.19615834075829874, time 733.0, rides 127\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 1809, reward 608.0, memory_length 2000, epsilon 0.19598179825161627, time 726.0, rides 130\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 1810, reward 848.0, memory_length 2000, epsilon 0.19580541463318982, time 722.0, rides 142\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 1811, reward 534.0, memory_length 2000, epsilon 0.19562918976001994, time 730.0, rides 135\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 1812, reward 660.0, memory_length 2000, epsilon 0.19545312348923594, time 723.0, rides 132\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1813, reward 706.0, memory_length 2000, epsilon 0.1952772156780956, time 727.0, rides 128\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 1814, reward 614.0, memory_length 2000, epsilon 0.19510146618398533, time 729.0, rides 130\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 1815, reward 803.0, memory_length 2000, epsilon 0.19492587486441973, time 726.0, rides 141\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 1816, reward 630.0, memory_length 2000, epsilon 0.19475044157704174, time 721.0, rides 135\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 1817, reward 637.0, memory_length 2000, epsilon 0.1945751661796224, time 725.0, rides 132\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1818, reward 546.0, memory_length 2000, epsilon 0.19440004853006074, time 720.0, rides 135\n",
      "Initial State is  [1, 1, 2]\n",
      "episode 1819, reward 776.0, memory_length 2000, epsilon 0.19422508848638367, time 732.0, rides 130\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 1820, reward 831.0, memory_length 2000, epsilon 0.19405028590674592, time 723.0, rides 132\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 1821, reward 580.0, memory_length 2000, epsilon 0.19387564064942983, time 729.0, rides 143\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 1822, reward 559.0, memory_length 2000, epsilon 0.19370115257284534, time 726.0, rides 140\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 1823, reward 672.0, memory_length 2000, epsilon 0.19352682153552977, time 730.0, rides 128\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 1824, reward 512.0, memory_length 2000, epsilon 0.1933526473961478, time 736.0, rides 127\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 1825, reward 636.0, memory_length 2000, epsilon 0.19317863001349125, time 734.0, rides 134\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 1826, reward 633.0, memory_length 2000, epsilon 0.1930047692464791, time 729.0, rides 126\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 1827, reward 509.0, memory_length 2000, epsilon 0.19283106495415728, time 730.0, rides 137\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 1828, reward 446.0, memory_length 2000, epsilon 0.19265751699569852, time 728.0, rides 150\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 1829, reward 437.0, memory_length 2000, epsilon 0.1924841252304024, time 726.0, rides 143\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1830, reward 773.0, memory_length 2000, epsilon 0.19231088951769504, time 727.0, rides 135\n",
      "Initial State is  [4, 4, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1831, reward 527.0, memory_length 2000, epsilon 0.1921378097171291, time 728.0, rides 138\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 1832, reward 748.0, memory_length 2000, epsilon 0.19196488568838369, time 728.0, rides 132\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 1833, reward 351.0, memory_length 2000, epsilon 0.19179211729126414, time 727.0, rides 142\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 1834, reward 620.0, memory_length 2000, epsilon 0.191619504385702, time 733.0, rides 121\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 1835, reward 575.0, memory_length 2000, epsilon 0.19144704683175487, time 731.0, rides 131\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 1836, reward 343.0, memory_length 2000, epsilon 0.1912747444896063, time 730.0, rides 123\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 1837, reward 577.0, memory_length 2000, epsilon 0.19110259721956566, time 730.0, rides 142\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1838, reward 312.0, memory_length 2000, epsilon 0.19093060488206803, time 725.0, rides 123\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 1839, reward 484.0, memory_length 2000, epsilon 0.19075876733767416, time 729.0, rides 129\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 1840, reward 567.0, memory_length 2000, epsilon 0.19058708444707026, time 727.0, rides 139\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 1841, reward 732.0, memory_length 2000, epsilon 0.1904155560710679, time 733.0, rides 142\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 1842, reward 361.0, memory_length 2000, epsilon 0.19024418207060392, time 731.0, rides 140\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 1843, reward 374.0, memory_length 2000, epsilon 0.19007296230674037, time 729.0, rides 142\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 1844, reward 535.0, memory_length 2000, epsilon 0.18990189664066429, time 728.0, rides 133\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 1845, reward 704.0, memory_length 2000, epsilon 0.18973098493368767, time 734.0, rides 129\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 1846, reward 927.0, memory_length 2000, epsilon 0.18956022704724734, time 726.0, rides 135\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 1847, reward 597.0, memory_length 2000, epsilon 0.1893896228429048, time 727.0, rides 128\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 1848, reward 800.0, memory_length 2000, epsilon 0.18921917218234618, time 724.0, rides 121\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 1849, reward 437.0, memory_length 2000, epsilon 0.18904887492738207, time 730.0, rides 131\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 1850, reward 906.0, memory_length 2000, epsilon 0.18887873093994742, time 734.0, rides 140\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 1851, reward 963.0, memory_length 2000, epsilon 0.18870874008210148, time 732.0, rides 134\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 1852, reward 754.0, memory_length 2000, epsilon 0.1885389022160276, time 732.0, rides 126\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 1853, reward 666.0, memory_length 2000, epsilon 0.18836921720403316, time 727.0, rides 147\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 1854, reward 474.0, memory_length 2000, epsilon 0.18819968490854952, time 725.0, rides 115\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 1855, reward 685.0, memory_length 2000, epsilon 0.18803030519213182, time 742.0, rides 136\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 1856, reward 618.0, memory_length 2000, epsilon 0.1878610779174589, time 723.0, rides 144\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 1857, reward 766.0, memory_length 2000, epsilon 0.18769200294733318, time 726.0, rides 143\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 1858, reward 449.0, memory_length 2000, epsilon 0.18752308014468058, time 729.0, rides 148\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 1859, reward 590.0, memory_length 2000, epsilon 0.18735430937255038, time 732.0, rides 134\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 1860, reward 900.0, memory_length 2000, epsilon 0.18718569049411507, time 724.0, rides 134\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 1861, reward 431.0, memory_length 2000, epsilon 0.18701722337267038, time 726.0, rides 128\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 1862, reward 674.0, memory_length 2000, epsilon 0.18684890787163497, time 728.0, rides 130\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 1863, reward 515.0, memory_length 2000, epsilon 0.1866807438545505, time 727.0, rides 132\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 1864, reward 519.0, memory_length 2000, epsilon 0.1865127311850814, time 726.0, rides 137\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 1865, reward 691.0, memory_length 2000, epsilon 0.18634486972701483, time 725.0, rides 126\n",
      "Initial State is  [2, 22, 6]\n",
      "episode 1866, reward 524.0, memory_length 2000, epsilon 0.18617715934426052, time 722.0, rides 122\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 1867, reward 483.0, memory_length 2000, epsilon 0.18600959990085067, time 732.0, rides 127\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 1868, reward 545.0, memory_length 2000, epsilon 0.1858421912609399, time 725.0, rides 131\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 1869, reward 938.0, memory_length 2000, epsilon 0.18567493328880505, time 729.0, rides 134\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 1870, reward 760.0, memory_length 2000, epsilon 0.18550782584884512, time 724.0, rides 133\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 1871, reward 415.0, memory_length 2000, epsilon 0.18534086880558115, time 730.0, rides 129\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 1872, reward 574.0, memory_length 2000, epsilon 0.18517406202365613, time 729.0, rides 135\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 1873, reward 698.0, memory_length 2000, epsilon 0.18500740536783483, time 731.0, rides 135\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 1874, reward 487.0, memory_length 2000, epsilon 0.18484089870300377, time 740.0, rides 134\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1875, reward 702.0, memory_length 2000, epsilon 0.18467454189417107, time 732.0, rides 129\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 1876, reward 421.0, memory_length 2000, epsilon 0.1845083348064663, time 723.0, rides 135\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 1877, reward 531.0, memory_length 2000, epsilon 0.1843422773051405, time 732.0, rides 149\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 1878, reward 450.0, memory_length 2000, epsilon 0.18417636925556585, time 728.0, rides 143\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 1879, reward 563.0, memory_length 2000, epsilon 0.18401061052323583, time 729.0, rides 122\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1880, reward 658.0, memory_length 2000, epsilon 0.1838450009737649, time 725.0, rides 129\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 1881, reward 609.0, memory_length 2000, epsilon 0.1836795404728885, time 727.0, rides 120\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 1882, reward 501.0, memory_length 2000, epsilon 0.1835142288864629, time 734.0, rides 135\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 1883, reward 477.0, memory_length 2000, epsilon 0.18334906608046508, time 729.0, rides 133\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 1884, reward 470.0, memory_length 2000, epsilon 0.18318405192099266, time 736.0, rides 140\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 1885, reward 615.0, memory_length 2000, epsilon 0.18301918627426375, time 736.0, rides 138\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 1886, reward 471.0, memory_length 2000, epsilon 0.18285446900661692, time 730.0, rides 125\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 1887, reward 729.0, memory_length 2000, epsilon 0.18268989998451096, time 726.0, rides 143\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 1888, reward 774.0, memory_length 2000, epsilon 0.1825254790745249, time 731.0, rides 135\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 1889, reward 525.0, memory_length 2000, epsilon 0.18236120614335782, time 726.0, rides 133\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 1890, reward 606.0, memory_length 2000, epsilon 0.1821970810578288, time 728.0, rides 146\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 1891, reward 915.0, memory_length 2000, epsilon 0.18203310368487677, time 729.0, rides 139\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 1892, reward 536.0, memory_length 2000, epsilon 0.18186927389156038, time 729.0, rides 125\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 1893, reward 518.0, memory_length 2000, epsilon 0.18170559154505797, time 731.0, rides 138\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 1894, reward 1072.0, memory_length 2000, epsilon 0.18154205651266742, time 725.0, rides 138\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 1895, reward 423.0, memory_length 2000, epsilon 0.18137866866180602, time 730.0, rides 135\n",
      "Initial State is  [2, 10, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1896, reward 789.0, memory_length 2000, epsilon 0.18121542786001038, time 728.0, rides 126\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 1897, reward 496.0, memory_length 2000, epsilon 0.18105233397493636, time 720.0, rides 133\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 1898, reward 456.0, memory_length 2000, epsilon 0.18088938687435893, time 728.0, rides 132\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 1899, reward 588.0, memory_length 2000, epsilon 0.180726586426172, time 725.0, rides 130\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 1900, reward 696.0, memory_length 2000, epsilon 0.18056393249838845, time 720.0, rides 117\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 1901, reward 731.0, memory_length 2000, epsilon 0.1804014249591399, time 730.0, rides 132\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 1902, reward 937.0, memory_length 2000, epsilon 0.1802390636766767, time 733.0, rides 131\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 1903, reward 696.0, memory_length 2000, epsilon 0.18007684851936767, time 729.0, rides 143\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 1904, reward 522.0, memory_length 2000, epsilon 0.17991477935570024, time 729.0, rides 129\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 1905, reward 419.0, memory_length 2000, epsilon 0.17975285605428012, time 725.0, rides 130\n",
      "Initial State is  [3, 9, 6]\n",
      "episode 1906, reward 539.0, memory_length 2000, epsilon 0.17959107848383127, time 732.0, rides 146\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 1907, reward 496.0, memory_length 2000, epsilon 0.1794294465131958, time 721.0, rides 132\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 1908, reward 580.0, memory_length 2000, epsilon 0.17926796001133394, time 727.0, rides 134\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 1909, reward 433.0, memory_length 2000, epsilon 0.17910661884732373, time 738.0, rides 133\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 1910, reward 541.0, memory_length 2000, epsilon 0.17894542289036114, time 729.0, rides 124\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 1911, reward 759.0, memory_length 2000, epsilon 0.1787843720097598, time 723.0, rides 137\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 1912, reward 391.0, memory_length 2000, epsilon 0.17862346607495103, time 728.0, rides 134\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 1913, reward 679.0, memory_length 2000, epsilon 0.17846270495548358, time 733.0, rides 129\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 1914, reward 927.0, memory_length 2000, epsilon 0.17830208852102364, time 732.0, rides 126\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 1915, reward 573.0, memory_length 2000, epsilon 0.17814161664135472, time 732.0, rides 135\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 1916, reward 584.0, memory_length 2000, epsilon 0.1779812891863775, time 731.0, rides 132\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 1917, reward 932.0, memory_length 2000, epsilon 0.17782110602610976, time 727.0, rides 138\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 1918, reward 605.0, memory_length 2000, epsilon 0.17766106703068626, time 727.0, rides 133\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 1919, reward 765.0, memory_length 2000, epsilon 0.17750117207035865, time 726.0, rides 133\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 1920, reward 203.0, memory_length 2000, epsilon 0.17734142101549533, time 724.0, rides 134\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 1921, reward 310.0, memory_length 2000, epsilon 0.17718181373658137, time 723.0, rides 130\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 1922, reward 510.0, memory_length 2000, epsilon 0.17702235010421843, time 731.0, rides 141\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 1923, reward 673.0, memory_length 2000, epsilon 0.17686302998912465, time 727.0, rides 139\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 1924, reward 490.0, memory_length 2000, epsilon 0.17670385326213445, time 725.0, rides 130\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 1925, reward 520.0, memory_length 2000, epsilon 0.17654481979419853, time 726.0, rides 128\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 1926, reward 679.0, memory_length 2000, epsilon 0.17638592945638376, time 730.0, rides 129\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1927, reward 848.0, memory_length 2000, epsilon 0.176227182119873, time 726.0, rides 136\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 1928, reward 728.0, memory_length 2000, epsilon 0.17606857765596512, time 727.0, rides 135\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 1929, reward 510.0, memory_length 2000, epsilon 0.17591011593607475, time 728.0, rides 126\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 1930, reward 645.0, memory_length 2000, epsilon 0.17575179683173228, time 731.0, rides 137\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1931, reward 769.0, memory_length 2000, epsilon 0.17559362021458372, time 732.0, rides 144\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 1932, reward 500.0, memory_length 2000, epsilon 0.17543558595639058, time 724.0, rides 128\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 1933, reward 435.0, memory_length 2000, epsilon 0.17527769392902984, time 727.0, rides 144\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 1934, reward 560.0, memory_length 2000, epsilon 0.1751199440044937, time 727.0, rides 149\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 1935, reward 614.0, memory_length 2000, epsilon 0.17496233605488967, time 724.0, rides 140\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 1936, reward 852.0, memory_length 2000, epsilon 0.17480486995244027, time 731.0, rides 138\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 1937, reward 495.0, memory_length 2000, epsilon 0.17464754556948306, time 731.0, rides 147\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 1938, reward 477.0, memory_length 2000, epsilon 0.17449036277847052, time 722.0, rides 135\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 1939, reward 323.0, memory_length 2000, epsilon 0.1743333214519699, time 735.0, rides 118\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 1940, reward 838.0, memory_length 2000, epsilon 0.17417642146266313, time 725.0, rides 121\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 1941, reward 491.0, memory_length 2000, epsilon 0.17401966268334673, time 727.0, rides 129\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 1942, reward 681.0, memory_length 2000, epsilon 0.17386304498693173, time 737.0, rides 121\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 1943, reward 770.0, memory_length 2000, epsilon 0.17370656824644348, time 729.0, rides 142\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 1944, reward 685.0, memory_length 2000, epsilon 0.17355023233502168, time 732.0, rides 142\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1945, reward 600.0, memory_length 2000, epsilon 0.17339403712592016, time 720.0, rides 136\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 1946, reward 560.0, memory_length 2000, epsilon 0.17323798249250683, time 735.0, rides 143\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 1947, reward 204.0, memory_length 2000, epsilon 0.17308206830826356, time 724.0, rides 137\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 1948, reward 759.0, memory_length 2000, epsilon 0.17292629444678612, time 732.0, rides 134\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 1949, reward 846.0, memory_length 2000, epsilon 0.17277066078178402, time 724.0, rides 138\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 1950, reward 514.0, memory_length 2000, epsilon 0.17261516718708042, time 728.0, rides 140\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 1951, reward 843.0, memory_length 2000, epsilon 0.17245981353661205, time 721.0, rides 135\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 1952, reward 871.0, memory_length 2000, epsilon 0.1723045997044291, time 729.0, rides 116\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 1953, reward 397.0, memory_length 2000, epsilon 0.17214952556469512, time 724.0, rides 121\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 1954, reward 606.0, memory_length 2000, epsilon 0.1719945909916869, time 729.0, rides 136\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 1955, reward 522.0, memory_length 2000, epsilon 0.1718397958597944, time 727.0, rides 124\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 1956, reward 344.0, memory_length 2000, epsilon 0.17168514004352056, time 728.0, rides 115\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 1957, reward 940.0, memory_length 2000, epsilon 0.1715306234174814, time 727.0, rides 140\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 1958, reward 365.0, memory_length 2000, epsilon 0.17137624585640568, time 734.0, rides 143\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 1959, reward 336.0, memory_length 2000, epsilon 0.17122200723513492, time 732.0, rides 139\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 1960, reward 748.0, memory_length 2000, epsilon 0.1710679074286233, time 733.0, rides 131\n",
      "Initial State is  [2, 8, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1961, reward 644.0, memory_length 2000, epsilon 0.17091394631193754, time 730.0, rides 136\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 1962, reward 782.0, memory_length 2000, epsilon 0.1707601237602568, time 731.0, rides 128\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 1963, reward 540.0, memory_length 2000, epsilon 0.17060643964887257, time 729.0, rides 145\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 1964, reward 688.0, memory_length 2000, epsilon 0.17045289385318857, time 734.0, rides 143\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 1965, reward 557.0, memory_length 2000, epsilon 0.1702994862487207, time 726.0, rides 124\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 1966, reward 738.0, memory_length 2000, epsilon 0.17014621671109686, time 730.0, rides 131\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 1967, reward 501.0, memory_length 2000, epsilon 0.16999308511605687, time 723.0, rides 144\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 1968, reward 798.0, memory_length 2000, epsilon 0.1698400913394524, time 729.0, rides 133\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 1969, reward 863.0, memory_length 2000, epsilon 0.1696872352572469, time 729.0, rides 131\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 1970, reward 1015.0, memory_length 2000, epsilon 0.1695345167455154, time 729.0, rides 134\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 1971, reward 723.0, memory_length 2000, epsilon 0.16938193568044443, time 731.0, rides 126\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 1972, reward 471.0, memory_length 2000, epsilon 0.16922949193833203, time 724.0, rides 139\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 1973, reward 531.0, memory_length 2000, epsilon 0.16907718539558753, time 724.0, rides 134\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 1974, reward 844.0, memory_length 2000, epsilon 0.1689250159287315, time 730.0, rides 126\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 1975, reward 330.0, memory_length 2000, epsilon 0.16877298341439564, time 729.0, rides 123\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 1976, reward 688.0, memory_length 2000, epsilon 0.1686210877293227, time 724.0, rides 141\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 1977, reward 412.0, memory_length 2000, epsilon 0.1684693287503663, time 729.0, rides 125\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 1978, reward 598.0, memory_length 2000, epsilon 0.16831770635449098, time 724.0, rides 126\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 1979, reward 771.0, memory_length 2000, epsilon 0.16816622041877194, time 726.0, rides 148\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 1980, reward 693.0, memory_length 2000, epsilon 0.16801487082039504, time 730.0, rides 137\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 1981, reward 488.0, memory_length 2000, epsilon 0.16786365743665668, time 726.0, rides 126\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 1982, reward 514.0, memory_length 2000, epsilon 0.16771258014496368, time 726.0, rides 134\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 1983, reward 722.0, memory_length 2000, epsilon 0.1675616388228332, time 731.0, rides 137\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 1984, reward 616.0, memory_length 2000, epsilon 0.16741083334789264, time 721.0, rides 133\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 1985, reward 817.0, memory_length 2000, epsilon 0.16726016359787954, time 732.0, rides 128\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 1986, reward 782.0, memory_length 2000, epsilon 0.16710962945064145, time 734.0, rides 130\n",
      "Initial State is  [2, 19, 4]\n",
      "episode 1987, reward 787.0, memory_length 2000, epsilon 0.16695923078413588, time 731.0, rides 135\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 1988, reward 458.0, memory_length 2000, epsilon 0.16680896747643015, time 726.0, rides 129\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 1989, reward 826.0, memory_length 2000, epsilon 0.16665883940570136, time 720.0, rides 126\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 1990, reward 524.0, memory_length 2000, epsilon 0.16650884645023623, time 724.0, rides 126\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 1991, reward 512.0, memory_length 2000, epsilon 0.166358988488431, time 727.0, rides 152\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 1992, reward 707.0, memory_length 2000, epsilon 0.1662092653987914, time 730.0, rides 123\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 1993, reward 604.0, memory_length 2000, epsilon 0.1660596770599325, time 728.0, rides 141\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 1994, reward 749.0, memory_length 2000, epsilon 0.16591022335057856, time 738.0, rides 137\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 1995, reward 379.0, memory_length 2000, epsilon 0.16576090414956304, time 740.0, rides 137\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 1996, reward 635.0, memory_length 2000, epsilon 0.16561171933582844, time 732.0, rides 146\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 1997, reward 800.0, memory_length 2000, epsilon 0.1654626687884262, time 733.0, rides 128\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 1998, reward 777.0, memory_length 2000, epsilon 0.1653137523865166, time 734.0, rides 140\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 1999, reward 618.0, memory_length 2000, epsilon 0.16516497000936875, time 722.0, rides 140\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 2000, reward 599.0, memory_length 2000, epsilon 0.1650163215363603, time 731.0, rides 135\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 2001, reward 757.0, memory_length 2000, epsilon 0.16486780684697758, time 725.0, rides 124\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 2002, reward 329.0, memory_length 2000, epsilon 0.1647194258208153, time 735.0, rides 142\n",
      "Initial State is  [4, 9, 3]\n",
      "episode 2003, reward 532.0, memory_length 2000, epsilon 0.16457117833757656, time 730.0, rides 140\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 2004, reward 955.0, memory_length 2000, epsilon 0.16442306427707273, time 736.0, rides 131\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 2005, reward 609.0, memory_length 2000, epsilon 0.16427508351922337, time 726.0, rides 130\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 2006, reward 767.0, memory_length 2000, epsilon 0.16412723594405607, time 725.0, rides 130\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 2007, reward 605.0, memory_length 2000, epsilon 0.1639795214317064, time 726.0, rides 132\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 2008, reward 634.0, memory_length 2000, epsilon 0.16383193986241787, time 721.0, rides 137\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 2009, reward 627.0, memory_length 2000, epsilon 0.16368449111654168, time 727.0, rides 124\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 2010, reward 544.0, memory_length 2000, epsilon 0.1635371750745368, time 728.0, rides 126\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2011, reward 302.0, memory_length 2000, epsilon 0.1633899916169697, time 732.0, rides 135\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 2012, reward 518.0, memory_length 2000, epsilon 0.16324294062451444, time 727.0, rides 133\n",
      "Initial State is  [3, 14, 6]\n",
      "episode 2013, reward 573.0, memory_length 2000, epsilon 0.16309602197795237, time 733.0, rides 131\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 2014, reward 441.0, memory_length 2000, epsilon 0.1629492355581722, time 722.0, rides 126\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 2015, reward 659.0, memory_length 2000, epsilon 0.16280258124616984, time 730.0, rides 112\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 2016, reward 444.0, memory_length 2000, epsilon 0.16265605892304827, time 728.0, rides 143\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 2017, reward 660.0, memory_length 2000, epsilon 0.16250966847001752, time 729.0, rides 124\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2018, reward 394.0, memory_length 2000, epsilon 0.1623634097683945, time 725.0, rides 128\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 2019, reward 473.0, memory_length 2000, epsilon 0.16221728269960295, time 730.0, rides 148\n",
      "Initial State is  [0, 7, 4]\n",
      "episode 2020, reward 641.0, memory_length 2000, epsilon 0.1620712871451733, time 726.0, rides 137\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 2021, reward 590.0, memory_length 2000, epsilon 0.16192542298674265, time 727.0, rides 136\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 2022, reward 779.0, memory_length 2000, epsilon 0.16177969010605459, time 723.0, rides 141\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2023, reward 881.0, memory_length 2000, epsilon 0.16163408838495913, time 730.0, rides 135\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 2024, reward 638.0, memory_length 2000, epsilon 0.16148861770541267, time 725.0, rides 141\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 2025, reward 349.0, memory_length 2000, epsilon 0.1613432779494778, time 723.0, rides 144\n",
      "Initial State is  [1, 6, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2026, reward 736.0, memory_length 2000, epsilon 0.1611980689993233, time 738.0, rides 134\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 2027, reward 786.0, memory_length 2000, epsilon 0.1610529907372239, time 726.0, rides 135\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 2028, reward 486.0, memory_length 2000, epsilon 0.1609080430455604, time 727.0, rides 138\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 2029, reward 746.0, memory_length 2000, epsilon 0.16076322580681937, time 727.0, rides 142\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 2030, reward 613.0, memory_length 2000, epsilon 0.16061853890359323, time 725.0, rides 135\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 2031, reward 412.0, memory_length 2000, epsilon 0.16047398221858, time 724.0, rides 145\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 2032, reward 729.0, memory_length 2000, epsilon 0.1603295556345833, time 721.0, rides 141\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 2033, reward 577.0, memory_length 2000, epsilon 0.16018525903451217, time 726.0, rides 124\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 2034, reward 665.0, memory_length 2000, epsilon 0.1600410923013811, time 739.0, rides 125\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 2035, reward 839.0, memory_length 2000, epsilon 0.15989705531830986, time 723.0, rides 127\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 2036, reward 698.0, memory_length 2000, epsilon 0.15975314796852338, time 721.0, rides 134\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 2037, reward 592.0, memory_length 2000, epsilon 0.1596093701353517, time 725.0, rides 129\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 2038, reward 433.0, memory_length 2000, epsilon 0.1594657217022299, time 731.0, rides 138\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 2039, reward 815.0, memory_length 2000, epsilon 0.15932220255269788, time 721.0, rides 142\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 2040, reward 419.0, memory_length 2000, epsilon 0.15917881257040045, time 723.0, rides 139\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 2041, reward 617.0, memory_length 2000, epsilon 0.15903555163908709, time 727.0, rides 142\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 2042, reward 631.0, memory_length 2000, epsilon 0.15889241964261192, time 738.0, rides 139\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 2043, reward 672.0, memory_length 2000, epsilon 0.15874941646493357, time 729.0, rides 144\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 2044, reward 600.0, memory_length 2000, epsilon 0.15860654199011512, time 726.0, rides 137\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 2045, reward 753.0, memory_length 2000, epsilon 0.158463796102324, time 731.0, rides 140\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2046, reward 494.0, memory_length 2000, epsilon 0.1583211786858319, time 723.0, rides 128\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 2047, reward 624.0, memory_length 2000, epsilon 0.15817868962501463, time 731.0, rides 137\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 2048, reward 538.0, memory_length 2000, epsilon 0.15803632880435212, time 727.0, rides 134\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 2049, reward 630.0, memory_length 2000, epsilon 0.1578940961084282, time 734.0, rides 123\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2050, reward 613.0, memory_length 2000, epsilon 0.1577519914219306, time 728.0, rides 155\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 2051, reward 490.0, memory_length 2000, epsilon 0.15761001462965088, time 726.0, rides 146\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 2052, reward 372.0, memory_length 2000, epsilon 0.1574681656164842, time 729.0, rides 129\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2053, reward 790.0, memory_length 2000, epsilon 0.15732644426742937, time 730.0, rides 137\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 2054, reward 423.0, memory_length 2000, epsilon 0.15718485046758868, time 730.0, rides 133\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 2055, reward 906.0, memory_length 2000, epsilon 0.15704338410216784, time 733.0, rides 148\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 2056, reward 349.0, memory_length 2000, epsilon 0.1569020450564759, time 732.0, rides 131\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 2057, reward 981.0, memory_length 2000, epsilon 0.15676083321592507, time 733.0, rides 138\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 2058, reward 828.0, memory_length 2000, epsilon 0.15661974846603074, time 737.0, rides 134\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 2059, reward 445.0, memory_length 2000, epsilon 0.1564787906924113, time 736.0, rides 125\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 2060, reward 586.0, memory_length 2000, epsilon 0.15633795978078813, time 728.0, rides 130\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 2061, reward 704.0, memory_length 2000, epsilon 0.15619725561698541, time 733.0, rides 141\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 2062, reward 930.0, memory_length 2000, epsilon 0.15605667808693013, time 731.0, rides 133\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 2063, reward 816.0, memory_length 2000, epsilon 0.1559162270766519, time 723.0, rides 144\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 2064, reward 738.0, memory_length 2000, epsilon 0.1557759024722829, time 731.0, rides 134\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 2065, reward 589.0, memory_length 2000, epsilon 0.15563570416005784, time 737.0, rides 130\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 2066, reward 735.0, memory_length 2000, epsilon 0.15549563202631378, time 731.0, rides 130\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 2067, reward 508.0, memory_length 2000, epsilon 0.1553556859574901, time 734.0, rides 145\n",
      "Initial State is  [2, 15, 0]\n",
      "episode 2068, reward 649.0, memory_length 2000, epsilon 0.15521586584012836, time 730.0, rides 146\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 2069, reward 777.0, memory_length 2000, epsilon 0.15507617156087225, time 726.0, rides 125\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 2070, reward 561.0, memory_length 2000, epsilon 0.15493660300646747, time 733.0, rides 130\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 2071, reward 607.0, memory_length 2000, epsilon 0.15479716006376165, time 733.0, rides 126\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 2072, reward 832.0, memory_length 2000, epsilon 0.15465784261970428, time 725.0, rides 125\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 2073, reward 571.0, memory_length 2000, epsilon 0.15451865056134653, time 729.0, rides 129\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 2074, reward 942.0, memory_length 2000, epsilon 0.15437958377584132, time 726.0, rides 143\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 2075, reward 477.0, memory_length 2000, epsilon 0.15424064215044306, time 726.0, rides 133\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 2076, reward 724.0, memory_length 2000, epsilon 0.15410182557250765, time 725.0, rides 135\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 2077, reward 929.0, memory_length 2000, epsilon 0.1539631339294924, time 726.0, rides 129\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 2078, reward 551.0, memory_length 2000, epsilon 0.15382456710895587, time 733.0, rides 146\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 2079, reward 739.0, memory_length 2000, epsilon 0.1536861249985578, time 723.0, rides 132\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 2080, reward 402.0, memory_length 2000, epsilon 0.1535478074860591, time 734.0, rides 142\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2081, reward 666.0, memory_length 2000, epsilon 0.15340961445932164, time 726.0, rides 111\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 2082, reward 572.0, memory_length 2000, epsilon 0.15327154580630825, time 724.0, rides 135\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 2083, reward 462.0, memory_length 2000, epsilon 0.15313360141508256, time 732.0, rides 118\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 2084, reward 740.0, memory_length 2000, epsilon 0.152995781173809, time 732.0, rides 144\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 2085, reward 860.0, memory_length 2000, epsilon 0.15285808497075257, time 727.0, rides 131\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 2086, reward 762.0, memory_length 2000, epsilon 0.1527205126942789, time 725.0, rides 131\n",
      "Initial State is  [0, 1, 6]\n",
      "episode 2087, reward 637.0, memory_length 2000, epsilon 0.15258306423285403, time 729.0, rides 138\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 2088, reward 388.0, memory_length 2000, epsilon 0.15244573947504447, time 730.0, rides 133\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 2089, reward 724.0, memory_length 2000, epsilon 0.15230853830951693, time 727.0, rides 146\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 2090, reward 551.0, memory_length 2000, epsilon 0.15217146062503836, time 731.0, rides 135\n",
      "Initial State is  [3, 11, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2091, reward 595.0, memory_length 2000, epsilon 0.15203450631047583, time 733.0, rides 133\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 2092, reward 524.0, memory_length 2000, epsilon 0.1518976752547964, time 734.0, rides 131\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 2093, reward 783.0, memory_length 2000, epsilon 0.15176096734706707, time 723.0, rides 151\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 2094, reward 831.0, memory_length 2000, epsilon 0.1516243824764547, time 731.0, rides 132\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 2095, reward 515.0, memory_length 2000, epsilon 0.1514879205322259, time 734.0, rides 131\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 2096, reward 502.0, memory_length 2000, epsilon 0.15135158140374688, time 727.0, rides 135\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 2097, reward 518.0, memory_length 2000, epsilon 0.15121536498048352, time 729.0, rides 131\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 2098, reward 669.0, memory_length 2000, epsilon 0.1510792711520011, time 723.0, rides 130\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 2099, reward 726.0, memory_length 2000, epsilon 0.15094329980796428, time 732.0, rides 130\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 2100, reward 504.0, memory_length 2000, epsilon 0.15080745083813712, time 732.0, rides 152\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 2101, reward 837.0, memory_length 2000, epsilon 0.1506717241323828, time 726.0, rides 141\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 2102, reward 645.0, memory_length 2000, epsilon 0.15053611958066365, time 733.0, rides 137\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 2103, reward 668.0, memory_length 2000, epsilon 0.15040063707304105, time 724.0, rides 128\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 2104, reward 822.0, memory_length 2000, epsilon 0.1502652764996753, time 729.0, rides 130\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2105, reward 648.0, memory_length 2000, epsilon 0.1501300377508256, time 729.0, rides 124\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 2106, reward 459.0, memory_length 2000, epsilon 0.14999492071684983, time 730.0, rides 128\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 2107, reward 787.0, memory_length 2000, epsilon 0.14985992528820466, time 728.0, rides 141\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 2108, reward 796.0, memory_length 2000, epsilon 0.14972505135544528, time 730.0, rides 126\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 2109, reward 415.0, memory_length 2000, epsilon 0.1495902988092254, time 728.0, rides 122\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 2110, reward 735.0, memory_length 2000, epsilon 0.1494556675402971, time 729.0, rides 137\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 2111, reward 513.0, memory_length 2000, epsilon 0.14932115743951083, time 730.0, rides 117\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 2112, reward 674.0, memory_length 2000, epsilon 0.14918676839781528, time 722.0, rides 146\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 2113, reward 874.0, memory_length 2000, epsilon 0.14905250030625725, time 730.0, rides 129\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 2114, reward 595.0, memory_length 2000, epsilon 0.1489183530559816, time 730.0, rides 128\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 2115, reward 794.0, memory_length 2000, epsilon 0.14878432653823123, time 737.0, rides 131\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 2116, reward 691.0, memory_length 2000, epsilon 0.14865042064434683, time 731.0, rides 121\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 2117, reward 658.0, memory_length 2000, epsilon 0.14851663526576692, time 731.0, rides 119\n",
      "Initial State is  [3, 3, 6]\n",
      "episode 2118, reward 382.0, memory_length 2000, epsilon 0.14838297029402772, time 729.0, rides 137\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 2119, reward 700.0, memory_length 2000, epsilon 0.1482494256207631, time 735.0, rides 126\n",
      "Initial State is  [1, 15, 4]\n",
      "episode 2120, reward 261.0, memory_length 2000, epsilon 0.1481160011377044, time 730.0, rides 135\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2121, reward 854.0, memory_length 2000, epsilon 0.14798269673668046, time 725.0, rides 132\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 2122, reward 791.0, memory_length 2000, epsilon 0.14784951230961746, time 735.0, rides 137\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2123, reward 549.0, memory_length 2000, epsilon 0.14771644774853881, time 724.0, rides 138\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 2124, reward 623.0, memory_length 2000, epsilon 0.14758350294556513, time 728.0, rides 140\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 2125, reward 263.0, memory_length 2000, epsilon 0.14745067779291413, time 722.0, rides 144\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 2126, reward 767.0, memory_length 2000, epsilon 0.1473179721829005, time 726.0, rides 129\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2127, reward 954.0, memory_length 2000, epsilon 0.14718538600793588, time 728.0, rides 134\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 2128, reward 888.0, memory_length 2000, epsilon 0.14705291916052873, time 722.0, rides 135\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 2129, reward 689.0, memory_length 2000, epsilon 0.14692057153328425, time 731.0, rides 145\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 2130, reward 252.0, memory_length 2000, epsilon 0.14678834301890428, time 732.0, rides 131\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 2131, reward 521.0, memory_length 2000, epsilon 0.14665623351018728, time 724.0, rides 131\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 2132, reward 407.0, memory_length 2000, epsilon 0.1465242429000281, time 724.0, rides 140\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 2133, reward 844.0, memory_length 2000, epsilon 0.1463923710814181, time 738.0, rides 119\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 2134, reward 551.0, memory_length 2000, epsilon 0.1462606179474448, time 734.0, rides 148\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 2135, reward 720.0, memory_length 2000, epsilon 0.1461289833912921, time 722.0, rides 143\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 2136, reward 757.0, memory_length 2000, epsilon 0.14599746730623994, time 727.0, rides 143\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 2137, reward 753.0, memory_length 2000, epsilon 0.14586606958566434, time 729.0, rides 137\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 2138, reward 894.0, memory_length 2000, epsilon 0.14573479012303725, time 729.0, rides 122\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 2139, reward 684.0, memory_length 2000, epsilon 0.1456036288119265, time 723.0, rides 136\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 2140, reward 737.0, memory_length 2000, epsilon 0.14547258554599576, time 725.0, rides 141\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2141, reward 681.0, memory_length 2000, epsilon 0.14534166021900435, time 728.0, rides 142\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2142, reward 622.0, memory_length 2000, epsilon 0.14521085272480724, time 730.0, rides 148\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 2143, reward 723.0, memory_length 2000, epsilon 0.1450801629573549, time 721.0, rides 128\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 2144, reward 497.0, memory_length 2000, epsilon 0.14494959081069328, time 730.0, rides 137\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 2145, reward 473.0, memory_length 2000, epsilon 0.14481913617896366, time 727.0, rides 134\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 2146, reward 434.0, memory_length 2000, epsilon 0.1446887989564026, time 729.0, rides 119\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2147, reward 885.0, memory_length 2000, epsilon 0.14455857903734184, time 731.0, rides 131\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 2148, reward 831.0, memory_length 2000, epsilon 0.14442847631620823, time 731.0, rides 140\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 2149, reward 695.0, memory_length 2000, epsilon 0.14429849068752365, time 729.0, rides 121\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2150, reward 432.0, memory_length 2000, epsilon 0.1441686220459049, time 727.0, rides 123\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 2151, reward 646.0, memory_length 2000, epsilon 0.14403887028606358, time 727.0, rides 132\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 2152, reward 885.0, memory_length 2000, epsilon 0.1439092353028061, time 727.0, rides 129\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 2153, reward 634.0, memory_length 2000, epsilon 0.1437797169910336, time 727.0, rides 117\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 2154, reward 648.0, memory_length 2000, epsilon 0.14365031524574165, time 734.0, rides 121\n",
      "Initial State is  [0, 21, 5]\n",
      "episode 2155, reward 525.0, memory_length 2000, epsilon 0.14352102996202049, time 723.0, rides 123\n",
      "Initial State is  [3, 15, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2156, reward 582.0, memory_length 2000, epsilon 0.14339186103505466, time 729.0, rides 124\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2157, reward 693.0, memory_length 2000, epsilon 0.14326280836012312, time 730.0, rides 120\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 2158, reward 642.0, memory_length 2000, epsilon 0.143133871832599, time 746.0, rides 145\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 2159, reward 631.0, memory_length 2000, epsilon 0.14300505134794966, time 723.0, rides 134\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2160, reward 767.0, memory_length 2000, epsilon 0.1428763468017365, time 726.0, rides 111\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 2161, reward 691.0, memory_length 2000, epsilon 0.14274775808961493, time 733.0, rides 135\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 2162, reward 669.0, memory_length 2000, epsilon 0.14261928510733426, time 722.0, rides 132\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2163, reward 637.0, memory_length 2000, epsilon 0.14249092775073766, time 733.0, rides 129\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 2164, reward 328.0, memory_length 2000, epsilon 0.14236268591576198, time 727.0, rides 114\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 2165, reward 764.0, memory_length 2000, epsilon 0.1422345594984378, time 725.0, rides 137\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 2166, reward 583.0, memory_length 2000, epsilon 0.1421065483948892, time 730.0, rides 155\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 2167, reward 801.0, memory_length 2000, epsilon 0.1419786525013338, time 740.0, rides 128\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 2168, reward 762.0, memory_length 2000, epsilon 0.1418508717140826, time 724.0, rides 133\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2169, reward 593.0, memory_length 2000, epsilon 0.1417232059295399, time 725.0, rides 132\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 2170, reward 854.0, memory_length 2000, epsilon 0.14159565504420332, time 727.0, rides 138\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 2171, reward 598.0, memory_length 2000, epsilon 0.14146821895466355, time 730.0, rides 125\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 2172, reward 796.0, memory_length 2000, epsilon 0.14134089755760434, time 728.0, rides 141\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 2173, reward 789.0, memory_length 2000, epsilon 0.1412136907498025, time 732.0, rides 141\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 2174, reward 661.0, memory_length 2000, epsilon 0.14108659842812768, time 725.0, rides 131\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 2175, reward 566.0, memory_length 2000, epsilon 0.14095962048954236, time 728.0, rides 138\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 2176, reward 602.0, memory_length 2000, epsilon 0.14083275683110177, time 724.0, rides 131\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 2177, reward 844.0, memory_length 2000, epsilon 0.14070600734995378, time 736.0, rides 142\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 2178, reward 779.0, memory_length 2000, epsilon 0.14057937194333883, time 729.0, rides 123\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 2179, reward 698.0, memory_length 2000, epsilon 0.14045285050858983, time 725.0, rides 140\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 2180, reward 507.0, memory_length 2000, epsilon 0.1403264429431321, time 732.0, rides 147\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 2181, reward 907.0, memory_length 2000, epsilon 0.14020014914448328, time 723.0, rides 139\n",
      "Initial State is  [0, 3, 6]\n",
      "episode 2182, reward 794.0, memory_length 2000, epsilon 0.14007396901025324, time 722.0, rides 137\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2183, reward 723.0, memory_length 2000, epsilon 0.139947902438144, time 731.0, rides 129\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 2184, reward 482.0, memory_length 2000, epsilon 0.13982194932594968, time 729.0, rides 132\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2185, reward 660.0, memory_length 2000, epsilon 0.13969610957155632, time 727.0, rides 146\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 2186, reward 486.0, memory_length 2000, epsilon 0.13957038307294192, time 726.0, rides 145\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2187, reward 487.0, memory_length 2000, epsilon 0.13944476972817627, time 725.0, rides 135\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2188, reward 666.0, memory_length 2000, epsilon 0.13931926943542092, time 730.0, rides 144\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2189, reward 588.0, memory_length 2000, epsilon 0.13919388209292904, time 722.0, rides 142\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 2190, reward 747.0, memory_length 2000, epsilon 0.1390686075990454, time 725.0, rides 132\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 2191, reward 653.0, memory_length 2000, epsilon 0.13894344585220628, time 737.0, rides 127\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 2192, reward 935.0, memory_length 2000, epsilon 0.1388183967509393, time 734.0, rides 125\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 2193, reward 544.0, memory_length 2000, epsilon 0.13869346019386344, time 727.0, rides 144\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 2194, reward 494.0, memory_length 2000, epsilon 0.13856863607968897, time 726.0, rides 124\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 2195, reward 704.0, memory_length 2000, epsilon 0.13844392430721725, time 726.0, rides 132\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 2196, reward 492.0, memory_length 2000, epsilon 0.13831932477534076, time 722.0, rides 130\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 2197, reward 492.0, memory_length 2000, epsilon 0.13819483738304295, time 725.0, rides 128\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 2198, reward 691.0, memory_length 2000, epsilon 0.1380704620293982, time 723.0, rides 138\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 2199, reward 901.0, memory_length 2000, epsilon 0.13794619861357174, time 728.0, rides 141\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 2200, reward 511.0, memory_length 2000, epsilon 0.1378220470348195, time 722.0, rides 143\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 2201, reward 392.0, memory_length 2000, epsilon 0.13769800719248818, time 734.0, rides 139\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2202, reward 900.0, memory_length 2000, epsilon 0.13757407898601492, time 732.0, rides 132\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 2203, reward 769.0, memory_length 2000, epsilon 0.13745026231492752, time 720.0, rides 138\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 2204, reward 292.0, memory_length 2000, epsilon 0.13732655707884409, time 736.0, rides 123\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 2205, reward 417.0, memory_length 2000, epsilon 0.13720296317747313, time 734.0, rides 121\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 2206, reward 479.0, memory_length 2000, epsilon 0.13707948051061342, time 726.0, rides 122\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 2207, reward 688.0, memory_length 2000, epsilon 0.13695610897815386, time 730.0, rides 129\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 2208, reward 909.0, memory_length 2000, epsilon 0.13683284848007352, time 731.0, rides 128\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 2209, reward 415.0, memory_length 2000, epsilon 0.13670969891644144, time 731.0, rides 131\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 2210, reward 441.0, memory_length 2000, epsilon 0.13658666018741664, time 727.0, rides 137\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 2211, reward 515.0, memory_length 2000, epsilon 0.13646373219324795, time 731.0, rides 152\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 2212, reward 435.0, memory_length 2000, epsilon 0.13634091483427402, time 732.0, rides 127\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 2213, reward 911.0, memory_length 2000, epsilon 0.13621820801092316, time 736.0, rides 133\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 2214, reward 553.0, memory_length 2000, epsilon 0.13609561162371334, time 727.0, rides 121\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 2215, reward 425.0, memory_length 2000, epsilon 0.135973125573252, time 726.0, rides 130\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 2216, reward 878.0, memory_length 2000, epsilon 0.13585074976023606, time 735.0, rides 144\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 2217, reward 753.0, memory_length 2000, epsilon 0.13572848408545185, time 733.0, rides 128\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 2218, reward 1114.0, memory_length 2000, epsilon 0.13560632844977494, time 728.0, rides 135\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 2219, reward 610.0, memory_length 2000, epsilon 0.13548428275417013, time 727.0, rides 151\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 2220, reward 419.0, memory_length 2000, epsilon 0.13536234689969137, time 728.0, rides 138\n",
      "Initial State is  [0, 22, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2221, reward 576.0, memory_length 2000, epsilon 0.13524052078748164, time 726.0, rides 132\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 2222, reward 951.0, memory_length 2000, epsilon 0.1351188043187729, time 728.0, rides 149\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 2223, reward 837.0, memory_length 2000, epsilon 0.134997197394886, time 733.0, rides 141\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 2224, reward 496.0, memory_length 2000, epsilon 0.1348756999172306, time 736.0, rides 131\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2225, reward 502.0, memory_length 2000, epsilon 0.1347543117873051, time 730.0, rides 138\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 2226, reward 795.0, memory_length 2000, epsilon 0.1346330329066965, time 727.0, rides 138\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 2227, reward 616.0, memory_length 2000, epsilon 0.13451186317708047, time 722.0, rides 136\n",
      "Initial State is  [0, 15, 2]\n",
      "episode 2228, reward 192.0, memory_length 2000, epsilon 0.1343908025002211, time 728.0, rides 141\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 2229, reward 851.0, memory_length 2000, epsilon 0.13426985077797088, time 733.0, rides 130\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 2230, reward 545.0, memory_length 2000, epsilon 0.1341490079122707, time 725.0, rides 141\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 2231, reward 485.0, memory_length 2000, epsilon 0.13402827380514964, time 728.0, rides 143\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 2232, reward 613.0, memory_length 2000, epsilon 0.133907648358725, time 723.0, rides 135\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 2233, reward 670.0, memory_length 2000, epsilon 0.13378713147520216, time 726.0, rides 127\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 2234, reward 810.0, memory_length 2000, epsilon 0.13366672305687446, time 733.0, rides 136\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 2235, reward 710.0, memory_length 2000, epsilon 0.13354642300612327, time 735.0, rides 134\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2236, reward 513.0, memory_length 2000, epsilon 0.13342623122541775, time 725.0, rides 134\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 2237, reward 685.0, memory_length 2000, epsilon 0.13330614761731488, time 734.0, rides 129\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 2238, reward 842.0, memory_length 2000, epsilon 0.1331861720844593, time 735.0, rides 123\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 2239, reward 401.0, memory_length 2000, epsilon 0.1330663045295833, time 721.0, rides 134\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 2240, reward 534.0, memory_length 2000, epsilon 0.13294654485550667, time 724.0, rides 134\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 2241, reward 598.0, memory_length 2000, epsilon 0.1328268929651367, time 722.0, rides 134\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 2242, reward 870.0, memory_length 2000, epsilon 0.13270734876146806, time 730.0, rides 139\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 2243, reward 460.0, memory_length 2000, epsilon 0.13258791214758273, time 736.0, rides 141\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 2244, reward 630.0, memory_length 2000, epsilon 0.1324685830266499, time 726.0, rides 124\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2245, reward 486.0, memory_length 2000, epsilon 0.13234936130192593, time 736.0, rides 136\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 2246, reward 826.0, memory_length 2000, epsilon 0.1322302468767542, time 734.0, rides 131\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 2247, reward 516.0, memory_length 2000, epsilon 0.13211123965456512, time 727.0, rides 131\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 2248, reward 592.0, memory_length 2000, epsilon 0.131992339538876, time 730.0, rides 135\n",
      "Initial State is  [2, 21, 2]\n",
      "episode 2249, reward 546.0, memory_length 2000, epsilon 0.13187354643329102, time 731.0, rides 151\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 2250, reward 433.0, memory_length 2000, epsilon 0.13175486024150107, time 737.0, rides 142\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 2251, reward 621.0, memory_length 2000, epsilon 0.1316362808672837, time 730.0, rides 167\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 2252, reward 851.0, memory_length 2000, epsilon 0.13151780821450315, time 724.0, rides 134\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 2253, reward 330.0, memory_length 2000, epsilon 0.1313994421871101, time 727.0, rides 147\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 2254, reward 887.0, memory_length 2000, epsilon 0.1312811826891417, time 726.0, rides 132\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 2255, reward 631.0, memory_length 2000, epsilon 0.13116302962472148, time 724.0, rides 127\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 2256, reward 482.0, memory_length 2000, epsilon 0.13104498289805924, time 723.0, rides 129\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2257, reward 599.0, memory_length 2000, epsilon 0.130927042413451, time 724.0, rides 139\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 2258, reward 366.0, memory_length 2000, epsilon 0.13080920807527888, time 728.0, rides 142\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 2259, reward 392.0, memory_length 2000, epsilon 0.13069147978801113, time 727.0, rides 131\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 2260, reward 624.0, memory_length 2000, epsilon 0.13057385745620192, time 725.0, rides 137\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 2261, reward 662.0, memory_length 2000, epsilon 0.13045634098449135, time 735.0, rides 129\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 2262, reward 761.0, memory_length 2000, epsilon 0.13033893027760532, time 729.0, rides 127\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 2263, reward 623.0, memory_length 2000, epsilon 0.13022162524035547, time 725.0, rides 132\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 2264, reward 735.0, memory_length 2000, epsilon 0.13010442577763914, time 731.0, rides 137\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2265, reward 943.0, memory_length 2000, epsilon 0.12998733179443928, time 723.0, rides 134\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 2266, reward 762.0, memory_length 2000, epsilon 0.12987034319582427, time 725.0, rides 133\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2267, reward 555.0, memory_length 2000, epsilon 0.12975345988694803, time 726.0, rides 142\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 2268, reward 487.0, memory_length 2000, epsilon 0.12963668177304977, time 724.0, rides 121\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 2269, reward 286.0, memory_length 2000, epsilon 0.12952000875945402, time 726.0, rides 138\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 2270, reward 552.0, memory_length 2000, epsilon 0.1294034407515705, time 730.0, rides 126\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 2271, reward 508.0, memory_length 2000, epsilon 0.12928697765489408, time 734.0, rides 132\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 2272, reward 789.0, memory_length 2000, epsilon 0.12917061937500468, time 734.0, rides 132\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 2273, reward 805.0, memory_length 2000, epsilon 0.12905436581756718, time 730.0, rides 128\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 2274, reward 390.0, memory_length 2000, epsilon 0.12893821688833138, time 735.0, rides 139\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 2275, reward 434.0, memory_length 2000, epsilon 0.12882217249313188, time 735.0, rides 124\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 2276, reward 542.0, memory_length 2000, epsilon 0.12870623253788807, time 730.0, rides 147\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 2277, reward 524.0, memory_length 2000, epsilon 0.12859039692860397, time 727.0, rides 132\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 2278, reward 489.0, memory_length 2000, epsilon 0.12847466557136822, time 726.0, rides 146\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 2279, reward 638.0, memory_length 2000, epsilon 0.128359038372354, time 737.0, rides 128\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 2280, reward 972.0, memory_length 2000, epsilon 0.12824351523781888, time 728.0, rides 132\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 2281, reward 619.0, memory_length 2000, epsilon 0.12812809607410483, time 731.0, rides 120\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 2282, reward 313.0, memory_length 2000, epsilon 0.12801278078763814, time 734.0, rides 136\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2283, reward 756.0, memory_length 2000, epsilon 0.12789756928492926, time 729.0, rides 125\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2284, reward 972.0, memory_length 2000, epsilon 0.12778246147257283, time 729.0, rides 136\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 2285, reward 984.0, memory_length 2000, epsilon 0.1276674572572475, time 724.0, rides 121\n",
      "Initial State is  [3, 3, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2286, reward 650.0, memory_length 2000, epsilon 0.12755255654571598, time 733.0, rides 124\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 2287, reward 376.0, memory_length 2000, epsilon 0.12743775924482484, time 724.0, rides 127\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 2288, reward 657.0, memory_length 2000, epsilon 0.1273230652615045, time 730.0, rides 137\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 2289, reward 1021.0, memory_length 2000, epsilon 0.12720847450276915, time 733.0, rides 134\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 2290, reward 695.0, memory_length 2000, epsilon 0.12709398687571666, time 726.0, rides 139\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 2291, reward 795.0, memory_length 2000, epsilon 0.1269796022875285, time 733.0, rides 128\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 2292, reward 516.0, memory_length 2000, epsilon 0.1268653206454697, time 727.0, rides 131\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 2293, reward 825.0, memory_length 2000, epsilon 0.1267511418568888, time 732.0, rides 134\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 2294, reward 677.0, memory_length 2000, epsilon 0.1266370658292176, time 731.0, rides 145\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 2295, reward 992.0, memory_length 2000, epsilon 0.12652309246997132, time 727.0, rides 141\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 2296, reward 850.0, memory_length 2000, epsilon 0.12640922168674834, time 724.0, rides 131\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 2297, reward 486.0, memory_length 2000, epsilon 0.12629545338723028, time 739.0, rides 140\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 2298, reward 835.0, memory_length 2000, epsilon 0.12618178747918177, time 725.0, rides 139\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 2299, reward 941.0, memory_length 2000, epsilon 0.1260682238704505, time 729.0, rides 142\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 2300, reward 803.0, memory_length 2000, epsilon 0.12595476246896709, time 738.0, rides 133\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 2301, reward 392.0, memory_length 2000, epsilon 0.125841403182745, time 728.0, rides 147\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 2302, reward 546.0, memory_length 2000, epsilon 0.12572814591988052, time 730.0, rides 140\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 2303, reward 572.0, memory_length 2000, epsilon 0.12561499058855263, time 721.0, rides 131\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 2304, reward 601.0, memory_length 2000, epsilon 0.12550193709702293, time 730.0, rides 139\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 2305, reward 685.0, memory_length 2000, epsilon 0.1253889853536356, time 720.0, rides 137\n",
      "Initial State is  [4, 6, 1]\n",
      "episode 2306, reward 534.0, memory_length 2000, epsilon 0.12527613526681733, time 729.0, rides 136\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 2307, reward 837.0, memory_length 2000, epsilon 0.12516338674507718, time 727.0, rides 124\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 2308, reward 904.0, memory_length 2000, epsilon 0.1250507396970066, time 726.0, rides 135\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 2309, reward 624.0, memory_length 2000, epsilon 0.1249381940312793, time 729.0, rides 143\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 2310, reward 529.0, memory_length 2000, epsilon 0.12482574965665115, time 727.0, rides 143\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 2311, reward 521.0, memory_length 2000, epsilon 0.12471340648196017, time 728.0, rides 138\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 2312, reward 763.0, memory_length 2000, epsilon 0.1246011644161264, time 736.0, rides 143\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 2313, reward 708.0, memory_length 2000, epsilon 0.12448902336815189, time 727.0, rides 127\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 2314, reward 669.0, memory_length 2000, epsilon 0.12437698324712056, time 732.0, rides 148\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 2315, reward 742.0, memory_length 2000, epsilon 0.12426504396219815, time 723.0, rides 135\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 2316, reward 691.0, memory_length 2000, epsilon 0.12415320542263217, time 723.0, rides 133\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 2317, reward 679.0, memory_length 2000, epsilon 0.1240414675377518, time 728.0, rides 146\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 2318, reward 640.0, memory_length 2000, epsilon 0.12392983021696782, time 726.0, rides 140\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 2319, reward 494.0, memory_length 2000, epsilon 0.12381829336977256, time 728.0, rides 136\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 2320, reward 835.0, memory_length 2000, epsilon 0.12370685690573976, time 724.0, rides 127\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 2321, reward 784.0, memory_length 2000, epsilon 0.1235955207345246, time 725.0, rides 127\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 2322, reward 877.0, memory_length 2000, epsilon 0.12348428476586353, time 728.0, rides 134\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 2323, reward 484.0, memory_length 2000, epsilon 0.12337314890957425, time 727.0, rides 131\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 2324, reward 693.0, memory_length 2000, epsilon 0.12326211307555564, time 734.0, rides 132\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 2325, reward 541.0, memory_length 2000, epsilon 0.12315117717378764, time 724.0, rides 122\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 2326, reward 537.0, memory_length 2000, epsilon 0.12304034111433122, time 726.0, rides 120\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 2327, reward 458.0, memory_length 2000, epsilon 0.12292960480732833, time 726.0, rides 137\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 2328, reward 787.0, memory_length 2000, epsilon 0.12281896816300172, time 734.0, rides 139\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2329, reward 823.0, memory_length 2000, epsilon 0.12270843109165502, time 726.0, rides 123\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 2330, reward 615.0, memory_length 2000, epsilon 0.12259799350367254, time 728.0, rides 140\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 2331, reward 443.0, memory_length 2000, epsilon 0.12248765530951923, time 736.0, rides 128\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 2332, reward 575.0, memory_length 2000, epsilon 0.12237741641974066, time 722.0, rides 132\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 2333, reward 800.0, memory_length 2000, epsilon 0.12226727674496289, time 731.0, rides 129\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 2334, reward 670.0, memory_length 2000, epsilon 0.12215723619589243, time 729.0, rides 138\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 2335, reward 442.0, memory_length 2000, epsilon 0.12204729468331613, time 732.0, rides 116\n",
      "Initial State is  [3, 4, 0]\n",
      "episode 2336, reward 854.0, memory_length 2000, epsilon 0.12193745211810114, time 730.0, rides 124\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 2337, reward 692.0, memory_length 2000, epsilon 0.12182770841119485, time 733.0, rides 133\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 2338, reward 745.0, memory_length 2000, epsilon 0.12171806347362477, time 733.0, rides 140\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 2339, reward 936.0, memory_length 2000, epsilon 0.1216085172164985, time 734.0, rides 142\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 2340, reward 687.0, memory_length 2000, epsilon 0.12149906955100365, time 726.0, rides 128\n",
      "Initial State is  [2, 5, 6]\n",
      "episode 2341, reward 606.0, memory_length 2000, epsilon 0.12138972038840774, time 724.0, rides 141\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 2342, reward 379.0, memory_length 2000, epsilon 0.12128046964005817, time 730.0, rides 133\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 2343, reward 571.0, memory_length 2000, epsilon 0.12117131721738213, time 732.0, rides 146\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 2344, reward 843.0, memory_length 2000, epsilon 0.12106226303188648, time 727.0, rides 136\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 2345, reward 807.0, memory_length 2000, epsilon 0.12095330699515779, time 729.0, rides 136\n",
      "Initial State is  [3, 7, 4]\n",
      "episode 2346, reward 875.0, memory_length 2000, epsilon 0.12084444901886214, time 726.0, rides 142\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 2347, reward 711.0, memory_length 2000, epsilon 0.12073568901474516, time 732.0, rides 128\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 2348, reward 781.0, memory_length 2000, epsilon 0.12062702689463188, time 739.0, rides 138\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 2349, reward 976.0, memory_length 2000, epsilon 0.12051846257042671, time 739.0, rides 134\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 2350, reward 600.0, memory_length 2000, epsilon 0.12040999595411332, time 729.0, rides 139\n",
      "Initial State is  [0, 9, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2351, reward 676.0, memory_length 2000, epsilon 0.12030162695775462, time 726.0, rides 140\n",
      "Initial State is  [4, 17, 6]\n",
      "episode 2352, reward 451.0, memory_length 2000, epsilon 0.12019335549349264, time 728.0, rides 125\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 2353, reward 739.0, memory_length 2000, epsilon 0.12008518147354849, time 735.0, rides 135\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 2354, reward 750.0, memory_length 2000, epsilon 0.1199771048102223, time 733.0, rides 134\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 2355, reward 533.0, memory_length 2000, epsilon 0.11986912541589309, time 723.0, rides 136\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 2356, reward 703.0, memory_length 2000, epsilon 0.11976124320301879, time 730.0, rides 127\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 2357, reward 773.0, memory_length 2000, epsilon 0.11965345808413606, time 729.0, rides 142\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 2358, reward 790.0, memory_length 2000, epsilon 0.11954576997186034, time 731.0, rides 134\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 2359, reward 364.0, memory_length 2000, epsilon 0.11943817877888566, time 729.0, rides 135\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 2360, reward 570.0, memory_length 2000, epsilon 0.11933068441798465, time 728.0, rides 124\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 2361, reward 739.0, memory_length 2000, epsilon 0.11922328680200847, time 729.0, rides 130\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 2362, reward 434.0, memory_length 2000, epsilon 0.11911598584388666, time 725.0, rides 118\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 2363, reward 466.0, memory_length 2000, epsilon 0.11900878145662716, time 722.0, rides 126\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 2364, reward 478.0, memory_length 2000, epsilon 0.1189016735533162, time 726.0, rides 127\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2365, reward 776.0, memory_length 2000, epsilon 0.11879466204711821, time 729.0, rides 146\n",
      "Initial State is  [3, 20, 1]\n",
      "episode 2366, reward 580.0, memory_length 2000, epsilon 0.1186877468512758, time 722.0, rides 142\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2367, reward 421.0, memory_length 2000, epsilon 0.11858092787910965, time 728.0, rides 145\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 2368, reward 740.0, memory_length 2000, epsilon 0.11847420504401845, time 734.0, rides 151\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 2369, reward 731.0, memory_length 2000, epsilon 0.11836757825947884, time 727.0, rides 145\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 2370, reward 761.0, memory_length 2000, epsilon 0.11826104743904531, time 735.0, rides 126\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 2371, reward 883.0, memory_length 2000, epsilon 0.11815461249635016, time 726.0, rides 131\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 2372, reward 948.0, memory_length 2000, epsilon 0.11804827334510344, time 724.0, rides 128\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 2373, reward 788.0, memory_length 2000, epsilon 0.11794202989909285, time 729.0, rides 121\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2374, reward 709.0, memory_length 2000, epsilon 0.11783588207218366, time 724.0, rides 136\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 2375, reward 764.0, memory_length 2000, epsilon 0.1177298297783187, time 732.0, rides 137\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 2376, reward 451.0, memory_length 2000, epsilon 0.1176238729315182, time 735.0, rides 129\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 2377, reward 719.0, memory_length 2000, epsilon 0.11751801144587984, time 738.0, rides 128\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 2378, reward 802.0, memory_length 2000, epsilon 0.11741224523557854, time 728.0, rides 131\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 2379, reward 502.0, memory_length 2000, epsilon 0.11730657421486652, time 727.0, rides 136\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 2380, reward 486.0, memory_length 2000, epsilon 0.11720099829807314, time 722.0, rides 123\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 2381, reward 844.0, memory_length 2000, epsilon 0.11709551739960487, time 728.0, rides 137\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 2382, reward 661.0, memory_length 2000, epsilon 0.11699013143394522, time 725.0, rides 135\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 2383, reward 630.0, memory_length 2000, epsilon 0.11688484031565467, time 725.0, rides 134\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 2384, reward 354.0, memory_length 2000, epsilon 0.11677964395937059, time 730.0, rides 121\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 2385, reward 688.0, memory_length 2000, epsilon 0.11667454227980716, time 726.0, rides 129\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 2386, reward 815.0, memory_length 2000, epsilon 0.11656953519175532, time 733.0, rides 131\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 2387, reward 669.0, memory_length 2000, epsilon 0.11646462261008274, time 724.0, rides 130\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 2388, reward 905.0, memory_length 2000, epsilon 0.11635980444973366, time 735.0, rides 128\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 2389, reward 977.0, memory_length 2000, epsilon 0.1162550806257289, time 729.0, rides 147\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 2390, reward 839.0, memory_length 2000, epsilon 0.11615045105316574, time 730.0, rides 141\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 2391, reward 646.0, memory_length 2000, epsilon 0.11604591564721789, time 725.0, rides 138\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 2392, reward 485.0, memory_length 2000, epsilon 0.1159414743231354, time 734.0, rides 133\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 2393, reward 1022.0, memory_length 2000, epsilon 0.11583712699624457, time 735.0, rides 125\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 2394, reward 480.0, memory_length 2000, epsilon 0.11573287358194795, time 729.0, rides 136\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 2395, reward 864.0, memory_length 2000, epsilon 0.11562871399572419, time 726.0, rides 119\n",
      "Initial State is  [2, 6, 1]\n",
      "episode 2396, reward 620.0, memory_length 2000, epsilon 0.11552464815312803, time 727.0, rides 124\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 2397, reward 783.0, memory_length 2000, epsilon 0.11542067596979022, time 735.0, rides 119\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 2398, reward 754.0, memory_length 2000, epsilon 0.11531679736141741, time 730.0, rides 140\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 2399, reward 444.0, memory_length 2000, epsilon 0.11521301224379213, time 725.0, rides 133\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 2400, reward 439.0, memory_length 2000, epsilon 0.11510932053277272, time 733.0, rides 130\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 2401, reward 877.0, memory_length 2000, epsilon 0.11500572214429322, time 728.0, rides 138\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2402, reward 649.0, memory_length 2000, epsilon 0.11490221699436336, time 722.0, rides 131\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 2403, reward 815.0, memory_length 2000, epsilon 0.11479880499906843, time 729.0, rides 126\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 2404, reward 606.0, memory_length 2000, epsilon 0.11469548607456927, time 725.0, rides 141\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 2405, reward 686.0, memory_length 2000, epsilon 0.11459226013710215, time 725.0, rides 137\n",
      "Initial State is  [1, 8, 3]\n",
      "episode 2406, reward 930.0, memory_length 2000, epsilon 0.11448912710297876, time 726.0, rides 136\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 2407, reward 746.0, memory_length 2000, epsilon 0.11438608688858608, time 728.0, rides 137\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 2408, reward 704.0, memory_length 2000, epsilon 0.11428313941038636, time 730.0, rides 137\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 2409, reward 795.0, memory_length 2000, epsilon 0.11418028458491701, time 724.0, rides 132\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 2410, reward 587.0, memory_length 2000, epsilon 0.11407752232879058, time 728.0, rides 138\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 2411, reward 735.0, memory_length 2000, epsilon 0.11397485255869466, time 726.0, rides 127\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 2412, reward 655.0, memory_length 2000, epsilon 0.11387227519139184, time 730.0, rides 127\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2413, reward 936.0, memory_length 2000, epsilon 0.11376979014371959, time 739.0, rides 135\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 2414, reward 804.0, memory_length 2000, epsilon 0.11366739733259024, time 728.0, rides 145\n",
      "Initial State is  [4, 7, 6]\n",
      "episode 2415, reward 655.0, memory_length 2000, epsilon 0.1135650966749909, time 730.0, rides 149\n",
      "Initial State is  [3, 8, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2416, reward 885.0, memory_length 2000, epsilon 0.11346288808798341, time 727.0, rides 129\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2417, reward 664.0, memory_length 2000, epsilon 0.11336077148870423, time 726.0, rides 136\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 2418, reward 845.0, memory_length 2000, epsilon 0.11325874679436439, time 731.0, rides 152\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 2419, reward 713.0, memory_length 2000, epsilon 0.11315681392224945, time 728.0, rides 144\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 2420, reward 563.0, memory_length 2000, epsilon 0.11305497278971943, time 730.0, rides 138\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 2421, reward 907.0, memory_length 2000, epsilon 0.11295322331420868, time 725.0, rides 131\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 2422, reward 434.0, memory_length 2000, epsilon 0.11285156541322588, time 725.0, rides 136\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 2423, reward 391.0, memory_length 2000, epsilon 0.11274999900435398, time 725.0, rides 131\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 2424, reward 699.0, memory_length 2000, epsilon 0.11264852400525006, time 733.0, rides 126\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 2425, reward 646.0, memory_length 2000, epsilon 0.11254714033364534, time 732.0, rides 147\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 2426, reward 641.0, memory_length 2000, epsilon 0.11244584790734506, time 730.0, rides 128\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 2427, reward 709.0, memory_length 2000, epsilon 0.11234464664422845, time 723.0, rides 132\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 2428, reward 1121.0, memory_length 2000, epsilon 0.11224353646224865, time 729.0, rides 139\n",
      "Initial State is  [3, 16, 0]\n",
      "episode 2429, reward 725.0, memory_length 2000, epsilon 0.11214251727943263, time 729.0, rides 130\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 2430, reward 739.0, memory_length 2000, epsilon 0.11204158901388113, time 731.0, rides 133\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 2431, reward 629.0, memory_length 2000, epsilon 0.11194075158376864, time 729.0, rides 132\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 2432, reward 779.0, memory_length 2000, epsilon 0.11184000490734325, time 733.0, rides 133\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 2433, reward 669.0, memory_length 2000, epsilon 0.11173934890292664, time 725.0, rides 131\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 2434, reward 651.0, memory_length 2000, epsilon 0.111638783488914, time 737.0, rides 125\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 2435, reward 896.0, memory_length 2000, epsilon 0.11153830858377398, time 729.0, rides 137\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 2436, reward 872.0, memory_length 2000, epsilon 0.11143792410604858, time 723.0, rides 135\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 2437, reward 697.0, memory_length 2000, epsilon 0.11133762997435313, time 726.0, rides 132\n",
      "Initial State is  [0, 13, 0]\n",
      "episode 2438, reward 693.0, memory_length 2000, epsilon 0.11123742610737622, time 737.0, rides 135\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 2439, reward 524.0, memory_length 2000, epsilon 0.11113731242387959, time 728.0, rides 125\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 2440, reward 655.0, memory_length 2000, epsilon 0.11103728884269809, time 734.0, rides 132\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 2441, reward 961.0, memory_length 2000, epsilon 0.11093735528273967, time 727.0, rides 124\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 2442, reward 859.0, memory_length 2000, epsilon 0.1108375116629852, time 726.0, rides 130\n",
      "Initial State is  [1, 9, 4]\n",
      "episode 2443, reward 790.0, memory_length 2000, epsilon 0.1107377579024885, time 734.0, rides 121\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 2444, reward 784.0, memory_length 2000, epsilon 0.11063809392037627, time 723.0, rides 138\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2445, reward 988.0, memory_length 2000, epsilon 0.11053851963584793, time 727.0, rides 128\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 2446, reward 626.0, memory_length 2000, epsilon 0.11043903496817567, time 730.0, rides 128\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 2447, reward 606.0, memory_length 2000, epsilon 0.1103396398367043, time 721.0, rides 136\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 2448, reward 815.0, memory_length 2000, epsilon 0.11024033416085127, time 726.0, rides 138\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 2449, reward 747.0, memory_length 2000, epsilon 0.11014111786010651, time 726.0, rides 114\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 2450, reward 843.0, memory_length 2000, epsilon 0.1100419908540324, time 734.0, rides 130\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 2451, reward 1024.0, memory_length 2000, epsilon 0.10994295306226377, time 732.0, rides 129\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 2452, reward 573.0, memory_length 2000, epsilon 0.10984400440450773, time 726.0, rides 130\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 2453, reward 436.0, memory_length 2000, epsilon 0.10974514480054368, time 733.0, rides 138\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 2454, reward 734.0, memory_length 2000, epsilon 0.10964637417022319, time 732.0, rides 155\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 2455, reward 729.0, memory_length 2000, epsilon 0.10954769243346998, time 729.0, rides 145\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 2456, reward 407.0, memory_length 2000, epsilon 0.10944909951027985, time 730.0, rides 132\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2457, reward 816.0, memory_length 2000, epsilon 0.1093505953207206, time 726.0, rides 130\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 2458, reward 603.0, memory_length 2000, epsilon 0.10925217978493194, time 734.0, rides 134\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 2459, reward 860.0, memory_length 2000, epsilon 0.10915385282312551, time 725.0, rides 130\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 2460, reward 670.0, memory_length 2000, epsilon 0.1090556143555847, time 726.0, rides 148\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 2461, reward 808.0, memory_length 2000, epsilon 0.10895746430266467, time 739.0, rides 138\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 2462, reward 892.0, memory_length 2000, epsilon 0.10885940258479228, time 731.0, rides 120\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 2463, reward 465.0, memory_length 2000, epsilon 0.10876142912246596, time 731.0, rides 120\n",
      "Initial State is  [1, 7, 2]\n",
      "episode 2464, reward 883.0, memory_length 2000, epsilon 0.10866354383625575, time 730.0, rides 136\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 2465, reward 774.0, memory_length 2000, epsilon 0.10856574664680312, time 737.0, rides 131\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 2466, reward 932.0, memory_length 2000, epsilon 0.10846803747482099, time 742.0, rides 127\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 2467, reward 799.0, memory_length 2000, epsilon 0.10837041624109366, time 736.0, rides 138\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 2468, reward 729.0, memory_length 2000, epsilon 0.10827288286647667, time 730.0, rides 130\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 2469, reward 412.0, memory_length 2000, epsilon 0.10817543727189684, time 725.0, rides 129\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 2470, reward 780.0, memory_length 2000, epsilon 0.10807807937835213, time 726.0, rides 133\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 2471, reward 874.0, memory_length 2000, epsilon 0.10798080910691162, time 729.0, rides 134\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 2472, reward 797.0, memory_length 2000, epsilon 0.10788362637871539, time 733.0, rides 136\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 2473, reward 1079.0, memory_length 2000, epsilon 0.10778653111497455, time 734.0, rides 142\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 2474, reward 656.0, memory_length 2000, epsilon 0.10768952323697108, time 729.0, rides 136\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 2475, reward 429.0, memory_length 2000, epsilon 0.1075926026660578, time 734.0, rides 135\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 2476, reward 622.0, memory_length 2000, epsilon 0.10749576932365836, time 729.0, rides 130\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 2477, reward 813.0, memory_length 2000, epsilon 0.10739902313126706, time 734.0, rides 131\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 2478, reward 663.0, memory_length 2000, epsilon 0.10730236401044892, time 724.0, rides 140\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 2479, reward 842.0, memory_length 2000, epsilon 0.10720579188283952, time 725.0, rides 137\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 2480, reward 876.0, memory_length 2000, epsilon 0.10710930667014495, time 737.0, rides 151\n",
      "Initial State is  [1, 6, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2481, reward 951.0, memory_length 2000, epsilon 0.10701290829414183, time 739.0, rides 145\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 2482, reward 650.0, memory_length 2000, epsilon 0.10691659667667709, time 727.0, rides 137\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 2483, reward 690.0, memory_length 2000, epsilon 0.10682037173966809, time 733.0, rides 141\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 2484, reward 1033.0, memory_length 2000, epsilon 0.10672423340510238, time 727.0, rides 133\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 2485, reward 749.0, memory_length 2000, epsilon 0.10662818159503779, time 730.0, rides 126\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 2486, reward 759.0, memory_length 2000, epsilon 0.10653221623160225, time 741.0, rides 142\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 2487, reward 1074.0, memory_length 2000, epsilon 0.1064363372369938, time 720.0, rides 143\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 2488, reward 893.0, memory_length 2000, epsilon 0.1063405445334805, time 729.0, rides 145\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 2489, reward 731.0, memory_length 2000, epsilon 0.10624483804340037, time 730.0, rides 130\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 2490, reward 630.0, memory_length 2000, epsilon 0.10614921768916132, time 727.0, rides 129\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 2491, reward 648.0, memory_length 2000, epsilon 0.10605368339324107, time 730.0, rides 155\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 2492, reward 893.0, memory_length 2000, epsilon 0.10595823507818716, time 722.0, rides 124\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 2493, reward 331.0, memory_length 2000, epsilon 0.10586287266661679, time 724.0, rides 132\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 2494, reward 541.0, memory_length 2000, epsilon 0.10576759608121683, time 727.0, rides 137\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 2495, reward 755.0, memory_length 2000, epsilon 0.10567240524474374, time 736.0, rides 130\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 2496, reward 947.0, memory_length 2000, epsilon 0.10557730008002346, time 727.0, rides 140\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2497, reward 610.0, memory_length 2000, epsilon 0.10548228050995144, time 737.0, rides 135\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2498, reward 821.0, memory_length 2000, epsilon 0.10538734645749248, time 727.0, rides 123\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 2499, reward 719.0, memory_length 2000, epsilon 0.10529249784568073, time 727.0, rides 137\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 2500, reward 590.0, memory_length 2000, epsilon 0.10519773459761962, time 725.0, rides 133\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 2501, reward 530.0, memory_length 2000, epsilon 0.10510305663648176, time 726.0, rides 127\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 2502, reward 680.0, memory_length 2000, epsilon 0.10500846388550893, time 733.0, rides 139\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 2503, reward 624.0, memory_length 2000, epsilon 0.10491395626801196, time 733.0, rides 125\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 2504, reward 774.0, memory_length 2000, epsilon 0.10481953370737075, time 727.0, rides 133\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 2505, reward 848.0, memory_length 2000, epsilon 0.10472519612703411, time 728.0, rides 145\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 2506, reward 865.0, memory_length 2000, epsilon 0.10463094345051978, time 728.0, rides 138\n",
      "Initial State is  [1, 17, 2]\n",
      "episode 2507, reward 732.0, memory_length 2000, epsilon 0.1045367756014143, time 726.0, rides 138\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 2508, reward 740.0, memory_length 2000, epsilon 0.10444269250337303, time 730.0, rides 136\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 2509, reward 455.0, memory_length 2000, epsilon 0.10434869408011999, time 740.0, rides 146\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 2510, reward 620.0, memory_length 2000, epsilon 0.10425478025544788, time 724.0, rides 139\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 2511, reward 530.0, memory_length 2000, epsilon 0.10416095095321798, time 721.0, rides 136\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 2512, reward 820.0, memory_length 2000, epsilon 0.10406720609736009, time 734.0, rides 144\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 2513, reward 641.0, memory_length 2000, epsilon 0.10397354561187246, time 731.0, rides 129\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2514, reward 787.0, memory_length 2000, epsilon 0.10387996942082177, time 728.0, rides 118\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 2515, reward 964.0, memory_length 2000, epsilon 0.10378647744834303, time 725.0, rides 132\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 2516, reward 850.0, memory_length 2000, epsilon 0.10369306961863953, time 726.0, rides 143\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 2517, reward 876.0, memory_length 2000, epsilon 0.10359974585598275, time 729.0, rides 142\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 2518, reward 552.0, memory_length 2000, epsilon 0.10350650608471236, time 731.0, rides 134\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 2519, reward 670.0, memory_length 2000, epsilon 0.10341335022923612, time 730.0, rides 151\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 2520, reward 616.0, memory_length 2000, epsilon 0.10332027821402981, time 731.0, rides 125\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 2521, reward 862.0, memory_length 2000, epsilon 0.10322728996363718, time 730.0, rides 136\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 2522, reward 814.0, memory_length 2000, epsilon 0.1031343854026699, time 725.0, rides 139\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 2523, reward 819.0, memory_length 2000, epsilon 0.1030415644558075, time 723.0, rides 140\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 2524, reward 664.0, memory_length 2000, epsilon 0.10294882704779727, time 728.0, rides 149\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 2525, reward 505.0, memory_length 2000, epsilon 0.10285617310345425, time 727.0, rides 131\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 2526, reward 680.0, memory_length 2000, epsilon 0.10276360254766115, time 734.0, rides 127\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 2527, reward 936.0, memory_length 2000, epsilon 0.10267111530536825, time 733.0, rides 136\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 2528, reward 723.0, memory_length 2000, epsilon 0.10257871130159342, time 728.0, rides 142\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 2529, reward 732.0, memory_length 2000, epsilon 0.10248639046142198, time 723.0, rides 138\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 2530, reward 873.0, memory_length 2000, epsilon 0.1023941527100067, time 733.0, rides 139\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 2531, reward 643.0, memory_length 2000, epsilon 0.10230199797256768, time 735.0, rides 138\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 2532, reward 733.0, memory_length 2000, epsilon 0.10220992617439237, time 733.0, rides 138\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 2533, reward 718.0, memory_length 2000, epsilon 0.10211793724083541, time 731.0, rides 143\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 2534, reward 985.0, memory_length 2000, epsilon 0.10202603109731866, time 727.0, rides 152\n",
      "Initial State is  [3, 8, 6]\n",
      "episode 2535, reward 638.0, memory_length 2000, epsilon 0.10193420766933106, time 734.0, rides 129\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 2536, reward 702.0, memory_length 2000, epsilon 0.10184246688242866, time 728.0, rides 149\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 2537, reward 378.0, memory_length 2000, epsilon 0.10175080866223447, time 726.0, rides 159\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 2538, reward 619.0, memory_length 2000, epsilon 0.10165923293443846, time 722.0, rides 133\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 2539, reward 882.0, memory_length 2000, epsilon 0.10156773962479747, time 729.0, rides 126\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 2540, reward 723.0, memory_length 2000, epsilon 0.10147632865913515, time 725.0, rides 130\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 2541, reward 405.0, memory_length 2000, epsilon 0.10138499996334194, time 725.0, rides 131\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 2542, reward 591.0, memory_length 2000, epsilon 0.10129375346337492, time 730.0, rides 121\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 2543, reward 979.0, memory_length 2000, epsilon 0.10120258908525788, time 723.0, rides 138\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 2544, reward 315.0, memory_length 2000, epsilon 0.10111150675508114, time 733.0, rides 125\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 2545, reward 776.0, memory_length 2000, epsilon 0.10102050639900156, time 724.0, rides 148\n",
      "Initial State is  [4, 8, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2546, reward 590.0, memory_length 2000, epsilon 0.10092958794324246, time 730.0, rides 134\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 2547, reward 594.0, memory_length 2000, epsilon 0.10083875131409353, time 730.0, rides 128\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 2548, reward 655.0, memory_length 2000, epsilon 0.10074799643791085, time 727.0, rides 137\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 2549, reward 621.0, memory_length 2000, epsilon 0.10065732324111673, time 723.0, rides 129\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 2550, reward 659.0, memory_length 2000, epsilon 0.10056673165019972, time 732.0, rides 122\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 2551, reward 952.0, memory_length 2000, epsilon 0.10047622159171454, time 726.0, rides 150\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 2552, reward 669.0, memory_length 2000, epsilon 0.100385792992282, time 725.0, rides 126\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 2553, reward 872.0, memory_length 2000, epsilon 0.10029544577858894, time 733.0, rides 136\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 2554, reward 699.0, memory_length 2000, epsilon 0.10020517987738821, time 726.0, rides 129\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2555, reward 648.0, memory_length 2000, epsilon 0.10011499521549856, time 727.0, rides 157\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 2556, reward 728.0, memory_length 2000, epsilon 0.10002489171980461, time 728.0, rides 143\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 2557, reward 520.0, memory_length 2000, epsilon 0.09993486931725679, time 722.0, rides 124\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 2558, reward 730.0, memory_length 2000, epsilon 0.09984492793487125, time 730.0, rides 142\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 2559, reward 691.0, memory_length 2000, epsilon 0.09975506749972987, time 721.0, rides 132\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 2560, reward 579.0, memory_length 2000, epsilon 0.09966528793898011, time 732.0, rides 125\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 2561, reward 1011.0, memory_length 2000, epsilon 0.09957558917983503, time 733.0, rides 125\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 2562, reward 1020.0, memory_length 2000, epsilon 0.09948597114957318, time 724.0, rides 132\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 2563, reward 952.0, memory_length 2000, epsilon 0.09939643377553856, time 732.0, rides 127\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 2564, reward 1155.0, memory_length 2000, epsilon 0.09930697698514057, time 723.0, rides 134\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 2565, reward 688.0, memory_length 2000, epsilon 0.09921760070585395, time 735.0, rides 149\n",
      "Initial State is  [2, 23, 4]\n",
      "episode 2566, reward 1183.0, memory_length 2000, epsilon 0.09912830486521867, time 730.0, rides 140\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 2567, reward 575.0, memory_length 2000, epsilon 0.09903908939083998, time 728.0, rides 138\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 2568, reward 664.0, memory_length 2000, epsilon 0.09894995421038823, time 729.0, rides 127\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 2569, reward 821.0, memory_length 2000, epsilon 0.09886089925159888, time 732.0, rides 131\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 2570, reward 1290.0, memory_length 2000, epsilon 0.09877192444227244, time 733.0, rides 130\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 2571, reward 719.0, memory_length 2000, epsilon 0.09868302971027439, time 735.0, rides 136\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 2572, reward 512.0, memory_length 2000, epsilon 0.09859421498353514, time 728.0, rides 134\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 2573, reward 1086.0, memory_length 2000, epsilon 0.09850548019004995, time 735.0, rides 133\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 2574, reward 740.0, memory_length 2000, epsilon 0.09841682525787891, time 734.0, rides 143\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 2575, reward 766.0, memory_length 2000, epsilon 0.09832825011514681, time 727.0, rides 137\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 2576, reward 614.0, memory_length 2000, epsilon 0.09823975469004319, time 728.0, rides 140\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 2577, reward 742.0, memory_length 2000, epsilon 0.09815133891082214, time 724.0, rides 134\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 2578, reward 882.0, memory_length 2000, epsilon 0.0980630027058024, time 730.0, rides 133\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 2579, reward 933.0, memory_length 2000, epsilon 0.09797474600336717, time 730.0, rides 138\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 2580, reward 783.0, memory_length 2000, epsilon 0.09788656873196414, time 728.0, rides 131\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 2581, reward 523.0, memory_length 2000, epsilon 0.09779847082010537, time 726.0, rides 139\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 2582, reward 1143.0, memory_length 2000, epsilon 0.09771045219636727, time 726.0, rides 152\n",
      "Initial State is  [4, 5, 3]\n",
      "episode 2583, reward 751.0, memory_length 2000, epsilon 0.09762251278939055, time 727.0, rides 129\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 2584, reward 519.0, memory_length 2000, epsilon 0.0975346525278801, time 726.0, rides 132\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 2585, reward 1026.0, memory_length 2000, epsilon 0.097446871340605, time 723.0, rides 128\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2586, reward 900.0, memory_length 2000, epsilon 0.09735916915639846, time 728.0, rides 144\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 2587, reward 919.0, memory_length 2000, epsilon 0.0972715459041577, time 730.0, rides 125\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 2588, reward 739.0, memory_length 2000, epsilon 0.09718400151284395, time 733.0, rides 135\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 2589, reward 846.0, memory_length 2000, epsilon 0.09709653591148239, time 727.0, rides 132\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 2590, reward 491.0, memory_length 2000, epsilon 0.09700914902916205, time 722.0, rides 135\n",
      "Initial State is  [3, 14, 3]\n",
      "episode 2591, reward 628.0, memory_length 2000, epsilon 0.09692184079503581, time 727.0, rides 129\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 2592, reward 586.0, memory_length 2000, epsilon 0.09683461113832027, time 733.0, rides 127\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 2593, reward 574.0, memory_length 2000, epsilon 0.09674745998829579, time 728.0, rides 121\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 2594, reward 757.0, memory_length 2000, epsilon 0.09666038727430631, time 728.0, rides 137\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 2595, reward 689.0, memory_length 2000, epsilon 0.09657339292575944, time 728.0, rides 138\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 2596, reward 650.0, memory_length 2000, epsilon 0.09648647687212625, time 733.0, rides 162\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 2597, reward 779.0, memory_length 2000, epsilon 0.09639963904294133, time 729.0, rides 130\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 2598, reward 758.0, memory_length 2000, epsilon 0.09631287936780268, time 729.0, rides 141\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 2599, reward 370.0, memory_length 2000, epsilon 0.09622619777637166, time 729.0, rides 129\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 2600, reward 719.0, memory_length 2000, epsilon 0.09613959419837292, time 733.0, rides 130\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 2601, reward 542.0, memory_length 2000, epsilon 0.09605306856359438, time 725.0, rides 130\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 2602, reward 829.0, memory_length 2000, epsilon 0.09596662080188714, time 727.0, rides 137\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 2603, reward 815.0, memory_length 2000, epsilon 0.09588025084316544, time 736.0, rides 143\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 2604, reward 708.0, memory_length 2000, epsilon 0.0957939586174066, time 734.0, rides 140\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 2605, reward 614.0, memory_length 2000, epsilon 0.09570774405465093, time 728.0, rides 128\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 2606, reward 972.0, memory_length 2000, epsilon 0.09562160708500175, time 727.0, rides 131\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 2607, reward 742.0, memory_length 2000, epsilon 0.09553554763862525, time 721.0, rides 136\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 2608, reward 791.0, memory_length 2000, epsilon 0.09544956564575048, time 724.0, rides 142\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 2609, reward 933.0, memory_length 2000, epsilon 0.09536366103666931, time 726.0, rides 131\n",
      "Initial State is  [1, 1, 5]\n",
      "episode 2610, reward 665.0, memory_length 2000, epsilon 0.0952778337417363, time 727.0, rides 130\n",
      "Initial State is  [1, 11, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2611, reward 715.0, memory_length 2000, epsilon 0.09519208369136874, time 730.0, rides 132\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 2612, reward 801.0, memory_length 2000, epsilon 0.09510641081604651, time 727.0, rides 135\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 2613, reward 427.0, memory_length 2000, epsilon 0.09502081504631207, time 734.0, rides 152\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 2614, reward 654.0, memory_length 2000, epsilon 0.0949352963127704, time 737.0, rides 134\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 2615, reward 910.0, memory_length 2000, epsilon 0.09484985454608891, time 735.0, rides 134\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 2616, reward 351.0, memory_length 2000, epsilon 0.09476448967699742, time 730.0, rides 146\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 2617, reward 615.0, memory_length 2000, epsilon 0.09467920163628812, time 731.0, rides 139\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 2618, reward 677.0, memory_length 2000, epsilon 0.09459399035481546, time 725.0, rides 149\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 2619, reward 882.0, memory_length 2000, epsilon 0.09450885576349612, time 723.0, rides 138\n",
      "Initial State is  [4, 1, 6]\n",
      "episode 2620, reward 482.0, memory_length 2000, epsilon 0.09442379779330896, time 729.0, rides 147\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 2621, reward 740.0, memory_length 2000, epsilon 0.09433881637529498, time 721.0, rides 146\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 2622, reward 746.0, memory_length 2000, epsilon 0.09425391144055721, time 728.0, rides 141\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2623, reward 530.0, memory_length 2000, epsilon 0.0941690829202607, time 734.0, rides 125\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 2624, reward 655.0, memory_length 2000, epsilon 0.09408433074563247, time 743.0, rides 145\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 2625, reward 668.0, memory_length 2000, epsilon 0.0939996548479614, time 731.0, rides 150\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 2626, reward 1101.0, memory_length 2000, epsilon 0.09391505515859823, time 723.0, rides 119\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 2627, reward 1007.0, memory_length 2000, epsilon 0.09383053160895549, time 726.0, rides 143\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 2628, reward 804.0, memory_length 2000, epsilon 0.09374608413050743, time 733.0, rides 150\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 2629, reward 591.0, memory_length 2000, epsilon 0.09366171265478998, time 728.0, rides 156\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 2630, reward 983.0, memory_length 2000, epsilon 0.09357741711340067, time 722.0, rides 156\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 2631, reward 951.0, memory_length 2000, epsilon 0.09349319743799861, time 732.0, rides 149\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 2632, reward 820.0, memory_length 2000, epsilon 0.0934090535603044, time 722.0, rides 144\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 2633, reward 687.0, memory_length 2000, epsilon 0.09332498541210013, time 727.0, rides 143\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 2634, reward 835.0, memory_length 2000, epsilon 0.09324099292522924, time 732.0, rides 135\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 2635, reward 1084.0, memory_length 2000, epsilon 0.09315707603159652, time 724.0, rides 139\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 2636, reward 957.0, memory_length 2000, epsilon 0.09307323466316808, time 733.0, rides 134\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 2637, reward 894.0, memory_length 2000, epsilon 0.09298946875197123, time 721.0, rides 132\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 2638, reward 845.0, memory_length 2000, epsilon 0.09290577823009445, time 726.0, rides 132\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 2639, reward 706.0, memory_length 2000, epsilon 0.09282216302968736, time 731.0, rides 139\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 2640, reward 303.0, memory_length 2000, epsilon 0.09273862308296064, time 729.0, rides 143\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 2641, reward 464.0, memory_length 2000, epsilon 0.09265515832218597, time 725.0, rides 129\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 2642, reward 565.0, memory_length 2000, epsilon 0.092571768679696, time 727.0, rides 128\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 2643, reward 555.0, memory_length 2000, epsilon 0.09248845408788428, time 733.0, rides 144\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 2644, reward 980.0, memory_length 2000, epsilon 0.09240521447920519, time 727.0, rides 129\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 2645, reward 939.0, memory_length 2000, epsilon 0.09232204978617391, time 727.0, rides 135\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 2646, reward 649.0, memory_length 2000, epsilon 0.09223895994136636, time 727.0, rides 136\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 2647, reward 804.0, memory_length 2000, epsilon 0.09215594487741913, time 725.0, rides 127\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 2648, reward 444.0, memory_length 2000, epsilon 0.09207300452702945, time 727.0, rides 129\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 2649, reward 975.0, memory_length 2000, epsilon 0.09199013882295512, time 732.0, rides 137\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 2650, reward 607.0, memory_length 2000, epsilon 0.09190734769801447, time 725.0, rides 134\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 2651, reward 830.0, memory_length 2000, epsilon 0.09182463108508625, time 728.0, rides 144\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 2652, reward 956.0, memory_length 2000, epsilon 0.09174198891710966, time 724.0, rides 141\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 2653, reward 688.0, memory_length 2000, epsilon 0.09165942112708426, time 726.0, rides 139\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 2654, reward 553.0, memory_length 2000, epsilon 0.09157692764806989, time 732.0, rides 148\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 2655, reward 650.0, memory_length 2000, epsilon 0.09149450841318663, time 726.0, rides 122\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 2656, reward 634.0, memory_length 2000, epsilon 0.09141216335561475, time 729.0, rides 132\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 2657, reward 684.0, memory_length 2000, epsilon 0.0913298924085947, time 725.0, rides 141\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 2658, reward 872.0, memory_length 2000, epsilon 0.09124769550542695, time 737.0, rides 132\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 2659, reward 911.0, memory_length 2000, epsilon 0.09116557257947207, time 726.0, rides 136\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 2660, reward 832.0, memory_length 2000, epsilon 0.09108352356415055, time 728.0, rides 134\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 2661, reward 988.0, memory_length 2000, epsilon 0.0910015483929428, time 727.0, rides 140\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 2662, reward 633.0, memory_length 2000, epsilon 0.09091964699938916, time 728.0, rides 134\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 2663, reward 643.0, memory_length 2000, epsilon 0.09083781931708972, time 727.0, rides 128\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 2664, reward 875.0, memory_length 2000, epsilon 0.09075606527970434, time 722.0, rides 124\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 2665, reward 827.0, memory_length 2000, epsilon 0.09067438482095261, time 728.0, rides 135\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 2666, reward 749.0, memory_length 2000, epsilon 0.09059277787461376, time 729.0, rides 149\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 2667, reward 728.0, memory_length 2000, epsilon 0.09051124437452661, time 726.0, rides 120\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 2668, reward 1000.0, memory_length 2000, epsilon 0.09042978425458953, time 723.0, rides 136\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 2669, reward 694.0, memory_length 2000, epsilon 0.0903483974487604, time 727.0, rides 133\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 2670, reward 782.0, memory_length 2000, epsilon 0.09026708389105652, time 724.0, rides 130\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2671, reward 662.0, memory_length 2000, epsilon 0.09018584351555457, time 731.0, rides 134\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 2672, reward 614.0, memory_length 2000, epsilon 0.09010467625639057, time 728.0, rides 129\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 2673, reward 730.0, memory_length 2000, epsilon 0.09002358204775981, time 737.0, rides 131\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 2674, reward 903.0, memory_length 2000, epsilon 0.08994256082391683, time 738.0, rides 137\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 2675, reward 793.0, memory_length 2000, epsilon 0.08986161251917531, time 729.0, rides 137\n",
      "Initial State is  [1, 13, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2676, reward 810.0, memory_length 2000, epsilon 0.08978073706790805, time 730.0, rides 139\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 2677, reward 640.0, memory_length 2000, epsilon 0.08969993440454693, time 725.0, rides 113\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 2678, reward 687.0, memory_length 2000, epsilon 0.08961920446358283, time 727.0, rides 135\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2679, reward 737.0, memory_length 2000, epsilon 0.08953854717956561, time 732.0, rides 130\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 2680, reward 1076.0, memory_length 2000, epsilon 0.089457962487104, time 730.0, rides 123\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 2681, reward 445.0, memory_length 2000, epsilon 0.08937745032086561, time 722.0, rides 118\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 2682, reward 693.0, memory_length 2000, epsilon 0.08929701061557684, time 724.0, rides 132\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 2683, reward 558.0, memory_length 2000, epsilon 0.08921664330602282, time 725.0, rides 134\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 2684, reward 792.0, memory_length 2000, epsilon 0.0891363483270474, time 726.0, rides 139\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 2685, reward 733.0, memory_length 2000, epsilon 0.08905612561355306, time 722.0, rides 136\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 2686, reward 806.0, memory_length 2000, epsilon 0.08897597510050086, time 727.0, rides 128\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 2687, reward 790.0, memory_length 2000, epsilon 0.0888958967229104, time 726.0, rides 136\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 2688, reward 759.0, memory_length 2000, epsilon 0.08881589041585979, time 725.0, rides 135\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 2689, reward 750.0, memory_length 2000, epsilon 0.08873595611448551, time 723.0, rides 129\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 2690, reward 722.0, memory_length 2000, epsilon 0.08865609375398247, time 725.0, rides 141\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 2691, reward 693.0, memory_length 2000, epsilon 0.08857630326960388, time 725.0, rides 134\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 2692, reward 809.0, memory_length 2000, epsilon 0.08849658459666124, time 735.0, rides 132\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 2693, reward 677.0, memory_length 2000, epsilon 0.08841693767052423, time 735.0, rides 135\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 2694, reward 931.0, memory_length 2000, epsilon 0.08833736242662076, time 725.0, rides 133\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 2695, reward 781.0, memory_length 2000, epsilon 0.0882578588004368, time 732.0, rides 139\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 2696, reward 885.0, memory_length 2000, epsilon 0.0881784267275164, time 726.0, rides 123\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 2697, reward 705.0, memory_length 2000, epsilon 0.08809906614346164, time 736.0, rides 137\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 2698, reward 647.0, memory_length 2000, epsilon 0.08801977698393251, time 724.0, rides 140\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 2699, reward 982.0, memory_length 2000, epsilon 0.08794055918464697, time 730.0, rides 131\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 2700, reward 761.0, memory_length 2000, epsilon 0.08786141268138078, time 737.0, rides 129\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 2701, reward 933.0, memory_length 2000, epsilon 0.08778233740996755, time 732.0, rides 136\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 2702, reward 900.0, memory_length 2000, epsilon 0.08770333330629858, time 730.0, rides 141\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 2703, reward 987.0, memory_length 2000, epsilon 0.0876244003063229, time 731.0, rides 142\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 2704, reward 836.0, memory_length 2000, epsilon 0.08754553834604721, time 736.0, rides 129\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 2705, reward 473.0, memory_length 2000, epsilon 0.08746674736153577, time 735.0, rides 139\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 2706, reward 1024.0, memory_length 2000, epsilon 0.08738802728891039, time 734.0, rides 118\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 2707, reward 735.0, memory_length 2000, epsilon 0.08730937806435037, time 740.0, rides 121\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 2708, reward 643.0, memory_length 2000, epsilon 0.08723079962409244, time 735.0, rides 126\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 2709, reward 748.0, memory_length 2000, epsilon 0.08715229190443076, time 729.0, rides 131\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 2710, reward 658.0, memory_length 2000, epsilon 0.08707385484171677, time 729.0, rides 126\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 2711, reward 745.0, memory_length 2000, epsilon 0.08699548837235922, time 731.0, rides 138\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 2712, reward 640.0, memory_length 2000, epsilon 0.0869171924328241, time 731.0, rides 123\n",
      "Initial State is  [1, 21, 2]\n",
      "episode 2713, reward 552.0, memory_length 2000, epsilon 0.08683896695963456, time 725.0, rides 125\n",
      "Initial State is  [1, 5, 3]\n",
      "episode 2714, reward 537.0, memory_length 2000, epsilon 0.08676081188937089, time 730.0, rides 135\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 2715, reward 605.0, memory_length 2000, epsilon 0.08668272715867045, time 734.0, rides 145\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 2716, reward 995.0, memory_length 2000, epsilon 0.08660471270422765, time 729.0, rides 126\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 2717, reward 814.0, memory_length 2000, epsilon 0.08652676846279385, time 730.0, rides 128\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 2718, reward 631.0, memory_length 2000, epsilon 0.08644889437117734, time 737.0, rides 137\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 2719, reward 669.0, memory_length 2000, epsilon 0.08637109036624328, time 726.0, rides 134\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 2720, reward 678.0, memory_length 2000, epsilon 0.08629335638491366, time 726.0, rides 132\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 2721, reward 972.0, memory_length 2000, epsilon 0.08621569236416723, time 734.0, rides 133\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 2722, reward 858.0, memory_length 2000, epsilon 0.08613809824103948, time 728.0, rides 134\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2723, reward 403.0, memory_length 2000, epsilon 0.08606057395262254, time 732.0, rides 124\n",
      "Initial State is  [3, 9, 5]\n",
      "episode 2724, reward 805.0, memory_length 2000, epsilon 0.08598311943606518, time 722.0, rides 146\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 2725, reward 749.0, memory_length 2000, epsilon 0.08590573462857272, time 733.0, rides 147\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 2726, reward 469.0, memory_length 2000, epsilon 0.085828419467407, time 734.0, rides 126\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 2727, reward 504.0, memory_length 2000, epsilon 0.08575117388988633, time 726.0, rides 141\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 2728, reward 873.0, memory_length 2000, epsilon 0.08567399783338543, time 733.0, rides 130\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 2729, reward 441.0, memory_length 2000, epsilon 0.08559689123533538, time 733.0, rides 126\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 2730, reward 756.0, memory_length 2000, epsilon 0.08551985403322358, time 733.0, rides 137\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 2731, reward 740.0, memory_length 2000, epsilon 0.08544288616459368, time 729.0, rides 132\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 2732, reward 489.0, memory_length 2000, epsilon 0.08536598756704554, time 726.0, rides 132\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2733, reward 631.0, memory_length 2000, epsilon 0.0852891581782352, time 730.0, rides 137\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 2734, reward 827.0, memory_length 2000, epsilon 0.08521239793587478, time 729.0, rides 127\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 2735, reward 735.0, memory_length 2000, epsilon 0.08513570677773248, time 731.0, rides 134\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 2736, reward 719.0, memory_length 2000, epsilon 0.08505908464163252, time 726.0, rides 140\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 2737, reward 1080.0, memory_length 2000, epsilon 0.08498253146545505, time 727.0, rides 137\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 2738, reward 920.0, memory_length 2000, epsilon 0.08490604718713614, time 731.0, rides 141\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 2739, reward 639.0, memory_length 2000, epsilon 0.08482963174466772, time 730.0, rides 126\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 2740, reward 826.0, memory_length 2000, epsilon 0.08475328507609751, time 728.0, rides 133\n",
      "Initial State is  [3, 3, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2741, reward 596.0, memory_length 2000, epsilon 0.08467700711952902, time 724.0, rides 136\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 2742, reward 711.0, memory_length 2000, epsilon 0.08460079781312145, time 733.0, rides 124\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2743, reward 794.0, memory_length 2000, epsilon 0.08452465709508963, time 727.0, rides 152\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 2744, reward 459.0, memory_length 2000, epsilon 0.08444858490370405, time 724.0, rides 140\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 2745, reward 634.0, memory_length 2000, epsilon 0.08437258117729071, time 729.0, rides 142\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 2746, reward 460.0, memory_length 2000, epsilon 0.08429664585423115, time 723.0, rides 133\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 2747, reward 940.0, memory_length 2000, epsilon 0.08422077887296234, time 725.0, rides 136\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 2748, reward 750.0, memory_length 2000, epsilon 0.08414498017197668, time 726.0, rides 135\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 2749, reward 717.0, memory_length 2000, epsilon 0.08406924968982189, time 730.0, rides 124\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 2750, reward 841.0, memory_length 2000, epsilon 0.08399358736510106, time 728.0, rides 145\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 2751, reward 756.0, memory_length 2000, epsilon 0.08391799313647247, time 723.0, rides 134\n",
      "Initial State is  [3, 19, 3]\n",
      "episode 2752, reward 612.0, memory_length 2000, epsilon 0.08384246694264964, time 727.0, rides 136\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 2753, reward 789.0, memory_length 2000, epsilon 0.08376700872240125, time 733.0, rides 139\n",
      "Initial State is  [4, 19, 4]\n",
      "episode 2754, reward 623.0, memory_length 2000, epsilon 0.0836916184145511, time 724.0, rides 127\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 2755, reward 512.0, memory_length 2000, epsilon 0.083616295957978, time 735.0, rides 113\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 2756, reward 946.0, memory_length 2000, epsilon 0.08354104129161581, time 728.0, rides 139\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 2757, reward 599.0, memory_length 2000, epsilon 0.08346585435445336, time 730.0, rides 143\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 2758, reward 678.0, memory_length 2000, epsilon 0.08339073508553435, time 729.0, rides 132\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 2759, reward 535.0, memory_length 2000, epsilon 0.08331568342395737, time 726.0, rides 130\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 2760, reward 632.0, memory_length 2000, epsilon 0.0832406993088758, time 731.0, rides 137\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 2761, reward 984.0, memory_length 2000, epsilon 0.08316578267949781, time 727.0, rides 128\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 2762, reward 729.0, memory_length 2000, epsilon 0.08309093347508627, time 725.0, rides 153\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 2763, reward 978.0, memory_length 2000, epsilon 0.0830161516349587, time 727.0, rides 133\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 2764, reward 883.0, memory_length 2000, epsilon 0.08294143709848723, time 735.0, rides 132\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2765, reward 631.0, memory_length 2000, epsilon 0.08286678980509858, time 727.0, rides 128\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 2766, reward 712.0, memory_length 2000, epsilon 0.08279220969427399, time 722.0, rides 127\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 2767, reward 733.0, memory_length 2000, epsilon 0.08271769670554914, time 730.0, rides 136\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 2768, reward 644.0, memory_length 2000, epsilon 0.08264325077851414, time 728.0, rides 145\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2769, reward 784.0, memory_length 2000, epsilon 0.08256887185281347, time 734.0, rides 136\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 2770, reward 1039.0, memory_length 2000, epsilon 0.08249455986814594, time 721.0, rides 141\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 2771, reward 776.0, memory_length 2000, epsilon 0.0824203147642646, time 726.0, rides 135\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 2772, reward 501.0, memory_length 2000, epsilon 0.08234613648097676, time 722.0, rides 134\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 2773, reward 876.0, memory_length 2000, epsilon 0.08227202495814388, time 734.0, rides 144\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 2774, reward 819.0, memory_length 2000, epsilon 0.08219798013568155, time 726.0, rides 137\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 2775, reward 665.0, memory_length 2000, epsilon 0.08212400195355944, time 732.0, rides 126\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 2776, reward 1216.0, memory_length 2000, epsilon 0.08205009035180123, time 723.0, rides 144\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 2777, reward 854.0, memory_length 2000, epsilon 0.08197624527048461, time 724.0, rides 129\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 2778, reward 694.0, memory_length 2000, epsilon 0.08190246664974117, time 730.0, rides 135\n",
      "Initial State is  [1, 16, 6]\n",
      "episode 2779, reward 768.0, memory_length 2000, epsilon 0.0818287544297564, time 729.0, rides 138\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 2780, reward 954.0, memory_length 2000, epsilon 0.08175510855076962, time 728.0, rides 131\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 2781, reward 806.0, memory_length 2000, epsilon 0.08168152895307393, time 730.0, rides 146\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 2782, reward 936.0, memory_length 2000, epsilon 0.08160801557701616, time 726.0, rides 131\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 2783, reward 881.0, memory_length 2000, epsilon 0.08153456836299684, time 729.0, rides 140\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 2784, reward 779.0, memory_length 2000, epsilon 0.08146118725147014, time 732.0, rides 137\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 2785, reward 1148.0, memory_length 2000, epsilon 0.08138787218294381, time 733.0, rides 147\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 2786, reward 1128.0, memory_length 2000, epsilon 0.08131462309797917, time 732.0, rides 127\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 2787, reward 560.0, memory_length 2000, epsilon 0.08124143993719099, time 725.0, rides 151\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 2788, reward 866.0, memory_length 2000, epsilon 0.08116832264124751, time 722.0, rides 149\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 2789, reward 788.0, memory_length 2000, epsilon 0.0810952711508704, time 729.0, rides 155\n",
      "Initial State is  [3, 5, 0]\n",
      "episode 2790, reward 634.0, memory_length 2000, epsilon 0.08102228540683461, time 733.0, rides 147\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 2791, reward 869.0, memory_length 2000, epsilon 0.08094936534996845, time 724.0, rides 139\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 2792, reward 1070.0, memory_length 2000, epsilon 0.08087651092115349, time 725.0, rides 139\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 2793, reward 1038.0, memory_length 2000, epsilon 0.08080372206132444, time 727.0, rides 142\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 2794, reward 927.0, memory_length 2000, epsilon 0.08073099871146924, time 733.0, rides 139\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 2795, reward 506.0, memory_length 2000, epsilon 0.08065834081262892, time 731.0, rides 134\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 2796, reward 694.0, memory_length 2000, epsilon 0.08058574830589756, time 731.0, rides 125\n",
      "Initial State is  [0, 4, 1]\n",
      "episode 2797, reward 908.0, memory_length 2000, epsilon 0.08051322113242225, time 730.0, rides 146\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 2798, reward 758.0, memory_length 2000, epsilon 0.08044075923340308, time 725.0, rides 130\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 2799, reward 483.0, memory_length 2000, epsilon 0.08036836255009301, time 721.0, rides 132\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 2800, reward 633.0, memory_length 2000, epsilon 0.08029603102379793, time 735.0, rides 134\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 2801, reward 623.0, memory_length 2000, epsilon 0.0802237645958765, time 733.0, rides 126\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 2802, reward 860.0, memory_length 2000, epsilon 0.08015156320774021, time 724.0, rides 126\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 2803, reward 832.0, memory_length 2000, epsilon 0.08007942680085324, time 725.0, rides 143\n",
      "Initial State is  [1, 21, 3]\n",
      "episode 2804, reward 704.0, memory_length 2000, epsilon 0.08000735531673248, time 729.0, rides 141\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 2805, reward 922.0, memory_length 2000, epsilon 0.07993534869694742, time 730.0, rides 136\n",
      "Initial State is  [1, 15, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2806, reward 933.0, memory_length 2000, epsilon 0.07986340688312017, time 725.0, rides 128\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 2807, reward 747.0, memory_length 2000, epsilon 0.07979152981692536, time 722.0, rides 144\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 2808, reward 749.0, memory_length 2000, epsilon 0.07971971744009013, time 731.0, rides 142\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 2809, reward 845.0, memory_length 2000, epsilon 0.07964796969439404, time 730.0, rides 127\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 2810, reward 779.0, memory_length 2000, epsilon 0.07957628652166908, time 739.0, rides 149\n",
      "Initial State is  [3, 16, 4]\n",
      "episode 2811, reward 842.0, memory_length 2000, epsilon 0.07950466786379957, time 727.0, rides 149\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 2812, reward 895.0, memory_length 2000, epsilon 0.07943311366272215, time 730.0, rides 133\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 2813, reward 665.0, memory_length 2000, epsilon 0.07936162386042571, time 728.0, rides 129\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 2814, reward 867.0, memory_length 2000, epsilon 0.07929019839895132, time 727.0, rides 139\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 2815, reward 775.0, memory_length 2000, epsilon 0.07921883722039226, time 726.0, rides 129\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 2816, reward 765.0, memory_length 2000, epsilon 0.07914754026689391, time 730.0, rides 143\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 2817, reward 834.0, memory_length 2000, epsilon 0.07907630748065371, time 730.0, rides 140\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 2818, reward 683.0, memory_length 2000, epsilon 0.07900513880392113, time 726.0, rides 140\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 2819, reward 842.0, memory_length 2000, epsilon 0.0789340341789976, time 740.0, rides 139\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 2820, reward 964.0, memory_length 2000, epsilon 0.07886299354823649, time 731.0, rides 134\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 2821, reward 843.0, memory_length 2000, epsilon 0.07879201685404308, time 728.0, rides 125\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 2822, reward 546.0, memory_length 2000, epsilon 0.07872110403887445, time 731.0, rides 142\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 2823, reward 1166.0, memory_length 2000, epsilon 0.07865025504523945, time 737.0, rides 132\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 2824, reward 651.0, memory_length 2000, epsilon 0.07857946981569874, time 725.0, rides 137\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 2825, reward 650.0, memory_length 2000, epsilon 0.07850874829286461, time 728.0, rides 161\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 2826, reward 954.0, memory_length 2000, epsilon 0.07843809041940103, time 722.0, rides 140\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 2827, reward 783.0, memory_length 2000, epsilon 0.07836749613802357, time 727.0, rides 136\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 2828, reward 547.0, memory_length 2000, epsilon 0.07829696539149936, time 734.0, rides 146\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2829, reward 663.0, memory_length 2000, epsilon 0.07822649812264701, time 732.0, rides 133\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 2830, reward 1080.0, memory_length 2000, epsilon 0.07815609427433663, time 733.0, rides 143\n",
      "Initial State is  [0, 6, 2]\n",
      "episode 2831, reward 871.0, memory_length 2000, epsilon 0.07808575378948973, time 726.0, rides 131\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 2832, reward 898.0, memory_length 2000, epsilon 0.07801547661107919, time 735.0, rides 127\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 2833, reward 671.0, memory_length 2000, epsilon 0.07794526268212922, time 727.0, rides 151\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 2834, reward 728.0, memory_length 2000, epsilon 0.0778751119457153, time 723.0, rides 132\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 2835, reward 694.0, memory_length 2000, epsilon 0.07780502434496415, time 726.0, rides 131\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 2836, reward 636.0, memory_length 2000, epsilon 0.07773499982305368, time 736.0, rides 131\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 2837, reward 672.0, memory_length 2000, epsilon 0.07766503832321293, time 725.0, rides 135\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 2838, reward 1017.0, memory_length 2000, epsilon 0.07759513978872204, time 724.0, rides 142\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 2839, reward 893.0, memory_length 2000, epsilon 0.07752530416291219, time 729.0, rides 123\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 2840, reward 781.0, memory_length 2000, epsilon 0.07745553138916557, time 726.0, rides 131\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 2841, reward 565.0, memory_length 2000, epsilon 0.07738582141091532, time 728.0, rides 131\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 2842, reward 944.0, memory_length 2000, epsilon 0.0773161741716455, time 727.0, rides 134\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 2843, reward 792.0, memory_length 2000, epsilon 0.07724658961489102, time 722.0, rides 139\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 2844, reward 793.0, memory_length 2000, epsilon 0.07717706768423761, time 729.0, rides 131\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 2845, reward 797.0, memory_length 2000, epsilon 0.0771076083233218, time 725.0, rides 138\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 2846, reward 1210.0, memory_length 2000, epsilon 0.0770382114758308, time 729.0, rides 130\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 2847, reward 849.0, memory_length 2000, epsilon 0.07696887708550255, time 732.0, rides 121\n",
      "Initial State is  [3, 7, 0]\n",
      "episode 2848, reward 778.0, memory_length 2000, epsilon 0.0768996050961256, time 726.0, rides 146\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 2849, reward 756.0, memory_length 2000, epsilon 0.0768303954515391, time 722.0, rides 138\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 2850, reward 486.0, memory_length 2000, epsilon 0.07676124809563271, time 725.0, rides 137\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 2851, reward 736.0, memory_length 2000, epsilon 0.07669216297234664, time 724.0, rides 133\n",
      "Initial State is  [4, 20, 3]\n",
      "episode 2852, reward 540.0, memory_length 2000, epsilon 0.07662314002567153, time 733.0, rides 139\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 2853, reward 676.0, memory_length 2000, epsilon 0.07655417919964842, time 728.0, rides 139\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 2854, reward 683.0, memory_length 2000, epsilon 0.07648528043836873, time 729.0, rides 142\n",
      "Initial State is  [1, 10, 2]\n",
      "episode 2855, reward 814.0, memory_length 2000, epsilon 0.0764164436859742, time 726.0, rides 138\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 2856, reward 656.0, memory_length 2000, epsilon 0.07634766888665681, time 733.0, rides 138\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 2857, reward 1020.0, memory_length 2000, epsilon 0.07627895598465882, time 726.0, rides 145\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 2858, reward 628.0, memory_length 2000, epsilon 0.07621030492427262, time 731.0, rides 129\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 2859, reward 892.0, memory_length 2000, epsilon 0.07614171564984078, time 725.0, rides 129\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 2860, reward 881.0, memory_length 2000, epsilon 0.07607318810575592, time 732.0, rides 133\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 2861, reward 856.0, memory_length 2000, epsilon 0.07600472223646074, time 726.0, rides 137\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 2862, reward 849.0, memory_length 2000, epsilon 0.07593631798644793, time 734.0, rides 150\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 2863, reward 585.0, memory_length 2000, epsilon 0.07586797530026013, time 734.0, rides 135\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 2864, reward 905.0, memory_length 2000, epsilon 0.0757996941224899, time 729.0, rides 129\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 2865, reward 641.0, memory_length 2000, epsilon 0.07573147439777965, time 722.0, rides 124\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 2866, reward 712.0, memory_length 2000, epsilon 0.07566331607082165, time 732.0, rides 146\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 2867, reward 815.0, memory_length 2000, epsilon 0.07559521908635791, time 730.0, rides 143\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 2868, reward 712.0, memory_length 2000, epsilon 0.07552718338918019, time 726.0, rides 134\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 2869, reward 1162.0, memory_length 2000, epsilon 0.07545920892412993, time 725.0, rides 132\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 2870, reward 702.0, memory_length 2000, epsilon 0.07539129563609821, time 730.0, rides 140\n",
      "Initial State is  [2, 10, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2871, reward 795.0, memory_length 2000, epsilon 0.07532344347002572, time 725.0, rides 149\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 2872, reward 1001.0, memory_length 2000, epsilon 0.07525565237090269, time 734.0, rides 136\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 2873, reward 903.0, memory_length 2000, epsilon 0.07518792228376887, time 726.0, rides 141\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 2874, reward 345.0, memory_length 2000, epsilon 0.07512025315371348, time 730.0, rides 131\n",
      "Initial State is  [2, 16, 2]\n",
      "episode 2875, reward 920.0, memory_length 2000, epsilon 0.07505264492587514, time 726.0, rides 142\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 2876, reward 886.0, memory_length 2000, epsilon 0.07498509754544186, time 730.0, rides 131\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 2877, reward 1187.0, memory_length 2000, epsilon 0.07491761095765095, time 722.0, rides 144\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 2878, reward 891.0, memory_length 2000, epsilon 0.07485018510778907, time 729.0, rides 128\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 2879, reward 849.0, memory_length 2000, epsilon 0.07478281994119206, time 723.0, rides 149\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 2880, reward 748.0, memory_length 2000, epsilon 0.07471551540324498, time 722.0, rides 133\n",
      "Initial State is  [3, 22, 0]\n",
      "episode 2881, reward 659.0, memory_length 2000, epsilon 0.07464827143938206, time 728.0, rides 132\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 2882, reward 1122.0, memory_length 2000, epsilon 0.07458108799508661, time 722.0, rides 145\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 2883, reward 1013.0, memory_length 2000, epsilon 0.07451396501589104, time 732.0, rides 141\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 2884, reward 822.0, memory_length 2000, epsilon 0.07444690244737674, time 728.0, rides 135\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 2885, reward 742.0, memory_length 2000, epsilon 0.0743799002351741, time 729.0, rides 133\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 2886, reward 818.0, memory_length 2000, epsilon 0.07431295832496244, time 725.0, rides 135\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 2887, reward 898.0, memory_length 2000, epsilon 0.07424607666246998, time 740.0, rides 136\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 2888, reward 884.0, memory_length 2000, epsilon 0.07417925519347375, time 721.0, rides 133\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 2889, reward 722.0, memory_length 2000, epsilon 0.07411249386379962, time 728.0, rides 132\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 2890, reward 660.0, memory_length 2000, epsilon 0.07404579261932219, time 731.0, rides 138\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 2891, reward 472.0, memory_length 2000, epsilon 0.0739791514059648, time 727.0, rides 138\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 2892, reward 1034.0, memory_length 2000, epsilon 0.07391257016969943, time 726.0, rides 152\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 2893, reward 933.0, memory_length 2000, epsilon 0.0738460488565467, time 725.0, rides 148\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 2894, reward 709.0, memory_length 2000, epsilon 0.07377958741257581, time 726.0, rides 130\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2895, reward 646.0, memory_length 2000, epsilon 0.0737131857839045, time 734.0, rides 130\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 2896, reward 790.0, memory_length 2000, epsilon 0.07364684391669898, time 726.0, rides 124\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 2897, reward 797.0, memory_length 2000, epsilon 0.07358056175717395, time 725.0, rides 117\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 2898, reward 1018.0, memory_length 2000, epsilon 0.0735143392515925, time 725.0, rides 141\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 2899, reward 882.0, memory_length 2000, epsilon 0.07344817634626606, time 723.0, rides 137\n",
      "Initial State is  [0, 8, 5]\n",
      "episode 2900, reward 828.0, memory_length 2000, epsilon 0.07338207298755442, time 728.0, rides 132\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 2901, reward 726.0, memory_length 2000, epsilon 0.07331602912186562, time 737.0, rides 138\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 2902, reward 763.0, memory_length 2000, epsilon 0.07325004469565594, time 733.0, rides 136\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 2903, reward 687.0, memory_length 2000, epsilon 0.07318411965542984, time 731.0, rides 132\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 2904, reward 858.0, memory_length 2000, epsilon 0.07311825394773995, time 722.0, rides 139\n",
      "Initial State is  [2, 22, 5]\n",
      "episode 2905, reward 803.0, memory_length 2000, epsilon 0.07305244751918698, time 729.0, rides 134\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2906, reward 682.0, memory_length 2000, epsilon 0.07298670031641971, time 725.0, rides 127\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 2907, reward 924.0, memory_length 2000, epsilon 0.07292101228613493, time 722.0, rides 149\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 2908, reward 496.0, memory_length 2000, epsilon 0.07285538337507741, time 730.0, rides 131\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 2909, reward 937.0, memory_length 2000, epsilon 0.07278981353003984, time 730.0, rides 147\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 2910, reward 702.0, memory_length 2000, epsilon 0.0727243026978628, time 725.0, rides 137\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 2911, reward 736.0, memory_length 2000, epsilon 0.07265885082543472, time 736.0, rides 136\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 2912, reward 541.0, memory_length 2000, epsilon 0.07259345785969183, time 726.0, rides 138\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 2913, reward 942.0, memory_length 2000, epsilon 0.0725281237476181, time 727.0, rides 146\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 2914, reward 927.0, memory_length 2000, epsilon 0.07246284843624524, time 723.0, rides 144\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 2915, reward 782.0, memory_length 2000, epsilon 0.07239763187265262, time 727.0, rides 128\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 2916, reward 735.0, memory_length 2000, epsilon 0.07233247400396724, time 729.0, rides 143\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 2917, reward 431.0, memory_length 2000, epsilon 0.07226737477736367, time 721.0, rides 132\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 2918, reward 1035.0, memory_length 2000, epsilon 0.07220233414006404, time 729.0, rides 134\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 2919, reward 786.0, memory_length 2000, epsilon 0.07213735203933798, time 729.0, rides 144\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 2920, reward 1163.0, memory_length 2000, epsilon 0.07207242842250257, time 723.0, rides 141\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 2921, reward 623.0, memory_length 2000, epsilon 0.07200756323692231, time 731.0, rides 133\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 2922, reward 1014.0, memory_length 2000, epsilon 0.07194275643000908, time 731.0, rides 132\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 2923, reward 728.0, memory_length 2000, epsilon 0.07187800794922207, time 725.0, rides 137\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 2924, reward 437.0, memory_length 2000, epsilon 0.07181331774206777, time 730.0, rides 125\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 2925, reward 976.0, memory_length 2000, epsilon 0.07174868575609991, time 729.0, rides 142\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 2926, reward 895.0, memory_length 2000, epsilon 0.07168411193891942, time 726.0, rides 132\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2927, reward 860.0, memory_length 2000, epsilon 0.07161959623817439, time 727.0, rides 128\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 2928, reward 691.0, memory_length 2000, epsilon 0.07155513860156003, time 729.0, rides 127\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 2929, reward 864.0, memory_length 2000, epsilon 0.07149073897681862, time 732.0, rides 127\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 2930, reward 803.0, memory_length 2000, epsilon 0.07142639731173948, time 729.0, rides 131\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 2931, reward 948.0, memory_length 2000, epsilon 0.07136211355415892, time 730.0, rides 131\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 2932, reward 1039.0, memory_length 2000, epsilon 0.07129788765196017, time 727.0, rides 140\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 2933, reward 676.0, memory_length 2000, epsilon 0.07123371955307341, time 736.0, rides 130\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 2934, reward 779.0, memory_length 2000, epsilon 0.07116960920547565, time 733.0, rides 149\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 2935, reward 601.0, memory_length 2000, epsilon 0.07110555655719071, time 726.0, rides 122\n",
      "Initial State is  [3, 23, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 2936, reward 745.0, memory_length 2000, epsilon 0.07104156155628924, time 737.0, rides 143\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 2937, reward 996.0, memory_length 2000, epsilon 0.07097762415088858, time 728.0, rides 137\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 2938, reward 806.0, memory_length 2000, epsilon 0.07091374428915279, time 725.0, rides 133\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 2939, reward 954.0, memory_length 2000, epsilon 0.07084992191929254, time 731.0, rides 140\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 2940, reward 847.0, memory_length 2000, epsilon 0.07078615698956518, time 730.0, rides 149\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 2941, reward 1093.0, memory_length 2000, epsilon 0.07072244944827458, time 726.0, rides 140\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 2942, reward 949.0, memory_length 2000, epsilon 0.07065879924377112, time 730.0, rides 154\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 2943, reward 817.0, memory_length 2000, epsilon 0.07059520632445172, time 730.0, rides 160\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 2944, reward 763.0, memory_length 2000, epsilon 0.07053167063875972, time 723.0, rides 139\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 2945, reward 820.0, memory_length 2000, epsilon 0.07046819213518483, time 723.0, rides 133\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 2946, reward 795.0, memory_length 2000, epsilon 0.07040477076226316, time 730.0, rides 140\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 2947, reward 919.0, memory_length 2000, epsilon 0.07034140646857712, time 721.0, rides 149\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 2948, reward 693.0, memory_length 2000, epsilon 0.07027809920275539, time 727.0, rides 136\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 2949, reward 1055.0, memory_length 2000, epsilon 0.07021484891347292, time 730.0, rides 132\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 2950, reward 761.0, memory_length 2000, epsilon 0.0701516555494508, time 721.0, rides 140\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 2951, reward 769.0, memory_length 2000, epsilon 0.07008851905945629, time 728.0, rides 130\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 2952, reward 423.0, memory_length 2000, epsilon 0.07002543939230278, time 735.0, rides 134\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 2953, reward 748.0, memory_length 2000, epsilon 0.06996241649684971, time 736.0, rides 148\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 2954, reward 898.0, memory_length 2000, epsilon 0.06989945032200255, time 737.0, rides 132\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 2955, reward 858.0, memory_length 2000, epsilon 0.06983654081671274, time 728.0, rides 132\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 2956, reward 522.0, memory_length 2000, epsilon 0.06977368792997769, time 724.0, rides 134\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 2957, reward 723.0, memory_length 2000, epsilon 0.06971089161084071, time 733.0, rides 142\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 2958, reward 1004.0, memory_length 2000, epsilon 0.06964815180839096, time 730.0, rides 132\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 2959, reward 612.0, memory_length 2000, epsilon 0.0695854684717634, time 735.0, rides 138\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 2960, reward 754.0, memory_length 2000, epsilon 0.06952284155013881, time 724.0, rides 134\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 2961, reward 938.0, memory_length 2000, epsilon 0.06946027099274368, time 729.0, rides 136\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 2962, reward 739.0, memory_length 2000, epsilon 0.06939775674885021, time 725.0, rides 135\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 2963, reward 590.0, memory_length 2000, epsilon 0.06933529876777625, time 732.0, rides 140\n",
      "Initial State is  [1, 8, 5]\n",
      "episode 2964, reward 677.0, memory_length 2000, epsilon 0.06927289699888525, time 732.0, rides 155\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 2965, reward 1043.0, memory_length 2000, epsilon 0.06921055139158626, time 729.0, rides 128\n",
      "Initial State is  [1, 16, 1]\n",
      "episode 2966, reward 908.0, memory_length 2000, epsilon 0.06914826189533382, time 733.0, rides 129\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 2967, reward 697.0, memory_length 2000, epsilon 0.06908602845962802, time 724.0, rides 142\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 2968, reward 635.0, memory_length 2000, epsilon 0.06902385103401436, time 727.0, rides 139\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 2969, reward 590.0, memory_length 2000, epsilon 0.06896172956808375, time 730.0, rides 136\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 2970, reward 747.0, memory_length 2000, epsilon 0.06889966401147248, time 729.0, rides 132\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 2971, reward 736.0, memory_length 2000, epsilon 0.06883765431386214, time 728.0, rides 140\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 2972, reward 1084.0, memory_length 2000, epsilon 0.06877570042497967, time 734.0, rides 149\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 2973, reward 882.0, memory_length 2000, epsilon 0.06871380229459718, time 724.0, rides 130\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 2974, reward 766.0, memory_length 2000, epsilon 0.06865195987253205, time 725.0, rides 154\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 2975, reward 1060.0, memory_length 2000, epsilon 0.06859017310864676, time 727.0, rides 129\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 2976, reward 638.0, memory_length 2000, epsilon 0.06852844195284898, time 727.0, rides 141\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 2977, reward 895.0, memory_length 2000, epsilon 0.06846676635509141, time 720.0, rides 146\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 2978, reward 839.0, memory_length 2000, epsilon 0.06840514626537184, time 731.0, rides 134\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 2979, reward 836.0, memory_length 2000, epsilon 0.068343581633733, time 724.0, rides 142\n",
      "Initial State is  [1, 22, 5]\n",
      "episode 2980, reward 543.0, memory_length 2000, epsilon 0.06828207241026264, time 727.0, rides 131\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 2981, reward 602.0, memory_length 2000, epsilon 0.06822061854509341, time 744.0, rides 134\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 2982, reward 553.0, memory_length 2000, epsilon 0.06815921998840282, time 729.0, rides 139\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 2983, reward 948.0, memory_length 2000, epsilon 0.06809787669041326, time 726.0, rides 146\n",
      "Initial State is  [2, 20, 0]\n",
      "episode 2984, reward 820.0, memory_length 2000, epsilon 0.06803658860139189, time 741.0, rides 141\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 2985, reward 545.0, memory_length 2000, epsilon 0.06797535567165064, time 731.0, rides 135\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 2986, reward 676.0, memory_length 2000, epsilon 0.06791417785154616, time 730.0, rides 152\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 2987, reward 843.0, memory_length 2000, epsilon 0.06785305509147976, time 725.0, rides 137\n",
      "Initial State is  [1, 2, 4]\n",
      "episode 2988, reward 887.0, memory_length 2000, epsilon 0.06779198734189743, time 724.0, rides 119\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 2989, reward 1204.0, memory_length 2000, epsilon 0.06773097455328972, time 723.0, rides 129\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 2990, reward 1282.0, memory_length 2000, epsilon 0.06767001667619175, time 727.0, rides 137\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 2991, reward 773.0, memory_length 2000, epsilon 0.06760911366118318, time 732.0, rides 136\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 2992, reward 857.0, memory_length 2000, epsilon 0.0675482654588881, time 726.0, rides 139\n",
      "Initial State is  [0, 14, 4]\n",
      "episode 2993, reward 587.0, memory_length 2000, epsilon 0.0674874720199751, time 739.0, rides 152\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 2994, reward 596.0, memory_length 2000, epsilon 0.06742673329515712, time 726.0, rides 119\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 2995, reward 847.0, memory_length 2000, epsilon 0.06736604923519147, time 727.0, rides 130\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 2996, reward 833.0, memory_length 2000, epsilon 0.0673054197908798, time 730.0, rides 153\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 2997, reward 792.0, memory_length 2000, epsilon 0.06724484491306801, time 732.0, rides 125\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 2998, reward 866.0, memory_length 2000, epsilon 0.06718432455264625, time 725.0, rides 138\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 2999, reward 853.0, memory_length 2000, epsilon 0.06712385866054887, time 737.0, rides 125\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 3000, reward 1101.0, memory_length 2000, epsilon 0.06706344718775438, time 727.0, rides 136\n",
      "Initial State is  [0, 10, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3001, reward 735.0, memory_length 2000, epsilon 0.0670030900852854, time 725.0, rides 129\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 3002, reward 994.0, memory_length 2000, epsilon 0.06694278730420865, time 727.0, rides 131\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 3003, reward 809.0, memory_length 2000, epsilon 0.06688253879563485, time 731.0, rides 133\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 3004, reward 817.0, memory_length 2000, epsilon 0.06682234451071878, time 726.0, rides 135\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 3005, reward 586.0, memory_length 2000, epsilon 0.06676220440065914, time 729.0, rides 137\n",
      "Initial State is  [4, 1, 4]\n",
      "episode 3006, reward 1118.0, memory_length 2000, epsilon 0.06670211841669854, time 727.0, rides 147\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 3007, reward 748.0, memory_length 2000, epsilon 0.06664208651012352, time 726.0, rides 136\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 3008, reward 1053.0, memory_length 2000, epsilon 0.06658210863226441, time 727.0, rides 141\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 3009, reward 721.0, memory_length 2000, epsilon 0.06652218473449537, time 724.0, rides 128\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 3010, reward 910.0, memory_length 2000, epsilon 0.06646231476823432, time 724.0, rides 142\n",
      "Initial State is  [2, 5, 3]\n",
      "episode 3011, reward 936.0, memory_length 2000, epsilon 0.06640249868494291, time 728.0, rides 124\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 3012, reward 553.0, memory_length 2000, epsilon 0.06634273643612647, time 741.0, rides 141\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 3013, reward 727.0, memory_length 2000, epsilon 0.06628302797333395, time 723.0, rides 133\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 3014, reward 1028.0, memory_length 2000, epsilon 0.06622337324815795, time 726.0, rides 145\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 3015, reward 614.0, memory_length 2000, epsilon 0.06616377221223461, time 727.0, rides 137\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 3016, reward 919.0, memory_length 2000, epsilon 0.06610422481724361, time 738.0, rides 128\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 3017, reward 827.0, memory_length 2000, epsilon 0.06604473101490808, time 723.0, rides 133\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3018, reward 556.0, memory_length 2000, epsilon 0.06598529075699466, time 729.0, rides 119\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3019, reward 838.0, memory_length 2000, epsilon 0.06592590399531337, time 730.0, rides 123\n",
      "Initial State is  [0, 9, 0]\n",
      "episode 3020, reward 1040.0, memory_length 2000, epsilon 0.06586657068171758, time 725.0, rides 124\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 3021, reward 680.0, memory_length 2000, epsilon 0.06580729076810404, time 728.0, rides 130\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 3022, reward 486.0, memory_length 2000, epsilon 0.06574806420641274, time 740.0, rides 130\n",
      "Initial State is  [1, 16, 0]\n",
      "episode 3023, reward 695.0, memory_length 2000, epsilon 0.06568889094862697, time 724.0, rides 142\n",
      "Initial State is  [2, 17, 5]\n",
      "episode 3024, reward 974.0, memory_length 2000, epsilon 0.0656297709467732, time 722.0, rides 139\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 3025, reward 713.0, memory_length 2000, epsilon 0.06557070415292111, time 732.0, rides 138\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 3026, reward 1002.0, memory_length 2000, epsilon 0.06551169051918349, time 729.0, rides 140\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 3027, reward 814.0, memory_length 2000, epsilon 0.06545272999771622, time 723.0, rides 138\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 3028, reward 977.0, memory_length 2000, epsilon 0.06539382254071827, time 730.0, rides 135\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 3029, reward 820.0, memory_length 2000, epsilon 0.06533496810043161, time 724.0, rides 132\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3030, reward 1017.0, memory_length 2000, epsilon 0.06527616662914122, time 730.0, rides 138\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 3031, reward 762.0, memory_length 2000, epsilon 0.065217418079175, time 721.0, rides 137\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 3032, reward 906.0, memory_length 2000, epsilon 0.06515872240290374, time 737.0, rides 142\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 3033, reward 909.0, memory_length 2000, epsilon 0.06510007955274112, time 740.0, rides 150\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 3034, reward 690.0, memory_length 2000, epsilon 0.06504148948114366, time 726.0, rides 135\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 3035, reward 749.0, memory_length 2000, epsilon 0.06498295214061063, time 726.0, rides 130\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 3036, reward 759.0, memory_length 2000, epsilon 0.06492446748368408, time 724.0, rides 140\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 3037, reward 670.0, memory_length 2000, epsilon 0.06486603546294877, time 733.0, rides 128\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 3038, reward 791.0, memory_length 2000, epsilon 0.06480765603103211, time 721.0, rides 129\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 3039, reward 938.0, memory_length 2000, epsilon 0.06474932914060419, time 727.0, rides 137\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 3040, reward 645.0, memory_length 2000, epsilon 0.06469105474437764, time 730.0, rides 128\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 3041, reward 1010.0, memory_length 2000, epsilon 0.0646328327951077, time 734.0, rides 125\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 3042, reward 827.0, memory_length 2000, epsilon 0.0645746632455921, time 726.0, rides 141\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 3043, reward 987.0, memory_length 2000, epsilon 0.06451654604867108, time 732.0, rides 156\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 3044, reward 733.0, memory_length 2000, epsilon 0.06445848115722727, time 725.0, rides 138\n",
      "Initial State is  [0, 11, 0]\n",
      "episode 3045, reward 814.0, memory_length 2000, epsilon 0.06440046852418577, time 724.0, rides 127\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 3046, reward 973.0, memory_length 2000, epsilon 0.064342508102514, time 735.0, rides 127\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 3047, reward 650.0, memory_length 2000, epsilon 0.06428459984522174, time 727.0, rides 139\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3048, reward 765.0, memory_length 2000, epsilon 0.06422674370536104, time 727.0, rides 136\n",
      "Initial State is  [4, 8, 5]\n",
      "episode 3049, reward 937.0, memory_length 2000, epsilon 0.06416893963602621, time 735.0, rides 143\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 3050, reward 807.0, memory_length 2000, epsilon 0.0641111875903538, time 734.0, rides 144\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 3051, reward 658.0, memory_length 2000, epsilon 0.06405348752152247, time 729.0, rides 137\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 3052, reward 799.0, memory_length 2000, epsilon 0.0639958393827531, time 729.0, rides 130\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 3053, reward 895.0, memory_length 2000, epsilon 0.06393824312730863, time 727.0, rides 132\n",
      "Initial State is  [2, 3, 4]\n",
      "episode 3054, reward 710.0, memory_length 2000, epsilon 0.06388069870849405, time 722.0, rides 128\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 3055, reward 954.0, memory_length 2000, epsilon 0.06382320607965641, time 728.0, rides 132\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 3056, reward 656.0, memory_length 2000, epsilon 0.06376576519418473, time 727.0, rides 140\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 3057, reward 901.0, memory_length 2000, epsilon 0.06370837600550996, time 729.0, rides 145\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 3058, reward 804.0, memory_length 2000, epsilon 0.063651038467105, time 724.0, rides 137\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 3059, reward 718.0, memory_length 2000, epsilon 0.0635937525324846, time 730.0, rides 135\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 3060, reward 861.0, memory_length 2000, epsilon 0.06353651815520536, time 731.0, rides 137\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 3061, reward 669.0, memory_length 2000, epsilon 0.06347933528886568, time 728.0, rides 131\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 3062, reward 900.0, memory_length 2000, epsilon 0.0634222038871057, time 740.0, rides 131\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3063, reward 1038.0, memory_length 2000, epsilon 0.06336512390360731, time 727.0, rides 149\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 3064, reward 552.0, memory_length 2000, epsilon 0.06330809529209407, time 724.0, rides 136\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3065, reward 858.0, memory_length 2000, epsilon 0.06325111800633118, time 724.0, rides 129\n",
      "Initial State is  [1, 7, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3066, reward 939.0, memory_length 2000, epsilon 0.06319419200012548, time 725.0, rides 131\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 3067, reward 965.0, memory_length 2000, epsilon 0.06313731722732537, time 725.0, rides 147\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 3068, reward 876.0, memory_length 2000, epsilon 0.06308049364182078, time 734.0, rides 143\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 3069, reward 877.0, memory_length 2000, epsilon 0.06302372119754314, time 729.0, rides 131\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3070, reward 1226.0, memory_length 2000, epsilon 0.06296699984846535, time 731.0, rides 135\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 3071, reward 787.0, memory_length 2000, epsilon 0.06291032954860173, time 730.0, rides 142\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3072, reward 915.0, memory_length 2000, epsilon 0.06285371025200799, time 732.0, rides 137\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 3073, reward 818.0, memory_length 2000, epsilon 0.06279714191278118, time 730.0, rides 137\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 3074, reward 740.0, memory_length 2000, epsilon 0.06274062448505968, time 728.0, rides 148\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 3075, reward 578.0, memory_length 2000, epsilon 0.06268415792302312, time 728.0, rides 137\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 3076, reward 702.0, memory_length 2000, epsilon 0.0626277421808924, time 730.0, rides 138\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 3077, reward 543.0, memory_length 2000, epsilon 0.06257137721292959, time 736.0, rides 122\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 3078, reward 884.0, memory_length 2000, epsilon 0.06251506297343795, time 736.0, rides 146\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 3079, reward 1027.0, memory_length 2000, epsilon 0.06245879941676186, time 727.0, rides 147\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3080, reward 613.0, memory_length 2000, epsilon 0.06240258649728677, time 724.0, rides 139\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 3081, reward 974.0, memory_length 2000, epsilon 0.06234642416943921, time 738.0, rides 140\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 3082, reward 698.0, memory_length 2000, epsilon 0.062290312387686717, time 735.0, rides 125\n",
      "Initial State is  [2, 11, 0]\n",
      "episode 3083, reward 634.0, memory_length 2000, epsilon 0.0622342511065378, time 731.0, rides 151\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 3084, reward 933.0, memory_length 2000, epsilon 0.06217824028054191, time 732.0, rides 134\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 3085, reward 702.0, memory_length 2000, epsilon 0.06212227986428942, time 722.0, rides 139\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 3086, reward 802.0, memory_length 2000, epsilon 0.06206636981241156, time 726.0, rides 143\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 3087, reward 608.0, memory_length 2000, epsilon 0.06201051007958039, time 730.0, rides 139\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 3088, reward 694.0, memory_length 2000, epsilon 0.061954700620508764, time 724.0, rides 142\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 3089, reward 810.0, memory_length 2000, epsilon 0.06189894138995031, time 728.0, rides 137\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 3090, reward 655.0, memory_length 2000, epsilon 0.06184323234269935, time 730.0, rides 138\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 3091, reward 909.0, memory_length 2000, epsilon 0.061787573433590925, time 732.0, rides 151\n",
      "Initial State is  [3, 0, 3]\n",
      "episode 3092, reward 842.0, memory_length 2000, epsilon 0.06173196461750069, time 730.0, rides 136\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 3093, reward 715.0, memory_length 2000, epsilon 0.06167640584934494, time 732.0, rides 138\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 3094, reward 703.0, memory_length 2000, epsilon 0.06162089708408053, time 733.0, rides 147\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 3095, reward 576.0, memory_length 2000, epsilon 0.061565438276704854, time 729.0, rides 137\n",
      "Initial State is  [4, 19, 1]\n",
      "episode 3096, reward 633.0, memory_length 2000, epsilon 0.06151002938225582, time 726.0, rides 137\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 3097, reward 833.0, memory_length 2000, epsilon 0.061454670355811786, time 726.0, rides 155\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 3098, reward 667.0, memory_length 2000, epsilon 0.061399361152491554, time 731.0, rides 136\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 3099, reward 802.0, memory_length 2000, epsilon 0.06134410172745431, time 732.0, rides 141\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 3100, reward 1335.0, memory_length 2000, epsilon 0.0612888920358996, time 722.0, rides 137\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 3101, reward 601.0, memory_length 2000, epsilon 0.06123373203306729, time 725.0, rides 132\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 3102, reward 850.0, memory_length 2000, epsilon 0.06117862167423753, time 731.0, rides 137\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 3103, reward 928.0, memory_length 2000, epsilon 0.061123560914730715, time 734.0, rides 134\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 3104, reward 752.0, memory_length 2000, epsilon 0.06106854970990746, time 726.0, rides 157\n",
      "Initial State is  [3, 16, 2]\n",
      "episode 3105, reward 1049.0, memory_length 2000, epsilon 0.06101358801516854, time 728.0, rides 138\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 3106, reward 715.0, memory_length 2000, epsilon 0.06095867578595489, time 732.0, rides 139\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 3107, reward 924.0, memory_length 2000, epsilon 0.06090381297774753, time 727.0, rides 146\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 3108, reward 676.0, memory_length 2000, epsilon 0.06084899954606756, time 725.0, rides 135\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 3109, reward 1139.0, memory_length 2000, epsilon 0.0607942354464761, time 729.0, rides 130\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 3110, reward 543.0, memory_length 2000, epsilon 0.06073952063457427, time 726.0, rides 142\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 3111, reward 688.0, memory_length 2000, epsilon 0.06068485506600316, time 727.0, rides 126\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 3112, reward 671.0, memory_length 2000, epsilon 0.06063023869644375, time 728.0, rides 130\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 3113, reward 726.0, memory_length 2000, epsilon 0.06057567148161695, time 723.0, rides 146\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 3114, reward 728.0, memory_length 2000, epsilon 0.0605211533772835, time 724.0, rides 130\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 3115, reward 896.0, memory_length 2000, epsilon 0.06046668433924394, time 724.0, rides 152\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 3116, reward 857.0, memory_length 2000, epsilon 0.06041226432333862, time 729.0, rides 135\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 3117, reward 1141.0, memory_length 2000, epsilon 0.06035789328544761, time 732.0, rides 137\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 3118, reward 1028.0, memory_length 2000, epsilon 0.06030357118149071, time 737.0, rides 128\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3119, reward 751.0, memory_length 2000, epsilon 0.06024929796742737, time 725.0, rides 141\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 3120, reward 1209.0, memory_length 2000, epsilon 0.06019507359925668, time 727.0, rides 150\n",
      "Initial State is  [4, 21, 1]\n",
      "episode 3121, reward 837.0, memory_length 2000, epsilon 0.06014089803301735, time 725.0, rides 142\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 3122, reward 1036.0, memory_length 2000, epsilon 0.06008677122478764, time 725.0, rides 129\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 3123, reward 798.0, memory_length 2000, epsilon 0.06003269313068533, time 729.0, rides 135\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 3124, reward 960.0, memory_length 2000, epsilon 0.05997866370686771, time 729.0, rides 133\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 3125, reward 529.0, memory_length 2000, epsilon 0.05992468290953153, time 724.0, rides 137\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 3126, reward 680.0, memory_length 2000, epsilon 0.059870750694912954, time 733.0, rides 140\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 3127, reward 787.0, memory_length 2000, epsilon 0.05981686701928753, time 729.0, rides 147\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 3128, reward 878.0, memory_length 2000, epsilon 0.05976303183897017, time 721.0, rides 130\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 3129, reward 926.0, memory_length 2000, epsilon 0.0597092451103151, time 722.0, rides 139\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 3130, reward 747.0, memory_length 2000, epsilon 0.05965550678971582, time 729.0, rides 141\n",
      "Initial State is  [0, 22, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3131, reward 809.0, memory_length 2000, epsilon 0.059601816833605076, time 722.0, rides 142\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 3132, reward 646.0, memory_length 2000, epsilon 0.05954817519845483, time 728.0, rides 146\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 3133, reward 668.0, memory_length 2000, epsilon 0.05949458184077622, time 722.0, rides 135\n",
      "Initial State is  [2, 9, 0]\n",
      "episode 3134, reward 879.0, memory_length 2000, epsilon 0.05944103671711952, time 727.0, rides 136\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 3135, reward 528.0, memory_length 2000, epsilon 0.059387539784074114, time 725.0, rides 134\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 3136, reward 707.0, memory_length 2000, epsilon 0.059334090998268446, time 728.0, rides 137\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 3137, reward 776.0, memory_length 2000, epsilon 0.059280690316370004, time 729.0, rides 137\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 3138, reward 1048.0, memory_length 2000, epsilon 0.05922733769508527, time 725.0, rides 142\n",
      "Initial State is  [1, 10, 5]\n",
      "episode 3139, reward 780.0, memory_length 2000, epsilon 0.05917403309115969, time 728.0, rides 134\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 3140, reward 619.0, memory_length 2000, epsilon 0.059120776461377644, time 724.0, rides 139\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 3141, reward 537.0, memory_length 2000, epsilon 0.059067567762562403, time 725.0, rides 124\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 3142, reward 926.0, memory_length 2000, epsilon 0.059014406951576094, time 723.0, rides 142\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 3143, reward 797.0, memory_length 2000, epsilon 0.05896129398531968, time 731.0, rides 127\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 3144, reward 779.0, memory_length 2000, epsilon 0.05890822882073289, time 734.0, rides 148\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3145, reward 799.0, memory_length 2000, epsilon 0.05885521141479423, time 736.0, rides 131\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 3146, reward 998.0, memory_length 2000, epsilon 0.058802241724520914, time 730.0, rides 131\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 3147, reward 961.0, memory_length 2000, epsilon 0.058749319706968846, time 735.0, rides 127\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 3148, reward 868.0, memory_length 2000, epsilon 0.05869644531923257, time 728.0, rides 150\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 3149, reward 1084.0, memory_length 2000, epsilon 0.05864361851844526, time 720.0, rides 140\n",
      "Initial State is  [3, 13, 0]\n",
      "episode 3150, reward 844.0, memory_length 2000, epsilon 0.05859083926177866, time 734.0, rides 134\n",
      "Initial State is  [0, 16, 5]\n",
      "episode 3151, reward 668.0, memory_length 2000, epsilon 0.05853810750644306, time 734.0, rides 154\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 3152, reward 756.0, memory_length 2000, epsilon 0.05848542320968726, time 724.0, rides 130\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 3153, reward 627.0, memory_length 2000, epsilon 0.058432786328798544, time 725.0, rides 128\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 3154, reward 741.0, memory_length 2000, epsilon 0.058380196821102626, time 724.0, rides 142\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 3155, reward 629.0, memory_length 2000, epsilon 0.05832765464396363, time 728.0, rides 129\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 3156, reward 775.0, memory_length 2000, epsilon 0.05827515975478406, time 728.0, rides 135\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 3157, reward 812.0, memory_length 2000, epsilon 0.05822271211100476, time 722.0, rides 132\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 3158, reward 694.0, memory_length 2000, epsilon 0.05817031167010485, time 729.0, rides 133\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 3159, reward 880.0, memory_length 2000, epsilon 0.05811795838960175, time 726.0, rides 137\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 3160, reward 860.0, memory_length 2000, epsilon 0.05806565222705111, time 725.0, rides 128\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 3161, reward 813.0, memory_length 2000, epsilon 0.05801339314004676, time 726.0, rides 140\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 3162, reward 889.0, memory_length 2000, epsilon 0.05796118108622072, time 736.0, rides 136\n",
      "Initial State is  [1, 5, 5]\n",
      "episode 3163, reward 794.0, memory_length 2000, epsilon 0.057909016023243116, time 732.0, rides 134\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3164, reward 820.0, memory_length 2000, epsilon 0.057856897908822195, time 735.0, rides 139\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 3165, reward 371.0, memory_length 2000, epsilon 0.057804826700704255, time 724.0, rides 125\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 3166, reward 537.0, memory_length 2000, epsilon 0.05775280235667362, time 724.0, rides 139\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 3167, reward 777.0, memory_length 2000, epsilon 0.05770082483455261, time 730.0, rides 135\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 3168, reward 744.0, memory_length 2000, epsilon 0.057648894092201516, time 731.0, rides 135\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 3169, reward 680.0, memory_length 2000, epsilon 0.05759701008751853, time 729.0, rides 127\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 3170, reward 871.0, memory_length 2000, epsilon 0.05754517277843976, time 732.0, rides 148\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 3171, reward 659.0, memory_length 2000, epsilon 0.05749338212293917, time 726.0, rides 133\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 3172, reward 805.0, memory_length 2000, epsilon 0.05744163807902852, time 736.0, rides 140\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 3173, reward 513.0, memory_length 2000, epsilon 0.0573899406047574, time 723.0, rides 146\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3174, reward 403.0, memory_length 2000, epsilon 0.05733828965821312, time 735.0, rides 133\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 3175, reward 901.0, memory_length 2000, epsilon 0.057286685197520726, time 726.0, rides 130\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 3176, reward 957.0, memory_length 2000, epsilon 0.057235127180842955, time 724.0, rides 124\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 3177, reward 1067.0, memory_length 2000, epsilon 0.05718361556638019, time 728.0, rides 134\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 3178, reward 843.0, memory_length 2000, epsilon 0.05713215031237045, time 725.0, rides 131\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 3179, reward 1001.0, memory_length 2000, epsilon 0.057080731377089314, time 729.0, rides 133\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 3180, reward 674.0, memory_length 2000, epsilon 0.05702935871884993, time 733.0, rides 138\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 3181, reward 973.0, memory_length 2000, epsilon 0.05697803229600297, time 729.0, rides 130\n",
      "Initial State is  [0, 19, 5]\n",
      "episode 3182, reward 794.0, memory_length 2000, epsilon 0.056926752066936565, time 728.0, rides 132\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 3183, reward 794.0, memory_length 2000, epsilon 0.05687551799007632, time 731.0, rides 147\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 3184, reward 762.0, memory_length 2000, epsilon 0.05682433002388525, time 723.0, rides 137\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 3185, reward 1174.0, memory_length 2000, epsilon 0.05677318812686375, time 722.0, rides 134\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 3186, reward 848.0, memory_length 2000, epsilon 0.056722092257549574, time 724.0, rides 135\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 3187, reward 642.0, memory_length 2000, epsilon 0.056671042374517776, time 736.0, rides 142\n",
      "Initial State is  [3, 22, 1]\n",
      "episode 3188, reward 660.0, memory_length 2000, epsilon 0.05662003843638071, time 731.0, rides 148\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 3189, reward 743.0, memory_length 2000, epsilon 0.056569080401787965, time 723.0, rides 133\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 3190, reward 554.0, memory_length 2000, epsilon 0.056518168229426353, time 724.0, rides 151\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 3191, reward 903.0, memory_length 2000, epsilon 0.05646730187801987, time 724.0, rides 138\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 3192, reward 524.0, memory_length 2000, epsilon 0.056416481306329654, time 726.0, rides 139\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 3193, reward 876.0, memory_length 2000, epsilon 0.056365706473153955, time 733.0, rides 142\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 3194, reward 815.0, memory_length 2000, epsilon 0.05631497733732812, time 725.0, rides 134\n",
      "Initial State is  [4, 6, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3195, reward 895.0, memory_length 2000, epsilon 0.05626429385772452, time 728.0, rides 147\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 3196, reward 747.0, memory_length 2000, epsilon 0.05621365599325257, time 724.0, rides 135\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 3197, reward 1039.0, memory_length 2000, epsilon 0.056163063702858645, time 727.0, rides 130\n",
      "Initial State is  [3, 2, 6]\n",
      "episode 3198, reward 767.0, memory_length 2000, epsilon 0.056112516945526075, time 730.0, rides 137\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 3199, reward 795.0, memory_length 2000, epsilon 0.0560620156802751, time 736.0, rides 130\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 3200, reward 793.0, memory_length 2000, epsilon 0.05601155986616285, time 728.0, rides 133\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 3201, reward 782.0, memory_length 2000, epsilon 0.055961149462283304, time 725.0, rides 149\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 3202, reward 810.0, memory_length 2000, epsilon 0.05591078442776725, time 738.0, rides 135\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 3203, reward 1035.0, memory_length 2000, epsilon 0.05586046472178226, time 724.0, rides 144\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 3204, reward 866.0, memory_length 2000, epsilon 0.05581019030353265, time 727.0, rides 147\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 3205, reward 910.0, memory_length 2000, epsilon 0.05575996113225947, time 726.0, rides 128\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 3206, reward 924.0, memory_length 2000, epsilon 0.05570977716724044, time 731.0, rides 142\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 3207, reward 956.0, memory_length 2000, epsilon 0.05565963836778992, time 735.0, rides 144\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 3208, reward 1024.0, memory_length 2000, epsilon 0.05560954469325891, time 728.0, rides 127\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 3209, reward 694.0, memory_length 2000, epsilon 0.055559496103034976, time 729.0, rides 130\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 3210, reward 628.0, memory_length 2000, epsilon 0.05550949255654224, time 728.0, rides 142\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 3211, reward 797.0, memory_length 2000, epsilon 0.05545953401324136, time 722.0, rides 131\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 3212, reward 842.0, memory_length 2000, epsilon 0.05540962043262944, time 733.0, rides 142\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 3213, reward 889.0, memory_length 2000, epsilon 0.055359751774240074, time 735.0, rides 130\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 3214, reward 1050.0, memory_length 2000, epsilon 0.055309927997643255, time 727.0, rides 137\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 3215, reward 931.0, memory_length 2000, epsilon 0.05526014906244538, time 726.0, rides 143\n",
      "Initial State is  [1, 8, 1]\n",
      "episode 3216, reward 540.0, memory_length 2000, epsilon 0.055210414928289174, time 727.0, rides 116\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 3217, reward 780.0, memory_length 2000, epsilon 0.05516072555485371, time 728.0, rides 148\n",
      "Initial State is  [2, 15, 1]\n",
      "episode 3218, reward 608.0, memory_length 2000, epsilon 0.05511108090185434, time 724.0, rides 138\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 3219, reward 947.0, memory_length 2000, epsilon 0.05506148092904267, time 731.0, rides 146\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 3220, reward 854.0, memory_length 2000, epsilon 0.05501192559620653, time 728.0, rides 141\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 3221, reward 670.0, memory_length 2000, epsilon 0.054962414863169946, time 730.0, rides 126\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 3222, reward 866.0, memory_length 2000, epsilon 0.054912948689793094, time 727.0, rides 144\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 3223, reward 849.0, memory_length 2000, epsilon 0.054863527035972276, time 726.0, rides 142\n",
      "Initial State is  [2, 6, 5]\n",
      "episode 3224, reward 820.0, memory_length 2000, epsilon 0.0548141498616399, time 731.0, rides 128\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 3225, reward 784.0, memory_length 2000, epsilon 0.05476481712676442, time 729.0, rides 139\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 3226, reward 1075.0, memory_length 2000, epsilon 0.05471552879135033, time 727.0, rides 134\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 3227, reward 703.0, memory_length 2000, epsilon 0.054666284815438115, time 727.0, rides 128\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 3228, reward 839.0, memory_length 2000, epsilon 0.05461708515910422, time 727.0, rides 133\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 3229, reward 455.0, memory_length 2000, epsilon 0.05456792978246102, time 730.0, rides 137\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 3230, reward 608.0, memory_length 2000, epsilon 0.0545188186456568, time 729.0, rides 123\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 3231, reward 788.0, memory_length 2000, epsilon 0.05446975170887571, time 729.0, rides 143\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 3232, reward 444.0, memory_length 2000, epsilon 0.05442072893233772, time 731.0, rides 125\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 3233, reward 616.0, memory_length 2000, epsilon 0.05437175027629862, time 723.0, rides 129\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 3234, reward 412.0, memory_length 2000, epsilon 0.05432281570104995, time 724.0, rides 139\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 3235, reward 547.0, memory_length 2000, epsilon 0.05427392516691901, time 730.0, rides 124\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 3236, reward 1021.0, memory_length 2000, epsilon 0.05422507863426878, time 724.0, rides 128\n",
      "Initial State is  [0, 18, 0]\n",
      "episode 3237, reward 838.0, memory_length 2000, epsilon 0.05417627606349794, time 725.0, rides 158\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 3238, reward 787.0, memory_length 2000, epsilon 0.054127517415040786, time 735.0, rides 129\n",
      "Initial State is  [1, 17, 0]\n",
      "episode 3239, reward 1110.0, memory_length 2000, epsilon 0.05407880264936725, time 723.0, rides 146\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 3240, reward 562.0, memory_length 2000, epsilon 0.054030131726982816, time 727.0, rides 124\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 3241, reward 935.0, memory_length 2000, epsilon 0.05398150460842853, time 738.0, rides 132\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 3242, reward 736.0, memory_length 2000, epsilon 0.053932921254280945, time 726.0, rides 130\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3243, reward 796.0, memory_length 2000, epsilon 0.05388438162515209, time 727.0, rides 136\n",
      "Initial State is  [2, 8, 0]\n",
      "episode 3244, reward 702.0, memory_length 2000, epsilon 0.053835885681689455, time 733.0, rides 137\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 3245, reward 760.0, memory_length 2000, epsilon 0.053787433384575936, time 734.0, rides 141\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 3246, reward 1114.0, memory_length 2000, epsilon 0.05373902469452982, time 728.0, rides 143\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 3247, reward 524.0, memory_length 2000, epsilon 0.05369065957230474, time 726.0, rides 127\n",
      "Initial State is  [3, 1, 2]\n",
      "episode 3248, reward 904.0, memory_length 2000, epsilon 0.05364233797868966, time 725.0, rides 135\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 3249, reward 988.0, memory_length 2000, epsilon 0.05359405987450884, time 730.0, rides 132\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 3250, reward 634.0, memory_length 2000, epsilon 0.05354582522062178, time 731.0, rides 138\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 3251, reward 776.0, memory_length 2000, epsilon 0.053497633977923224, time 734.0, rides 142\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 3252, reward 886.0, memory_length 2000, epsilon 0.05344948610734309, time 728.0, rides 132\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3253, reward 961.0, memory_length 2000, epsilon 0.05340138156984648, time 723.0, rides 139\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 3254, reward 690.0, memory_length 2000, epsilon 0.05335332032643362, time 730.0, rides 142\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 3255, reward 789.0, memory_length 2000, epsilon 0.05330530233813983, time 727.0, rides 125\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 3256, reward 862.0, memory_length 2000, epsilon 0.0532573275660355, time 729.0, rides 133\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 3257, reward 454.0, memory_length 2000, epsilon 0.05320939597122607, time 725.0, rides 130\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3258, reward 643.0, memory_length 2000, epsilon 0.05316150751485197, time 721.0, rides 133\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 3259, reward 306.0, memory_length 2000, epsilon 0.053113662158088604, time 725.0, rides 134\n",
      "Initial State is  [1, 15, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3260, reward 781.0, memory_length 2000, epsilon 0.053065859862146326, time 729.0, rides 142\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3261, reward 883.0, memory_length 2000, epsilon 0.053018100588270396, time 729.0, rides 136\n",
      "Initial State is  [4, 20, 5]\n",
      "episode 3262, reward 660.0, memory_length 2000, epsilon 0.05297038429774095, time 729.0, rides 136\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 3263, reward 762.0, memory_length 2000, epsilon 0.05292271095187299, time 726.0, rides 144\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 3264, reward 883.0, memory_length 2000, epsilon 0.0528750805120163, time 732.0, rides 133\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 3265, reward 908.0, memory_length 2000, epsilon 0.052827492939555486, time 725.0, rides 137\n",
      "Initial State is  [2, 11, 5]\n",
      "episode 3266, reward 754.0, memory_length 2000, epsilon 0.052779948195909886, time 727.0, rides 135\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3267, reward 1036.0, memory_length 2000, epsilon 0.052732446242533565, time 737.0, rides 133\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 3268, reward 793.0, memory_length 2000, epsilon 0.05268498704091528, time 726.0, rides 141\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 3269, reward 813.0, memory_length 2000, epsilon 0.05263757055257846, time 727.0, rides 145\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 3270, reward 599.0, memory_length 2000, epsilon 0.052590196739081135, time 728.0, rides 129\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 3271, reward 662.0, memory_length 2000, epsilon 0.05254286556201596, time 730.0, rides 141\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 3272, reward 853.0, memory_length 2000, epsilon 0.05249557698301015, time 724.0, rides 137\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 3273, reward 854.0, memory_length 2000, epsilon 0.05244833096372544, time 730.0, rides 123\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 3274, reward 623.0, memory_length 2000, epsilon 0.05240112746585809, time 725.0, rides 149\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 3275, reward 729.0, memory_length 2000, epsilon 0.052353966451138816, time 723.0, rides 129\n",
      "Initial State is  [4, 22, 3]\n",
      "episode 3276, reward 820.0, memory_length 2000, epsilon 0.05230684788133279, time 732.0, rides 121\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 3277, reward 539.0, memory_length 2000, epsilon 0.05225977171823959, time 730.0, rides 131\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 3278, reward 814.0, memory_length 2000, epsilon 0.05221273792369317, time 735.0, rides 128\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 3279, reward 934.0, memory_length 2000, epsilon 0.052165746459561846, time 735.0, rides 143\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 3280, reward 970.0, memory_length 2000, epsilon 0.052118797287748236, time 734.0, rides 150\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 3281, reward 938.0, memory_length 2000, epsilon 0.052071890370189264, time 729.0, rides 138\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 3282, reward 686.0, memory_length 2000, epsilon 0.05202502566885609, time 729.0, rides 135\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3283, reward 777.0, memory_length 2000, epsilon 0.05197820314575412, time 728.0, rides 139\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 3284, reward 713.0, memory_length 2000, epsilon 0.05193142276292294, time 728.0, rides 133\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3285, reward 963.0, memory_length 2000, epsilon 0.05188468448243631, time 731.0, rides 137\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 3286, reward 940.0, memory_length 2000, epsilon 0.05183798826640211, time 732.0, rides 132\n",
      "Initial State is  [4, 6, 6]\n",
      "episode 3287, reward 664.0, memory_length 2000, epsilon 0.05179133407696235, time 726.0, rides 145\n",
      "Initial State is  [3, 16, 5]\n",
      "episode 3288, reward 584.0, memory_length 2000, epsilon 0.05174472187629308, time 731.0, rides 142\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 3289, reward 656.0, memory_length 2000, epsilon 0.05169815162660442, time 734.0, rides 129\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 3290, reward 939.0, memory_length 2000, epsilon 0.051651623290140475, time 729.0, rides 137\n",
      "Initial State is  [3, 2, 4]\n",
      "episode 3291, reward 820.0, memory_length 2000, epsilon 0.051605136829179346, time 720.0, rides 141\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 3292, reward 1103.0, memory_length 2000, epsilon 0.05155869220603308, time 732.0, rides 138\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 3293, reward 934.0, memory_length 2000, epsilon 0.05151228938304765, time 724.0, rides 135\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3294, reward 983.0, memory_length 2000, epsilon 0.05146592832260291, time 729.0, rides 136\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 3295, reward 1031.0, memory_length 2000, epsilon 0.05141960898711257, time 729.0, rides 132\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 3296, reward 716.0, memory_length 2000, epsilon 0.05137333133902417, time 730.0, rides 131\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 3297, reward 758.0, memory_length 2000, epsilon 0.05132709534081905, time 732.0, rides 143\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 3298, reward 972.0, memory_length 2000, epsilon 0.05128090095501231, time 724.0, rides 143\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 3299, reward 823.0, memory_length 2000, epsilon 0.0512347481441528, time 732.0, rides 146\n",
      "Initial State is  [4, 0, 1]\n",
      "episode 3300, reward 1238.0, memory_length 2000, epsilon 0.05118863687082306, time 736.0, rides 138\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 3301, reward 668.0, memory_length 2000, epsilon 0.05114256709763932, time 732.0, rides 144\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 3302, reward 1203.0, memory_length 2000, epsilon 0.05109653878725144, time 732.0, rides 147\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 3303, reward 910.0, memory_length 2000, epsilon 0.051050551902342915, time 727.0, rides 138\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 3304, reward 696.0, memory_length 2000, epsilon 0.05100460640563081, time 730.0, rides 126\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 3305, reward 845.0, memory_length 2000, epsilon 0.05095870225986574, time 726.0, rides 146\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 3306, reward 919.0, memory_length 2000, epsilon 0.05091283942783186, time 730.0, rides 130\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 3307, reward 627.0, memory_length 2000, epsilon 0.05086701787234681, time 728.0, rides 148\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 3308, reward 616.0, memory_length 2000, epsilon 0.0508212375562617, time 730.0, rides 131\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3309, reward 649.0, memory_length 2000, epsilon 0.05077549844246106, time 742.0, rides 124\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 3310, reward 771.0, memory_length 2000, epsilon 0.050729800493862845, time 734.0, rides 129\n",
      "Initial State is  [2, 4, 4]\n",
      "episode 3311, reward 578.0, memory_length 2000, epsilon 0.05068414367341837, time 726.0, rides 145\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 3312, reward 903.0, memory_length 2000, epsilon 0.05063852794411229, time 733.0, rides 149\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 3313, reward 977.0, memory_length 2000, epsilon 0.05059295326896259, time 727.0, rides 132\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 3314, reward 894.0, memory_length 2000, epsilon 0.05054741961102052, time 736.0, rides 134\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 3315, reward 552.0, memory_length 2000, epsilon 0.050501926933370606, time 730.0, rides 124\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 3316, reward 922.0, memory_length 2000, epsilon 0.050456475199130574, time 731.0, rides 128\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 3317, reward 799.0, memory_length 2000, epsilon 0.050411064371451354, time 725.0, rides 148\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 3318, reward 600.0, memory_length 2000, epsilon 0.05036569441351705, time 731.0, rides 132\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 3319, reward 550.0, memory_length 2000, epsilon 0.05032036528854488, time 723.0, rides 127\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 3320, reward 982.0, memory_length 2000, epsilon 0.05027507695978519, time 728.0, rides 156\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 3321, reward 1012.0, memory_length 2000, epsilon 0.05022982939052138, time 738.0, rides 130\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 3322, reward 804.0, memory_length 2000, epsilon 0.05018462254406991, time 729.0, rides 152\n",
      "Initial State is  [1, 8, 0]\n",
      "episode 3323, reward 913.0, memory_length 2000, epsilon 0.05013945638378025, time 724.0, rides 135\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 3324, reward 911.0, memory_length 2000, epsilon 0.050094330873034845, time 724.0, rides 123\n",
      "Initial State is  [3, 3, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3325, reward 699.0, memory_length 2000, epsilon 0.05004924597524912, time 724.0, rides 136\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 3326, reward 893.0, memory_length 2000, epsilon 0.05000420165387139, time 728.0, rides 135\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 3327, reward 455.0, memory_length 2000, epsilon 0.049959197872382906, time 732.0, rides 135\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 3328, reward 781.0, memory_length 2000, epsilon 0.04991423459429776, time 731.0, rides 129\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 3329, reward 1271.0, memory_length 2000, epsilon 0.04986931178316289, time 732.0, rides 146\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 3330, reward 904.0, memory_length 2000, epsilon 0.04982442940255804, time 726.0, rides 127\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 3331, reward 956.0, memory_length 2000, epsilon 0.049779587416095734, time 725.0, rides 132\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 3332, reward 1026.0, memory_length 2000, epsilon 0.04973478578742125, time 722.0, rides 146\n",
      "Initial State is  [0, 11, 1]\n",
      "episode 3333, reward 939.0, memory_length 2000, epsilon 0.04969002448021257, time 729.0, rides 151\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 3334, reward 730.0, memory_length 2000, epsilon 0.04964530345818038, time 723.0, rides 147\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 3335, reward 791.0, memory_length 2000, epsilon 0.04960062268506801, time 728.0, rides 128\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 3336, reward 959.0, memory_length 2000, epsilon 0.049555982124651454, time 736.0, rides 131\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 3337, reward 594.0, memory_length 2000, epsilon 0.04951138174073927, time 728.0, rides 130\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 3338, reward 628.0, memory_length 2000, epsilon 0.0494668214971726, time 724.0, rides 137\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 3339, reward 790.0, memory_length 2000, epsilon 0.049422301357825146, time 722.0, rides 158\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 3340, reward 855.0, memory_length 2000, epsilon 0.049377821286603105, time 731.0, rides 146\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 3341, reward 805.0, memory_length 2000, epsilon 0.049333381247445164, time 727.0, rides 143\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 3342, reward 702.0, memory_length 2000, epsilon 0.049288981204322464, time 726.0, rides 144\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 3343, reward 674.0, memory_length 2000, epsilon 0.04924462112123857, time 726.0, rides 130\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 3344, reward 969.0, memory_length 2000, epsilon 0.04920030096222946, time 739.0, rides 147\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 3345, reward 1183.0, memory_length 2000, epsilon 0.049156020691363454, time 723.0, rides 137\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 3346, reward 899.0, memory_length 2000, epsilon 0.049111780272741226, time 721.0, rides 130\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 3347, reward 826.0, memory_length 2000, epsilon 0.049067579670495756, time 727.0, rides 140\n",
      "Initial State is  [4, 12, 1]\n",
      "episode 3348, reward 1216.0, memory_length 2000, epsilon 0.049023418848792306, time 726.0, rides 129\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3349, reward 710.0, memory_length 2000, epsilon 0.04897929777182839, time 724.0, rides 124\n",
      "Initial State is  [0, 2, 6]\n",
      "episode 3350, reward 1276.0, memory_length 2000, epsilon 0.04893521640383375, time 733.0, rides 129\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 3351, reward 990.0, memory_length 2000, epsilon 0.0488911747090703, time 723.0, rides 143\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 3352, reward 989.0, memory_length 2000, epsilon 0.04884717265183213, time 728.0, rides 162\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 3353, reward 894.0, memory_length 2000, epsilon 0.048803210196445485, time 725.0, rides 145\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 3354, reward 691.0, memory_length 2000, epsilon 0.04875928730726868, time 733.0, rides 141\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 3355, reward 878.0, memory_length 2000, epsilon 0.048715403948692136, time 737.0, rides 128\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 3356, reward 464.0, memory_length 2000, epsilon 0.048671560085138316, time 726.0, rides 134\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 3357, reward 684.0, memory_length 2000, epsilon 0.04862775568106169, time 727.0, rides 132\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3358, reward 692.0, memory_length 2000, epsilon 0.04858399070094873, time 731.0, rides 133\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 3359, reward 730.0, memory_length 2000, epsilon 0.04854026510931787, time 726.0, rides 148\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 3360, reward 900.0, memory_length 2000, epsilon 0.04849657887071949, time 732.0, rides 132\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 3361, reward 965.0, memory_length 2000, epsilon 0.04845293194973584, time 725.0, rides 132\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 3362, reward 795.0, memory_length 2000, epsilon 0.04840932431098108, time 728.0, rides 142\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 3363, reward 581.0, memory_length 2000, epsilon 0.048365755919101194, time 722.0, rides 136\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 3364, reward 771.0, memory_length 2000, epsilon 0.048322226738774, time 727.0, rides 134\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 3365, reward 883.0, memory_length 2000, epsilon 0.04827873673470911, time 728.0, rides 128\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 3366, reward 756.0, memory_length 2000, epsilon 0.04823528587164787, time 733.0, rides 144\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 3367, reward 991.0, memory_length 2000, epsilon 0.04819187411436338, time 730.0, rides 139\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 3368, reward 415.0, memory_length 2000, epsilon 0.04814850142766045, time 723.0, rides 145\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 3369, reward 892.0, memory_length 2000, epsilon 0.04810516777637556, time 731.0, rides 156\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 3370, reward 782.0, memory_length 2000, epsilon 0.04806187312537682, time 728.0, rides 144\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 3371, reward 892.0, memory_length 2000, epsilon 0.048018617439563975, time 724.0, rides 140\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 3372, reward 1072.0, memory_length 2000, epsilon 0.047975400683868366, time 729.0, rides 132\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 3373, reward 963.0, memory_length 2000, epsilon 0.047932222823252886, time 728.0, rides 136\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 3374, reward 605.0, memory_length 2000, epsilon 0.04788908382271196, time 731.0, rides 142\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 3375, reward 681.0, memory_length 2000, epsilon 0.047845983647271516, time 733.0, rides 143\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 3376, reward 796.0, memory_length 2000, epsilon 0.04780292226198897, time 725.0, rides 149\n",
      "Initial State is  [4, 15, 1]\n",
      "episode 3377, reward 808.0, memory_length 2000, epsilon 0.04775989963195318, time 735.0, rides 146\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 3378, reward 847.0, memory_length 2000, epsilon 0.04771691572228442, time 727.0, rides 127\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 3379, reward 773.0, memory_length 2000, epsilon 0.04767397049813436, time 736.0, rides 134\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 3380, reward 903.0, memory_length 2000, epsilon 0.047631063924686044, time 724.0, rides 139\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 3381, reward 839.0, memory_length 2000, epsilon 0.04758819596715383, time 722.0, rides 132\n",
      "Initial State is  [1, 15, 1]\n",
      "episode 3382, reward 1164.0, memory_length 2000, epsilon 0.04754536659078339, time 733.0, rides 141\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 3383, reward 812.0, memory_length 2000, epsilon 0.047502575760851685, time 730.0, rides 123\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 3384, reward 930.0, memory_length 2000, epsilon 0.047459823442666915, time 728.0, rides 153\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 3385, reward 789.0, memory_length 2000, epsilon 0.047417109601568516, time 727.0, rides 117\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 3386, reward 888.0, memory_length 2000, epsilon 0.047374434202927106, time 725.0, rides 152\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 3387, reward 422.0, memory_length 2000, epsilon 0.04733179721214447, time 725.0, rides 140\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 3388, reward 697.0, memory_length 2000, epsilon 0.04728919859465354, time 724.0, rides 131\n",
      "Initial State is  [1, 17, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3389, reward 1165.0, memory_length 2000, epsilon 0.04724663831591835, time 730.0, rides 119\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 3390, reward 814.0, memory_length 2000, epsilon 0.04720411634143402, time 730.0, rides 130\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 3391, reward 699.0, memory_length 2000, epsilon 0.04716163263672673, time 725.0, rides 129\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 3392, reward 816.0, memory_length 2000, epsilon 0.04711918716735367, time 723.0, rides 135\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 3393, reward 747.0, memory_length 2000, epsilon 0.047076779898903055, time 729.0, rides 132\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 3394, reward 794.0, memory_length 2000, epsilon 0.04703441079699404, time 721.0, rides 130\n",
      "Initial State is  [3, 4, 3]\n",
      "episode 3395, reward 830.0, memory_length 2000, epsilon 0.046992079827276746, time 728.0, rides 150\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 3396, reward 763.0, memory_length 2000, epsilon 0.0469497869554322, time 728.0, rides 123\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 3397, reward 985.0, memory_length 2000, epsilon 0.04690753214717231, time 723.0, rides 142\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 3398, reward 457.0, memory_length 2000, epsilon 0.04686531536823985, time 723.0, rides 146\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 3399, reward 843.0, memory_length 2000, epsilon 0.04682313658440844, time 734.0, rides 123\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 3400, reward 877.0, memory_length 2000, epsilon 0.04678099576148247, time 726.0, rides 127\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 3401, reward 620.0, memory_length 2000, epsilon 0.04673889286529714, time 726.0, rides 152\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 3402, reward 768.0, memory_length 2000, epsilon 0.04669682786171837, time 725.0, rides 139\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 3403, reward 929.0, memory_length 2000, epsilon 0.04665480071664282, time 725.0, rides 127\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 3404, reward 700.0, memory_length 2000, epsilon 0.046612811395997836, time 728.0, rides 145\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 3405, reward 644.0, memory_length 2000, epsilon 0.046570859865741436, time 727.0, rides 135\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 3406, reward 985.0, memory_length 2000, epsilon 0.04652894609186227, time 732.0, rides 145\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 3407, reward 983.0, memory_length 2000, epsilon 0.046487070040379594, time 732.0, rides 149\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 3408, reward 1051.0, memory_length 2000, epsilon 0.046445231677343254, time 729.0, rides 134\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 3409, reward 946.0, memory_length 2000, epsilon 0.04640343096883365, time 730.0, rides 126\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 3410, reward 1008.0, memory_length 2000, epsilon 0.046361667880961695, time 733.0, rides 146\n",
      "Initial State is  [1, 15, 6]\n",
      "episode 3411, reward 928.0, memory_length 2000, epsilon 0.04631994237986883, time 727.0, rides 127\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 3412, reward 884.0, memory_length 2000, epsilon 0.046278254431726944, time 724.0, rides 129\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 3413, reward 585.0, memory_length 2000, epsilon 0.046236604002738386, time 726.0, rides 127\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 3414, reward 1003.0, memory_length 2000, epsilon 0.04619499105913592, time 727.0, rides 134\n",
      "Initial State is  [1, 6, 0]\n",
      "episode 3415, reward 833.0, memory_length 2000, epsilon 0.0461534155671827, time 728.0, rides 130\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 3416, reward 525.0, memory_length 2000, epsilon 0.046111877493172235, time 736.0, rides 130\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 3417, reward 906.0, memory_length 2000, epsilon 0.04607037680342838, time 730.0, rides 130\n",
      "Initial State is  [0, 2, 2]\n",
      "episode 3418, reward 762.0, memory_length 2000, epsilon 0.04602891346430529, time 735.0, rides 131\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 3419, reward 1144.0, memory_length 2000, epsilon 0.04598748744218742, time 720.0, rides 136\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 3420, reward 911.0, memory_length 2000, epsilon 0.04594609870348945, time 725.0, rides 130\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3421, reward 874.0, memory_length 2000, epsilon 0.04590474721465631, time 726.0, rides 134\n",
      "Initial State is  [0, 22, 3]\n",
      "episode 3422, reward 1060.0, memory_length 2000, epsilon 0.04586343294216312, time 735.0, rides 124\n",
      "Initial State is  [2, 9, 2]\n",
      "episode 3423, reward 795.0, memory_length 2000, epsilon 0.04582215585251517, time 730.0, rides 148\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3424, reward 1179.0, memory_length 2000, epsilon 0.04578091591224791, time 732.0, rides 138\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 3425, reward 773.0, memory_length 2000, epsilon 0.04573971308792688, time 728.0, rides 133\n",
      "Initial State is  [2, 15, 5]\n",
      "episode 3426, reward 533.0, memory_length 2000, epsilon 0.04569854734614775, time 735.0, rides 140\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 3427, reward 894.0, memory_length 2000, epsilon 0.045657418653536216, time 725.0, rides 145\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 3428, reward 722.0, memory_length 2000, epsilon 0.045616326976748035, time 725.0, rides 131\n",
      "Initial State is  [4, 12, 6]\n",
      "episode 3429, reward 815.0, memory_length 2000, epsilon 0.045575272282468965, time 730.0, rides 148\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 3430, reward 1008.0, memory_length 2000, epsilon 0.04553425453741474, time 724.0, rides 132\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 3431, reward 464.0, memory_length 2000, epsilon 0.04549327370833107, time 731.0, rides 143\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 3432, reward 652.0, memory_length 2000, epsilon 0.04545232976199357, time 730.0, rides 155\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 3433, reward 755.0, memory_length 2000, epsilon 0.04541142266520778, time 728.0, rides 139\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 3434, reward 738.0, memory_length 2000, epsilon 0.04537055238480909, time 726.0, rides 140\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 3435, reward 745.0, memory_length 2000, epsilon 0.04532971888766276, time 732.0, rides 141\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 3436, reward 910.0, memory_length 2000, epsilon 0.045288922140663865, time 722.0, rides 132\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3437, reward 896.0, memory_length 2000, epsilon 0.04524816211073727, time 728.0, rides 135\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3438, reward 1293.0, memory_length 2000, epsilon 0.045207438764837606, time 727.0, rides 143\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 3439, reward 673.0, memory_length 2000, epsilon 0.04516675206994925, time 727.0, rides 140\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3440, reward 1007.0, memory_length 2000, epsilon 0.045126101993086296, time 734.0, rides 140\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 3441, reward 792.0, memory_length 2000, epsilon 0.045085488501292514, time 729.0, rides 135\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 3442, reward 832.0, memory_length 2000, epsilon 0.04504491156164135, time 731.0, rides 136\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 3443, reward 656.0, memory_length 2000, epsilon 0.04500437114123587, time 724.0, rides 135\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 3444, reward 814.0, memory_length 2000, epsilon 0.04496386720720876, time 727.0, rides 137\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 3445, reward 627.0, memory_length 2000, epsilon 0.044923399726722275, time 723.0, rides 130\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 3446, reward 1097.0, memory_length 2000, epsilon 0.044882968666968226, time 727.0, rides 146\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 3447, reward 867.0, memory_length 2000, epsilon 0.04484257399516795, time 729.0, rides 136\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 3448, reward 785.0, memory_length 2000, epsilon 0.0448022156785723, time 732.0, rides 139\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 3449, reward 521.0, memory_length 2000, epsilon 0.04476189368446159, time 729.0, rides 145\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 3450, reward 861.0, memory_length 2000, epsilon 0.04472160798014557, time 728.0, rides 132\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 3451, reward 1018.0, memory_length 2000, epsilon 0.04468135853296344, time 725.0, rides 143\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3452, reward 1044.0, memory_length 2000, epsilon 0.04464114531028377, time 730.0, rides 142\n",
      "Initial State is  [2, 11, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3453, reward 736.0, memory_length 2000, epsilon 0.044600968279504515, time 723.0, rides 138\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 3454, reward 777.0, memory_length 2000, epsilon 0.04456082740805296, time 726.0, rides 131\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 3455, reward 891.0, memory_length 2000, epsilon 0.044520722663385706, time 727.0, rides 148\n",
      "Initial State is  [0, 18, 1]\n",
      "episode 3456, reward 711.0, memory_length 2000, epsilon 0.04448065401298866, time 723.0, rides 141\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 3457, reward 645.0, memory_length 2000, epsilon 0.04444062142437697, time 727.0, rides 134\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 3458, reward 763.0, memory_length 2000, epsilon 0.04440062486509503, time 733.0, rides 150\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 3459, reward 1186.0, memory_length 2000, epsilon 0.04436066430271644, time 731.0, rides 133\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 3460, reward 943.0, memory_length 2000, epsilon 0.044320739704844, time 733.0, rides 137\n",
      "Initial State is  [1, 12, 3]\n",
      "episode 3461, reward 1102.0, memory_length 2000, epsilon 0.04428085103910964, time 733.0, rides 138\n",
      "Initial State is  [0, 1, 0]\n",
      "episode 3462, reward 829.0, memory_length 2000, epsilon 0.044240998273174445, time 726.0, rides 142\n",
      "Initial State is  [1, 8, 2]\n",
      "episode 3463, reward 634.0, memory_length 2000, epsilon 0.04420118137472859, time 729.0, rides 139\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 3464, reward 610.0, memory_length 2000, epsilon 0.04416140031149133, time 731.0, rides 150\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 3465, reward 1025.0, memory_length 2000, epsilon 0.04412165505121099, time 725.0, rides 139\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 3466, reward 755.0, memory_length 2000, epsilon 0.0440819455616649, time 730.0, rides 140\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 3467, reward 737.0, memory_length 2000, epsilon 0.0440422718106594, time 728.0, rides 149\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 3468, reward 955.0, memory_length 2000, epsilon 0.04400263376602981, time 734.0, rides 132\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 3469, reward 638.0, memory_length 2000, epsilon 0.04396303139564038, time 727.0, rides 131\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 3470, reward 1079.0, memory_length 2000, epsilon 0.0439234646673843, time 728.0, rides 140\n",
      "Initial State is  [1, 11, 3]\n",
      "episode 3471, reward 712.0, memory_length 2000, epsilon 0.04388393354918366, time 741.0, rides 133\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 3472, reward 766.0, memory_length 2000, epsilon 0.04384443800898939, time 731.0, rides 134\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 3473, reward 692.0, memory_length 2000, epsilon 0.0438049780147813, time 738.0, rides 132\n",
      "Initial State is  [0, 8, 3]\n",
      "episode 3474, reward 548.0, memory_length 2000, epsilon 0.043765553534568, time 730.0, rides 132\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 3475, reward 636.0, memory_length 2000, epsilon 0.04372616453638689, time 731.0, rides 159\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 3476, reward 705.0, memory_length 2000, epsilon 0.04368681098830414, time 728.0, rides 145\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 3477, reward 995.0, memory_length 2000, epsilon 0.04364749285841466, time 731.0, rides 136\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 3478, reward 698.0, memory_length 2000, epsilon 0.04360821011484209, time 731.0, rides 128\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 3479, reward 756.0, memory_length 2000, epsilon 0.04356896272573873, time 723.0, rides 143\n",
      "Initial State is  [1, 10, 3]\n",
      "episode 3480, reward 874.0, memory_length 2000, epsilon 0.04352975065928556, time 723.0, rides 131\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 3481, reward 842.0, memory_length 2000, epsilon 0.04349057388369221, time 727.0, rides 161\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 3482, reward 893.0, memory_length 2000, epsilon 0.04345143236719688, time 737.0, rides 136\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 3483, reward 989.0, memory_length 2000, epsilon 0.0434123260780664, time 732.0, rides 143\n",
      "Initial State is  [3, 4, 1]\n",
      "episode 3484, reward 965.0, memory_length 2000, epsilon 0.04337325498459614, time 731.0, rides 143\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 3485, reward 1270.0, memory_length 2000, epsilon 0.04333421905511001, time 727.0, rides 137\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 3486, reward 630.0, memory_length 2000, epsilon 0.043295218257960406, time 729.0, rides 132\n",
      "Initial State is  [0, 12, 3]\n",
      "episode 3487, reward 1062.0, memory_length 2000, epsilon 0.04325625256152824, time 736.0, rides 154\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 3488, reward 834.0, memory_length 2000, epsilon 0.04321732193422287, time 723.0, rides 129\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 3489, reward 1018.0, memory_length 2000, epsilon 0.04317842634448207, time 732.0, rides 143\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 3490, reward 1152.0, memory_length 2000, epsilon 0.04313956576077203, time 731.0, rides 136\n",
      "Initial State is  [2, 3, 0]\n",
      "episode 3491, reward 1112.0, memory_length 2000, epsilon 0.04310074015158734, time 728.0, rides 138\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 3492, reward 755.0, memory_length 2000, epsilon 0.04306194948545091, time 724.0, rides 137\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 3493, reward 729.0, memory_length 2000, epsilon 0.043023193730914004, time 736.0, rides 138\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 3494, reward 399.0, memory_length 2000, epsilon 0.04298447285655618, time 727.0, rides 130\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 3495, reward 1044.0, memory_length 2000, epsilon 0.04294578683098528, time 731.0, rides 140\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 3496, reward 824.0, memory_length 2000, epsilon 0.04290713562283739, time 728.0, rides 133\n",
      "Initial State is  [1, 14, 3]\n",
      "episode 3497, reward 877.0, memory_length 2000, epsilon 0.04286851920077684, time 731.0, rides 127\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 3498, reward 929.0, memory_length 2000, epsilon 0.04282993753349614, time 735.0, rides 137\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 3499, reward 768.0, memory_length 2000, epsilon 0.042791390589716, time 725.0, rides 139\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 3500, reward 982.0, memory_length 2000, epsilon 0.04275287833818525, time 734.0, rides 149\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 3501, reward 773.0, memory_length 2000, epsilon 0.04271440074768088, time 725.0, rides 130\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 3502, reward 988.0, memory_length 2000, epsilon 0.042675957787007966, time 726.0, rides 135\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 3503, reward 816.0, memory_length 2000, epsilon 0.04263754942499966, time 731.0, rides 146\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 3504, reward 796.0, memory_length 2000, epsilon 0.042599175630517155, time 722.0, rides 135\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 3505, reward 887.0, memory_length 2000, epsilon 0.04256083637244969, time 723.0, rides 144\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 3506, reward 743.0, memory_length 2000, epsilon 0.04252253161971448, time 727.0, rides 135\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 3507, reward 796.0, memory_length 2000, epsilon 0.04248426134125674, time 730.0, rides 146\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 3508, reward 767.0, memory_length 2000, epsilon 0.04244602550604961, time 726.0, rides 143\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 3509, reward 900.0, memory_length 2000, epsilon 0.04240782408309417, time 729.0, rides 123\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 3510, reward 977.0, memory_length 2000, epsilon 0.04236965704141938, time 730.0, rides 135\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 3511, reward 749.0, memory_length 2000, epsilon 0.042331524350082105, time 730.0, rides 129\n",
      "Initial State is  [2, 5, 2]\n",
      "episode 3512, reward 546.0, memory_length 2000, epsilon 0.04229342597816703, time 721.0, rides 132\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 3513, reward 524.0, memory_length 2000, epsilon 0.04225536189478668, time 729.0, rides 120\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 3514, reward 772.0, memory_length 2000, epsilon 0.04221733206908137, time 727.0, rides 139\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 3515, reward 571.0, memory_length 2000, epsilon 0.042179336470219195, time 721.0, rides 139\n",
      "Initial State is  [3, 20, 5]\n",
      "episode 3516, reward 556.0, memory_length 2000, epsilon 0.042141375067396, time 721.0, rides 127\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 3517, reward 559.0, memory_length 2000, epsilon 0.04210344782983534, time 733.0, rides 126\n",
      "Initial State is  [3, 9, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3518, reward 806.0, memory_length 2000, epsilon 0.04206555472678849, time 739.0, rides 127\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 3519, reward 685.0, memory_length 2000, epsilon 0.04202769572753438, time 726.0, rides 133\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 3520, reward 853.0, memory_length 2000, epsilon 0.04198987080137959, time 729.0, rides 147\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 3521, reward 911.0, memory_length 2000, epsilon 0.04195207991765835, time 728.0, rides 134\n",
      "Initial State is  [2, 2, 4]\n",
      "episode 3522, reward 891.0, memory_length 2000, epsilon 0.041914323045732456, time 729.0, rides 124\n",
      "Initial State is  [2, 23, 0]\n",
      "episode 3523, reward 859.0, memory_length 2000, epsilon 0.0418766001549913, time 725.0, rides 133\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 3524, reward 668.0, memory_length 2000, epsilon 0.0418389112148518, time 726.0, rides 134\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 3525, reward 329.0, memory_length 2000, epsilon 0.041801256194758434, time 738.0, rides 131\n",
      "Initial State is  [1, 20, 3]\n",
      "episode 3526, reward 785.0, memory_length 2000, epsilon 0.04176363506418315, time 731.0, rides 126\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 3527, reward 755.0, memory_length 2000, epsilon 0.041726047792625384, time 722.0, rides 164\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 3528, reward 811.0, memory_length 2000, epsilon 0.04168849434961202, time 734.0, rides 137\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 3529, reward 500.0, memory_length 2000, epsilon 0.04165097470469737, time 732.0, rides 142\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 3530, reward 860.0, memory_length 2000, epsilon 0.041613488827463144, time 721.0, rides 147\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 3531, reward 995.0, memory_length 2000, epsilon 0.04157603668751843, time 731.0, rides 126\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 3532, reward 717.0, memory_length 2000, epsilon 0.041538618254499664, time 726.0, rides 155\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 3533, reward 753.0, memory_length 2000, epsilon 0.04150123349807061, time 725.0, rides 146\n",
      "Initial State is  [4, 12, 5]\n",
      "episode 3534, reward 1025.0, memory_length 2000, epsilon 0.04146388238792235, time 731.0, rides 139\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 3535, reward 786.0, memory_length 2000, epsilon 0.041426564893773214, time 732.0, rides 141\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 3536, reward 670.0, memory_length 2000, epsilon 0.041389280985368815, time 729.0, rides 132\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 3537, reward 643.0, memory_length 2000, epsilon 0.04135203063248198, time 729.0, rides 131\n",
      "Initial State is  [3, 3, 0]\n",
      "episode 3538, reward 727.0, memory_length 2000, epsilon 0.041314813804912746, time 728.0, rides 137\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 3539, reward 691.0, memory_length 2000, epsilon 0.04127763047248832, time 731.0, rides 137\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 3540, reward 883.0, memory_length 2000, epsilon 0.04124048060506308, time 726.0, rides 126\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 3541, reward 791.0, memory_length 2000, epsilon 0.04120336417251852, time 730.0, rides 138\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 3542, reward 932.0, memory_length 2000, epsilon 0.04116628114476325, time 730.0, rides 139\n",
      "Initial State is  [3, 18, 2]\n",
      "episode 3543, reward 645.0, memory_length 2000, epsilon 0.04112923149173296, time 726.0, rides 129\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 3544, reward 847.0, memory_length 2000, epsilon 0.0410922151833904, time 731.0, rides 142\n",
      "Initial State is  [1, 15, 5]\n",
      "episode 3545, reward 714.0, memory_length 2000, epsilon 0.04105523218972535, time 723.0, rides 137\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 3546, reward 783.0, memory_length 2000, epsilon 0.0410182824807546, time 737.0, rides 139\n",
      "Initial State is  [4, 17, 1]\n",
      "episode 3547, reward 585.0, memory_length 2000, epsilon 0.04098136602652192, time 729.0, rides 130\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 3548, reward 479.0, memory_length 2000, epsilon 0.04094448279709805, time 727.0, rides 130\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3549, reward 683.0, memory_length 2000, epsilon 0.040907632762580665, time 724.0, rides 137\n",
      "Initial State is  [0, 4, 3]\n",
      "episode 3550, reward 863.0, memory_length 2000, epsilon 0.04087081589309434, time 728.0, rides 133\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 3551, reward 841.0, memory_length 2000, epsilon 0.040834032158790556, time 729.0, rides 125\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3552, reward 858.0, memory_length 2000, epsilon 0.04079728152984764, time 733.0, rides 147\n",
      "Initial State is  [4, 13, 0]\n",
      "episode 3553, reward 806.0, memory_length 2000, epsilon 0.04076056397647078, time 731.0, rides 135\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 3554, reward 621.0, memory_length 2000, epsilon 0.04072387946889196, time 725.0, rides 137\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 3555, reward 882.0, memory_length 2000, epsilon 0.040687227977369955, time 735.0, rides 125\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 3556, reward 633.0, memory_length 2000, epsilon 0.04065060947219032, time 735.0, rides 140\n",
      "Initial State is  [2, 7, 0]\n",
      "episode 3557, reward 655.0, memory_length 2000, epsilon 0.040614023923665345, time 725.0, rides 138\n",
      "Initial State is  [4, 19, 3]\n",
      "episode 3558, reward 1114.0, memory_length 2000, epsilon 0.040577471302134044, time 727.0, rides 140\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 3559, reward 870.0, memory_length 2000, epsilon 0.04054095157796212, time 726.0, rides 144\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 3560, reward 872.0, memory_length 2000, epsilon 0.040504464721541955, time 731.0, rides 132\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 3561, reward 672.0, memory_length 2000, epsilon 0.04046801070329257, time 740.0, rides 132\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 3562, reward 781.0, memory_length 2000, epsilon 0.0404315894936596, time 728.0, rides 135\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 3563, reward 711.0, memory_length 2000, epsilon 0.04039520106311531, time 731.0, rides 138\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 3564, reward 765.0, memory_length 2000, epsilon 0.040358845382158504, time 736.0, rides 138\n",
      "Initial State is  [3, 6, 0]\n",
      "episode 3565, reward 957.0, memory_length 2000, epsilon 0.04032252242131456, time 729.0, rides 143\n",
      "Initial State is  [3, 0, 5]\n",
      "episode 3566, reward 702.0, memory_length 2000, epsilon 0.04028623215113537, time 727.0, rides 161\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 3567, reward 516.0, memory_length 2000, epsilon 0.04024997454219935, time 730.0, rides 134\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 3568, reward 532.0, memory_length 2000, epsilon 0.04021374956511137, time 733.0, rides 136\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 3569, reward 848.0, memory_length 2000, epsilon 0.04017755719050277, time 733.0, rides 130\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3570, reward 1110.0, memory_length 2000, epsilon 0.040141397389031316, time 726.0, rides 136\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 3571, reward 777.0, memory_length 2000, epsilon 0.04010527013138119, time 730.0, rides 131\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3572, reward 768.0, memory_length 2000, epsilon 0.04006917538826295, time 730.0, rides 137\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 3573, reward 1074.0, memory_length 2000, epsilon 0.04003311313041351, time 726.0, rides 127\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3574, reward 580.0, memory_length 2000, epsilon 0.03999708332859614, time 734.0, rides 124\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 3575, reward 1095.0, memory_length 2000, epsilon 0.0399610859536004, time 728.0, rides 140\n",
      "Initial State is  [0, 20, 1]\n",
      "episode 3576, reward 936.0, memory_length 2000, epsilon 0.03992512097624216, time 722.0, rides 142\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 3577, reward 964.0, memory_length 2000, epsilon 0.03988918836736354, time 727.0, rides 136\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 3578, reward 621.0, memory_length 2000, epsilon 0.039853288097832916, time 731.0, rides 118\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 3579, reward 935.0, memory_length 2000, epsilon 0.03981742013854487, time 728.0, rides 149\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 3580, reward 839.0, memory_length 2000, epsilon 0.039781584460420176, time 729.0, rides 123\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 3581, reward 891.0, memory_length 2000, epsilon 0.0397457810344058, time 726.0, rides 139\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 3582, reward 941.0, memory_length 2000, epsilon 0.03971000983147483, time 729.0, rides 131\n",
      "Initial State is  [3, 8, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3583, reward 902.0, memory_length 2000, epsilon 0.039674270822626506, time 730.0, rides 134\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 3584, reward 792.0, memory_length 2000, epsilon 0.03963856397888614, time 732.0, rides 148\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 3585, reward 549.0, memory_length 2000, epsilon 0.03960288927130514, time 737.0, rides 139\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 3586, reward 1001.0, memory_length 2000, epsilon 0.03956724667096097, time 726.0, rides 140\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3587, reward 761.0, memory_length 2000, epsilon 0.039531636148957106, time 725.0, rides 131\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3588, reward 724.0, memory_length 2000, epsilon 0.039496057676423044, time 733.0, rides 147\n",
      "Initial State is  [3, 4, 2]\n",
      "episode 3589, reward 778.0, memory_length 2000, epsilon 0.039460511224514265, time 730.0, rides 139\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 3590, reward 1062.0, memory_length 2000, epsilon 0.039424996764412204, time 733.0, rides 144\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 3591, reward 672.0, memory_length 2000, epsilon 0.03938951426732423, time 727.0, rides 127\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 3592, reward 1004.0, memory_length 2000, epsilon 0.03935406370448364, time 721.0, rides 131\n",
      "Initial State is  [1, 10, 6]\n",
      "episode 3593, reward 744.0, memory_length 2000, epsilon 0.0393186450471496, time 728.0, rides 134\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 3594, reward 567.0, memory_length 2000, epsilon 0.03928325826660717, time 736.0, rides 126\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 3595, reward 940.0, memory_length 2000, epsilon 0.03924790333416722, time 730.0, rides 127\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3596, reward 695.0, memory_length 2000, epsilon 0.03921258022116647, time 725.0, rides 135\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 3597, reward 555.0, memory_length 2000, epsilon 0.03917728889896742, time 725.0, rides 126\n",
      "Initial State is  [4, 23, 4]\n",
      "episode 3598, reward 698.0, memory_length 2000, epsilon 0.03914202933895835, time 731.0, rides 131\n",
      "Initial State is  [3, 15, 1]\n",
      "episode 3599, reward 939.0, memory_length 2000, epsilon 0.03910680151255329, time 732.0, rides 136\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 3600, reward 750.0, memory_length 2000, epsilon 0.03907160539119199, time 730.0, rides 138\n",
      "Initial State is  [0, 17, 5]\n",
      "episode 3601, reward 1019.0, memory_length 2000, epsilon 0.039036440946339915, time 726.0, rides 123\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 3602, reward 663.0, memory_length 2000, epsilon 0.039001308149488205, time 724.0, rides 138\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 3603, reward 1105.0, memory_length 2000, epsilon 0.038966206972153666, time 736.0, rides 141\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 3604, reward 816.0, memory_length 2000, epsilon 0.03893113738587873, time 736.0, rides 137\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 3605, reward 920.0, memory_length 2000, epsilon 0.038896099362231436, time 727.0, rides 129\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 3606, reward 746.0, memory_length 2000, epsilon 0.038861092872805425, time 725.0, rides 130\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 3607, reward 631.0, memory_length 2000, epsilon 0.0388261178892199, time 728.0, rides 124\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 3608, reward 841.0, memory_length 2000, epsilon 0.0387911743831196, time 724.0, rides 121\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 3609, reward 963.0, memory_length 2000, epsilon 0.038756262326174795, time 735.0, rides 130\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 3610, reward 870.0, memory_length 2000, epsilon 0.038721381690081234, time 726.0, rides 134\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 3611, reward 947.0, memory_length 2000, epsilon 0.03868653244656016, time 731.0, rides 135\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 3612, reward 805.0, memory_length 2000, epsilon 0.038651714567358254, time 728.0, rides 123\n",
      "Initial State is  [4, 13, 2]\n",
      "episode 3613, reward 862.0, memory_length 2000, epsilon 0.03861692802424763, time 730.0, rides 130\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 3614, reward 734.0, memory_length 2000, epsilon 0.03858217278902581, time 724.0, rides 144\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 3615, reward 899.0, memory_length 2000, epsilon 0.038547448833515685, time 724.0, rides 137\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 3616, reward 819.0, memory_length 2000, epsilon 0.03851275612956552, time 724.0, rides 126\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 3617, reward 646.0, memory_length 2000, epsilon 0.03847809464904891, time 732.0, rides 132\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 3618, reward 1166.0, memory_length 2000, epsilon 0.03844346436386476, time 729.0, rides 147\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 3619, reward 778.0, memory_length 2000, epsilon 0.038408865245937285, time 729.0, rides 143\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 3620, reward 798.0, memory_length 2000, epsilon 0.03837429726721594, time 723.0, rides 146\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 3621, reward 799.0, memory_length 2000, epsilon 0.038339760399675446, time 728.0, rides 140\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 3622, reward 782.0, memory_length 2000, epsilon 0.03830525461531574, time 730.0, rides 136\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 3623, reward 868.0, memory_length 2000, epsilon 0.038270779886161954, time 730.0, rides 125\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 3624, reward 780.0, memory_length 2000, epsilon 0.038236336184264405, time 729.0, rides 138\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 3625, reward 725.0, memory_length 2000, epsilon 0.03820192348169857, time 727.0, rides 132\n",
      "Initial State is  [0, 11, 3]\n",
      "episode 3626, reward 1034.0, memory_length 2000, epsilon 0.03816754175056504, time 734.0, rides 132\n",
      "Initial State is  [0, 8, 6]\n",
      "episode 3627, reward 659.0, memory_length 2000, epsilon 0.03813319096298953, time 732.0, rides 146\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 3628, reward 1044.0, memory_length 2000, epsilon 0.038098871091122845, time 726.0, rides 139\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 3629, reward 928.0, memory_length 2000, epsilon 0.03806458210714083, time 732.0, rides 152\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 3630, reward 1000.0, memory_length 2000, epsilon 0.0380303239832444, time 724.0, rides 144\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3631, reward 874.0, memory_length 2000, epsilon 0.03799609669165948, time 729.0, rides 137\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 3632, reward 869.0, memory_length 2000, epsilon 0.03796190020463699, time 731.0, rides 142\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 3633, reward 802.0, memory_length 2000, epsilon 0.03792773449445282, time 733.0, rides 141\n",
      "Initial State is  [1, 14, 2]\n",
      "episode 3634, reward 764.0, memory_length 2000, epsilon 0.03789359953340781, time 729.0, rides 131\n",
      "Initial State is  [4, 1, 0]\n",
      "episode 3635, reward 921.0, memory_length 2000, epsilon 0.03785949529382775, time 725.0, rides 134\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 3636, reward 907.0, memory_length 2000, epsilon 0.037825421748063304, time 722.0, rides 140\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 3637, reward 795.0, memory_length 2000, epsilon 0.037791378868490044, time 725.0, rides 135\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3638, reward 813.0, memory_length 2000, epsilon 0.0377573666275084, time 729.0, rides 147\n",
      "Initial State is  [0, 14, 5]\n",
      "episode 3639, reward 888.0, memory_length 2000, epsilon 0.037723384997543644, time 728.0, rides 133\n",
      "Initial State is  [4, 4, 2]\n",
      "episode 3640, reward 541.0, memory_length 2000, epsilon 0.037689433951045855, time 727.0, rides 141\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 3641, reward 593.0, memory_length 2000, epsilon 0.03765551346048991, time 726.0, rides 135\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 3642, reward 621.0, memory_length 2000, epsilon 0.03762162349837547, time 731.0, rides 137\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 3643, reward 1046.0, memory_length 2000, epsilon 0.03758776403722693, time 727.0, rides 147\n",
      "Initial State is  [4, 16, 2]\n",
      "episode 3644, reward 754.0, memory_length 2000, epsilon 0.03755393504959343, time 727.0, rides 140\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 3645, reward 558.0, memory_length 2000, epsilon 0.03752013650804879, time 728.0, rides 129\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 3646, reward 880.0, memory_length 2000, epsilon 0.03748636838519155, time 731.0, rides 138\n",
      "Initial State is  [1, 19, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3647, reward 1230.0, memory_length 2000, epsilon 0.03745263065364488, time 734.0, rides 144\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 3648, reward 933.0, memory_length 2000, epsilon 0.0374189232860566, time 729.0, rides 126\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 3649, reward 1247.0, memory_length 2000, epsilon 0.03738524625509915, time 725.0, rides 143\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 3650, reward 1016.0, memory_length 2000, epsilon 0.03735159953346956, time 722.0, rides 147\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 3651, reward 569.0, memory_length 2000, epsilon 0.03731798309388944, time 734.0, rides 130\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 3652, reward 962.0, memory_length 2000, epsilon 0.037284396909104935, time 728.0, rides 139\n",
      "Initial State is  [1, 12, 2]\n",
      "episode 3653, reward 809.0, memory_length 2000, epsilon 0.037250840951886736, time 728.0, rides 138\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 3654, reward 846.0, memory_length 2000, epsilon 0.037217315195030035, time 722.0, rides 149\n",
      "Initial State is  [3, 16, 3]\n",
      "episode 3655, reward 706.0, memory_length 2000, epsilon 0.03718381961135451, time 726.0, rides 133\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 3656, reward 831.0, memory_length 2000, epsilon 0.03715035417370429, time 725.0, rides 133\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 3657, reward 970.0, memory_length 2000, epsilon 0.03711691885494796, time 729.0, rides 134\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 3658, reward 1026.0, memory_length 2000, epsilon 0.03708351362797851, time 730.0, rides 129\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3659, reward 835.0, memory_length 2000, epsilon 0.037050138465713325, time 729.0, rides 129\n",
      "Initial State is  [4, 8, 0]\n",
      "episode 3660, reward 850.0, memory_length 2000, epsilon 0.03701679334109418, time 730.0, rides 136\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 3661, reward 1073.0, memory_length 2000, epsilon 0.036983478227087196, time 731.0, rides 140\n",
      "Initial State is  [2, 10, 5]\n",
      "episode 3662, reward 1089.0, memory_length 2000, epsilon 0.03695019309668282, time 725.0, rides 150\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 3663, reward 792.0, memory_length 2000, epsilon 0.036916937922895805, time 726.0, rides 131\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 3664, reward 787.0, memory_length 2000, epsilon 0.036883712678765196, time 726.0, rides 127\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 3665, reward 847.0, memory_length 2000, epsilon 0.036850517337354304, time 733.0, rides 136\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 3666, reward 969.0, memory_length 2000, epsilon 0.036817351871750684, time 728.0, rides 140\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 3667, reward 851.0, memory_length 2000, epsilon 0.03678421625506611, time 729.0, rides 136\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 3668, reward 872.0, memory_length 2000, epsilon 0.03675111046043655, time 731.0, rides 135\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 3669, reward 823.0, memory_length 2000, epsilon 0.036718034461022155, time 728.0, rides 127\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3670, reward 744.0, memory_length 2000, epsilon 0.03668498823000724, time 725.0, rides 153\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 3671, reward 850.0, memory_length 2000, epsilon 0.03665197174060023, time 734.0, rides 127\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 3672, reward 589.0, memory_length 2000, epsilon 0.03661898496603369, time 732.0, rides 138\n",
      "Initial State is  [2, 2, 5]\n",
      "episode 3673, reward 654.0, memory_length 2000, epsilon 0.03658602787956426, time 735.0, rides 144\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 3674, reward 1001.0, memory_length 2000, epsilon 0.03655310045447265, time 731.0, rides 137\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 3675, reward 870.0, memory_length 2000, epsilon 0.036520202664063625, time 732.0, rides 137\n",
      "Initial State is  [0, 5, 2]\n",
      "episode 3676, reward 1006.0, memory_length 2000, epsilon 0.03648733448166597, time 729.0, rides 149\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 3677, reward 914.0, memory_length 2000, epsilon 0.036454495880632466, time 732.0, rides 127\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 3678, reward 1041.0, memory_length 2000, epsilon 0.0364216868343399, time 727.0, rides 121\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 3679, reward 855.0, memory_length 2000, epsilon 0.03638890731618899, time 735.0, rides 129\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 3680, reward 953.0, memory_length 2000, epsilon 0.03635615729960442, time 732.0, rides 131\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 3681, reward 616.0, memory_length 2000, epsilon 0.036323436758034774, time 720.0, rides 127\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 3682, reward 739.0, memory_length 2000, epsilon 0.036290745664952544, time 728.0, rides 131\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 3683, reward 641.0, memory_length 2000, epsilon 0.036258083993854086, time 726.0, rides 133\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 3684, reward 531.0, memory_length 2000, epsilon 0.03622545171825962, time 726.0, rides 140\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 3685, reward 771.0, memory_length 2000, epsilon 0.03619284881171318, time 727.0, rides 136\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 3686, reward 671.0, memory_length 2000, epsilon 0.03616027524778264, time 725.0, rides 126\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 3687, reward 531.0, memory_length 2000, epsilon 0.036127731000059636, time 728.0, rides 128\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 3688, reward 756.0, memory_length 2000, epsilon 0.03609521604215958, time 727.0, rides 139\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 3689, reward 897.0, memory_length 2000, epsilon 0.03606273034772164, time 727.0, rides 133\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 3690, reward 716.0, memory_length 2000, epsilon 0.036030273890408686, time 731.0, rides 141\n",
      "Initial State is  [3, 19, 5]\n",
      "episode 3691, reward 832.0, memory_length 2000, epsilon 0.035997846643907316, time 732.0, rides 137\n",
      "Initial State is  [4, 15, 6]\n",
      "episode 3692, reward 962.0, memory_length 2000, epsilon 0.0359654485819278, time 724.0, rides 144\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 3693, reward 779.0, memory_length 2000, epsilon 0.03593307967820407, time 728.0, rides 141\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 3694, reward 1282.0, memory_length 2000, epsilon 0.03590073990649369, time 732.0, rides 136\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 3695, reward 681.0, memory_length 2000, epsilon 0.03586842924057784, time 729.0, rides 127\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 3696, reward 755.0, memory_length 2000, epsilon 0.035836147654261324, time 730.0, rides 137\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 3697, reward 778.0, memory_length 2000, epsilon 0.03580389512137249, time 735.0, rides 138\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 3698, reward 583.0, memory_length 2000, epsilon 0.03577167161576326, time 729.0, rides 121\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 3699, reward 748.0, memory_length 2000, epsilon 0.03573947711130907, time 722.0, rides 143\n",
      "Initial State is  [4, 6, 3]\n",
      "episode 3700, reward 672.0, memory_length 2000, epsilon 0.035707311581908895, time 722.0, rides 135\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 3701, reward 884.0, memory_length 2000, epsilon 0.035675175001485177, time 728.0, rides 140\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3702, reward 1061.0, memory_length 2000, epsilon 0.03564306734398384, time 731.0, rides 137\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 3703, reward 729.0, memory_length 2000, epsilon 0.035610988583374255, time 727.0, rides 132\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 3704, reward 760.0, memory_length 2000, epsilon 0.03557893869364922, time 728.0, rides 138\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 3705, reward 898.0, memory_length 2000, epsilon 0.035546917648824936, time 725.0, rides 136\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 3706, reward 1068.0, memory_length 2000, epsilon 0.03551492542294099, time 726.0, rides 132\n",
      "Initial State is  [4, 10, 6]\n",
      "episode 3707, reward 870.0, memory_length 2000, epsilon 0.03548296199006035, time 731.0, rides 125\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 3708, reward 722.0, memory_length 2000, epsilon 0.035451027324269295, time 731.0, rides 137\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 3709, reward 871.0, memory_length 2000, epsilon 0.035419121399677456, time 727.0, rides 135\n",
      "Initial State is  [2, 12, 1]\n",
      "episode 3710, reward 657.0, memory_length 2000, epsilon 0.03538724419041774, time 725.0, rides 131\n",
      "Initial State is  [2, 19, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3711, reward 756.0, memory_length 2000, epsilon 0.03535539567064636, time 727.0, rides 131\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 3712, reward 969.0, memory_length 2000, epsilon 0.03532357581454278, time 724.0, rides 128\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 3713, reward 1002.0, memory_length 2000, epsilon 0.035291784596309696, time 722.0, rides 135\n",
      "Initial State is  [4, 13, 4]\n",
      "episode 3714, reward 726.0, memory_length 2000, epsilon 0.035260021990173016, time 729.0, rides 137\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 3715, reward 811.0, memory_length 2000, epsilon 0.03522828797038186, time 725.0, rides 133\n",
      "Initial State is  [1, 20, 5]\n",
      "episode 3716, reward 704.0, memory_length 2000, epsilon 0.03519658251120852, time 728.0, rides 127\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3717, reward 724.0, memory_length 2000, epsilon 0.03516490558694843, time 739.0, rides 137\n",
      "Initial State is  [0, 22, 5]\n",
      "episode 3718, reward 568.0, memory_length 2000, epsilon 0.035133257171920174, time 728.0, rides 132\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 3719, reward 837.0, memory_length 2000, epsilon 0.035101637240465444, time 726.0, rides 133\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 3720, reward 659.0, memory_length 2000, epsilon 0.035070045766949026, time 732.0, rides 136\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 3721, reward 1072.0, memory_length 2000, epsilon 0.03503848272575877, time 734.0, rides 136\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 3722, reward 873.0, memory_length 2000, epsilon 0.035006948091305584, time 728.0, rides 130\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 3723, reward 439.0, memory_length 2000, epsilon 0.03497544183802341, time 731.0, rides 146\n",
      "Initial State is  [1, 22, 4]\n",
      "episode 3724, reward 651.0, memory_length 2000, epsilon 0.03494396394036919, time 724.0, rides 127\n",
      "Initial State is  [1, 3, 2]\n",
      "episode 3725, reward 743.0, memory_length 2000, epsilon 0.034912514372822855, time 724.0, rides 126\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 3726, reward 809.0, memory_length 2000, epsilon 0.034881093109887316, time 737.0, rides 126\n",
      "Initial State is  [2, 8, 1]\n",
      "episode 3727, reward 668.0, memory_length 2000, epsilon 0.034849700126088415, time 733.0, rides 128\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 3728, reward 787.0, memory_length 2000, epsilon 0.03481833539597493, time 727.0, rides 128\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 3729, reward 868.0, memory_length 2000, epsilon 0.034786998894118557, time 727.0, rides 142\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 3730, reward 780.0, memory_length 2000, epsilon 0.03475569059511385, time 733.0, rides 137\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 3731, reward 823.0, memory_length 2000, epsilon 0.03472441047357825, time 729.0, rides 135\n",
      "Initial State is  [4, 4, 1]\n",
      "episode 3732, reward 652.0, memory_length 2000, epsilon 0.03469315850415203, time 731.0, rides 128\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 3733, reward 1096.0, memory_length 2000, epsilon 0.03466193466149829, time 726.0, rides 127\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 3734, reward 807.0, memory_length 2000, epsilon 0.034630738920302946, time 733.0, rides 134\n",
      "Initial State is  [0, 4, 6]\n",
      "episode 3735, reward 731.0, memory_length 2000, epsilon 0.03459957125527467, time 734.0, rides 143\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3736, reward 815.0, memory_length 2000, epsilon 0.034568431641144926, time 730.0, rides 133\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 3737, reward 724.0, memory_length 2000, epsilon 0.034537320052667894, time 723.0, rides 138\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3738, reward 825.0, memory_length 2000, epsilon 0.03450623646462049, time 728.0, rides 138\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 3739, reward 692.0, memory_length 2000, epsilon 0.03447518085180233, time 729.0, rides 130\n",
      "Initial State is  [1, 21, 0]\n",
      "episode 3740, reward 804.0, memory_length 2000, epsilon 0.03444415318903571, time 724.0, rides 123\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 3741, reward 988.0, memory_length 2000, epsilon 0.03441315345116558, time 733.0, rides 151\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 3742, reward 962.0, memory_length 2000, epsilon 0.03438218161305953, time 745.0, rides 145\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 3743, reward 557.0, memory_length 2000, epsilon 0.03435123764960778, time 723.0, rides 139\n",
      "Initial State is  [2, 7, 4]\n",
      "episode 3744, reward 794.0, memory_length 2000, epsilon 0.034320321535723126, time 726.0, rides 135\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 3745, reward 803.0, memory_length 2000, epsilon 0.034289433246340977, time 736.0, rides 127\n",
      "Initial State is  [4, 22, 1]\n",
      "episode 3746, reward 1139.0, memory_length 2000, epsilon 0.03425857275641927, time 728.0, rides 147\n",
      "Initial State is  [1, 10, 1]\n",
      "episode 3747, reward 1167.0, memory_length 2000, epsilon 0.03422774004093849, time 724.0, rides 125\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 3748, reward 625.0, memory_length 2000, epsilon 0.034196935074901645, time 723.0, rides 139\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 3749, reward 579.0, memory_length 2000, epsilon 0.03416615783333423, time 723.0, rides 155\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3750, reward 883.0, memory_length 2000, epsilon 0.03413540829128423, time 726.0, rides 135\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 3751, reward 820.0, memory_length 2000, epsilon 0.03410468642382208, time 730.0, rides 139\n",
      "Initial State is  [3, 21, 2]\n",
      "episode 3752, reward 769.0, memory_length 2000, epsilon 0.03407399220604063, time 726.0, rides 138\n",
      "Initial State is  [2, 9, 5]\n",
      "episode 3753, reward 630.0, memory_length 2000, epsilon 0.034043325613055196, time 727.0, rides 157\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3754, reward 857.0, memory_length 2000, epsilon 0.034012686620003445, time 730.0, rides 138\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 3755, reward 725.0, memory_length 2000, epsilon 0.03398207520204544, time 727.0, rides 157\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 3756, reward 850.0, memory_length 2000, epsilon 0.0339514913343636, time 726.0, rides 130\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 3757, reward 995.0, memory_length 2000, epsilon 0.03392093499216267, time 733.0, rides 137\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 3758, reward 1144.0, memory_length 2000, epsilon 0.033890406150669725, time 728.0, rides 149\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 3759, reward 721.0, memory_length 2000, epsilon 0.03385990478513412, time 730.0, rides 133\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 3760, reward 1031.0, memory_length 2000, epsilon 0.0338294308708275, time 734.0, rides 150\n",
      "Initial State is  [1, 14, 6]\n",
      "episode 3761, reward 926.0, memory_length 2000, epsilon 0.033798984383043754, time 720.0, rides 150\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 3762, reward 769.0, memory_length 2000, epsilon 0.03376856529709901, time 726.0, rides 138\n",
      "Initial State is  [4, 17, 5]\n",
      "episode 3763, reward 701.0, memory_length 2000, epsilon 0.03373817358833162, time 735.0, rides 145\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 3764, reward 928.0, memory_length 2000, epsilon 0.03370780923210212, time 729.0, rides 152\n",
      "Initial State is  [1, 20, 1]\n",
      "episode 3765, reward 826.0, memory_length 2000, epsilon 0.033677472203793225, time 729.0, rides 142\n",
      "Initial State is  [3, 19, 6]\n",
      "episode 3766, reward 934.0, memory_length 2000, epsilon 0.03364716247880981, time 725.0, rides 138\n",
      "Initial State is  [1, 19, 5]\n",
      "episode 3767, reward 605.0, memory_length 2000, epsilon 0.033616880032578886, time 722.0, rides 136\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 3768, reward 968.0, memory_length 2000, epsilon 0.03358662484054956, time 735.0, rides 135\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 3769, reward 679.0, memory_length 2000, epsilon 0.03355639687819307, time 727.0, rides 141\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 3770, reward 1113.0, memory_length 2000, epsilon 0.033526196121002695, time 726.0, rides 149\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 3771, reward 711.0, memory_length 2000, epsilon 0.03349602254449379, time 727.0, rides 134\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 3772, reward 1208.0, memory_length 2000, epsilon 0.03346587612420374, time 727.0, rides 141\n",
      "Initial State is  [3, 1, 1]\n",
      "episode 3773, reward 839.0, memory_length 2000, epsilon 0.03343575683569196, time 725.0, rides 130\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 3774, reward 581.0, memory_length 2000, epsilon 0.033405664654539834, time 733.0, rides 146\n",
      "Initial State is  [1, 8, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3775, reward 1304.0, memory_length 2000, epsilon 0.03337559955635075, time 724.0, rides 161\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 3776, reward 787.0, memory_length 2000, epsilon 0.033345561516750034, time 731.0, rides 132\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 3777, reward 1004.0, memory_length 2000, epsilon 0.03331555051138496, time 725.0, rides 139\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 3778, reward 979.0, memory_length 2000, epsilon 0.033285566515924715, time 728.0, rides 143\n",
      "Initial State is  [3, 21, 6]\n",
      "episode 3779, reward 958.0, memory_length 2000, epsilon 0.03325560950606038, time 729.0, rides 142\n",
      "Initial State is  [2, 9, 3]\n",
      "episode 3780, reward 601.0, memory_length 2000, epsilon 0.033225679457504924, time 728.0, rides 150\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3781, reward 811.0, memory_length 2000, epsilon 0.03319577634599317, time 728.0, rides 129\n",
      "Initial State is  [4, 18, 6]\n",
      "episode 3782, reward 858.0, memory_length 2000, epsilon 0.033165900147281775, time 730.0, rides 139\n",
      "Initial State is  [0, 1, 4]\n",
      "episode 3783, reward 799.0, memory_length 2000, epsilon 0.03313605083714922, time 737.0, rides 138\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 3784, reward 850.0, memory_length 2000, epsilon 0.033106228391395785, time 725.0, rides 134\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3785, reward 883.0, memory_length 2000, epsilon 0.03307643278584353, time 721.0, rides 139\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 3786, reward 840.0, memory_length 2000, epsilon 0.033046663996336274, time 733.0, rides 135\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 3787, reward 914.0, memory_length 2000, epsilon 0.03301692199873957, time 729.0, rides 127\n",
      "Initial State is  [3, 0, 1]\n",
      "episode 3788, reward 1139.0, memory_length 2000, epsilon 0.0329872067689407, time 727.0, rides 132\n",
      "Initial State is  [2, 10, 4]\n",
      "episode 3789, reward 1060.0, memory_length 2000, epsilon 0.03295751828284865, time 740.0, rides 135\n",
      "Initial State is  [2, 20, 6]\n",
      "episode 3790, reward 1108.0, memory_length 2000, epsilon 0.032927856516394086, time 735.0, rides 139\n",
      "Initial State is  [3, 10, 4]\n",
      "episode 3791, reward 1021.0, memory_length 2000, epsilon 0.032898221445529334, time 730.0, rides 130\n",
      "Initial State is  [3, 6, 1]\n",
      "episode 3792, reward 1040.0, memory_length 2000, epsilon 0.03286861304622836, time 733.0, rides 143\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 3793, reward 971.0, memory_length 2000, epsilon 0.03283903129448675, time 729.0, rides 132\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 3794, reward 937.0, memory_length 2000, epsilon 0.03280947616632171, time 731.0, rides 136\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 3795, reward 738.0, memory_length 2000, epsilon 0.03277994763777202, time 720.0, rides 137\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 3796, reward 934.0, memory_length 2000, epsilon 0.03275044568489802, time 733.0, rides 138\n",
      "Initial State is  [2, 0, 2]\n",
      "episode 3797, reward 718.0, memory_length 2000, epsilon 0.03272097028378161, time 735.0, rides 140\n",
      "Initial State is  [3, 6, 6]\n",
      "episode 3798, reward 788.0, memory_length 2000, epsilon 0.032691521410526204, time 730.0, rides 129\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 3799, reward 1095.0, memory_length 2000, epsilon 0.03266209904125673, time 734.0, rides 141\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 3800, reward 644.0, memory_length 2000, epsilon 0.032632703152119594, time 726.0, rides 134\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 3801, reward 750.0, memory_length 2000, epsilon 0.03260333371928269, time 734.0, rides 126\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 3802, reward 692.0, memory_length 2000, epsilon 0.03257399071893533, time 733.0, rides 117\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 3803, reward 902.0, memory_length 2000, epsilon 0.03254467412728829, time 725.0, rides 128\n",
      "Initial State is  [4, 15, 5]\n",
      "episode 3804, reward 930.0, memory_length 2000, epsilon 0.03251538392057373, time 735.0, rides 134\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 3805, reward 758.0, memory_length 2000, epsilon 0.03248612007504521, time 725.0, rides 135\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 3806, reward 788.0, memory_length 2000, epsilon 0.03245688256697767, time 727.0, rides 152\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 3807, reward 956.0, memory_length 2000, epsilon 0.032427671372667395, time 729.0, rides 134\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 3808, reward 950.0, memory_length 2000, epsilon 0.03239848646843199, time 720.0, rides 138\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 3809, reward 782.0, memory_length 2000, epsilon 0.0323693278306104, time 743.0, rides 132\n",
      "Initial State is  [4, 9, 1]\n",
      "episode 3810, reward 471.0, memory_length 2000, epsilon 0.03234019543556285, time 727.0, rides 125\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 3811, reward 1170.0, memory_length 2000, epsilon 0.03231108925967084, time 729.0, rides 130\n",
      "Initial State is  [4, 13, 5]\n",
      "episode 3812, reward 724.0, memory_length 2000, epsilon 0.03228200927933714, time 724.0, rides 138\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 3813, reward 658.0, memory_length 2000, epsilon 0.032252955470985736, time 727.0, rides 136\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 3814, reward 996.0, memory_length 2000, epsilon 0.03222392781106185, time 728.0, rides 141\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 3815, reward 699.0, memory_length 2000, epsilon 0.03219492627603189, time 726.0, rides 147\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 3816, reward 997.0, memory_length 2000, epsilon 0.03216595084238346, time 731.0, rides 131\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 3817, reward 839.0, memory_length 2000, epsilon 0.032137001486625315, time 726.0, rides 129\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 3818, reward 855.0, memory_length 2000, epsilon 0.03210807818528735, time 729.0, rides 138\n",
      "Initial State is  [4, 3, 4]\n",
      "episode 3819, reward 822.0, memory_length 2000, epsilon 0.032079180914920596, time 727.0, rides 150\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 3820, reward 776.0, memory_length 2000, epsilon 0.03205030965209717, time 733.0, rides 142\n",
      "Initial State is  [2, 3, 5]\n",
      "episode 3821, reward 916.0, memory_length 2000, epsilon 0.03202146437341028, time 723.0, rides 134\n",
      "Initial State is  [4, 20, 0]\n",
      "episode 3822, reward 982.0, memory_length 2000, epsilon 0.03199264505547421, time 723.0, rides 148\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 3823, reward 861.0, memory_length 2000, epsilon 0.031963851674924285, time 728.0, rides 128\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 3824, reward 511.0, memory_length 2000, epsilon 0.031935084208416856, time 731.0, rides 146\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 3825, reward 740.0, memory_length 2000, epsilon 0.03190634263262928, time 727.0, rides 128\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 3826, reward 939.0, memory_length 2000, epsilon 0.03187762692425991, time 722.0, rides 144\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 3827, reward 758.0, memory_length 2000, epsilon 0.031848937060028074, time 722.0, rides 132\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 3828, reward 821.0, memory_length 2000, epsilon 0.03182027301667405, time 720.0, rides 132\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3829, reward 1067.0, memory_length 2000, epsilon 0.03179163477095904, time 731.0, rides 135\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 3830, reward 1026.0, memory_length 2000, epsilon 0.031763022299665176, time 734.0, rides 137\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 3831, reward 1003.0, memory_length 2000, epsilon 0.031734435579595474, time 726.0, rides 127\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 3832, reward 1139.0, memory_length 2000, epsilon 0.03170587458757384, time 733.0, rides 141\n",
      "Initial State is  [3, 17, 1]\n",
      "episode 3833, reward 848.0, memory_length 2000, epsilon 0.03167733930044502, time 732.0, rides 133\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 3834, reward 982.0, memory_length 2000, epsilon 0.03164882969507462, time 726.0, rides 143\n",
      "Initial State is  [4, 3, 3]\n",
      "episode 3835, reward 816.0, memory_length 2000, epsilon 0.03162034574834905, time 728.0, rides 143\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 3836, reward 1098.0, memory_length 2000, epsilon 0.03159188743717554, time 732.0, rides 129\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 3837, reward 568.0, memory_length 2000, epsilon 0.03156345473848208, time 730.0, rides 126\n",
      "Initial State is  [0, 5, 6]\n",
      "episode 3838, reward 625.0, memory_length 2000, epsilon 0.031535047629217446, time 730.0, rides 131\n",
      "Initial State is  [4, 16, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3839, reward 912.0, memory_length 2000, epsilon 0.03150666608635115, time 728.0, rides 128\n",
      "Initial State is  [0, 22, 0]\n",
      "episode 3840, reward 903.0, memory_length 2000, epsilon 0.031478310086873434, time 728.0, rides 144\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 3841, reward 649.0, memory_length 2000, epsilon 0.03144997960779525, time 728.0, rides 129\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 3842, reward 1012.0, memory_length 2000, epsilon 0.031421674626148234, time 728.0, rides 137\n",
      "Initial State is  [1, 12, 6]\n",
      "episode 3843, reward 977.0, memory_length 2000, epsilon 0.0313933951189847, time 729.0, rides 123\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 3844, reward 536.0, memory_length 2000, epsilon 0.031365141063377615, time 727.0, rides 132\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 3845, reward 707.0, memory_length 2000, epsilon 0.031336912436420575, time 723.0, rides 131\n",
      "Initial State is  [3, 6, 2]\n",
      "episode 3846, reward 835.0, memory_length 2000, epsilon 0.0313087092152278, time 728.0, rides 125\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 3847, reward 777.0, memory_length 2000, epsilon 0.031280531376934095, time 727.0, rides 149\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 3848, reward 730.0, memory_length 2000, epsilon 0.03125237889869485, time 727.0, rides 139\n",
      "Initial State is  [4, 1, 3]\n",
      "episode 3849, reward 785.0, memory_length 2000, epsilon 0.031224251757686027, time 726.0, rides 148\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3850, reward 997.0, memory_length 2000, epsilon 0.03119614993110411, time 729.0, rides 130\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 3851, reward 517.0, memory_length 2000, epsilon 0.031168073396166115, time 725.0, rides 125\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 3852, reward 816.0, memory_length 2000, epsilon 0.031140022130109565, time 727.0, rides 135\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 3853, reward 1027.0, memory_length 2000, epsilon 0.031111996110192466, time 727.0, rides 129\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 3854, reward 972.0, memory_length 2000, epsilon 0.031083995313693293, time 725.0, rides 128\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 3855, reward 879.0, memory_length 2000, epsilon 0.031056019717910967, time 733.0, rides 131\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 3856, reward 857.0, memory_length 2000, epsilon 0.031028069300164846, time 724.0, rides 130\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 3857, reward 1048.0, memory_length 2000, epsilon 0.031000144037794698, time 730.0, rides 125\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 3858, reward 715.0, memory_length 2000, epsilon 0.030972243908160682, time 727.0, rides 140\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 3859, reward 895.0, memory_length 2000, epsilon 0.030944368888643336, time 731.0, rides 132\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 3860, reward 747.0, memory_length 2000, epsilon 0.030916518956643557, time 727.0, rides 135\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 3861, reward 767.0, memory_length 2000, epsilon 0.030888694089582575, time 721.0, rides 132\n",
      "Initial State is  [0, 0, 3]\n",
      "episode 3862, reward 1046.0, memory_length 2000, epsilon 0.03086089426490195, time 730.0, rides 127\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 3863, reward 758.0, memory_length 2000, epsilon 0.03083311946006354, time 723.0, rides 141\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 3864, reward 843.0, memory_length 2000, epsilon 0.030805369652549482, time 729.0, rides 128\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 3865, reward 633.0, memory_length 2000, epsilon 0.03077764481986219, time 726.0, rides 135\n",
      "Initial State is  [3, 21, 3]\n",
      "episode 3866, reward 847.0, memory_length 2000, epsilon 0.03074994493952431, time 731.0, rides 132\n",
      "Initial State is  [2, 1, 5]\n",
      "episode 3867, reward 714.0, memory_length 2000, epsilon 0.03072226998907874, time 739.0, rides 138\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 3868, reward 944.0, memory_length 2000, epsilon 0.030694619946088568, time 731.0, rides 134\n",
      "Initial State is  [4, 13, 1]\n",
      "episode 3869, reward 656.0, memory_length 2000, epsilon 0.030666994788137086, time 727.0, rides 143\n",
      "Initial State is  [4, 2, 1]\n",
      "episode 3870, reward 819.0, memory_length 2000, epsilon 0.03063939449282776, time 739.0, rides 135\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 3871, reward 915.0, memory_length 2000, epsilon 0.030611819037784215, time 724.0, rides 140\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 3872, reward 940.0, memory_length 2000, epsilon 0.03058426840065021, time 730.0, rides 127\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 3873, reward 749.0, memory_length 2000, epsilon 0.030556742559089623, time 724.0, rides 128\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 3874, reward 714.0, memory_length 2000, epsilon 0.03052924149078644, time 729.0, rides 129\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 3875, reward 727.0, memory_length 2000, epsilon 0.030501765173444734, time 724.0, rides 134\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 3876, reward 814.0, memory_length 2000, epsilon 0.030474313584788634, time 732.0, rides 145\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 3877, reward 906.0, memory_length 2000, epsilon 0.030446886702562324, time 723.0, rides 123\n",
      "Initial State is  [3, 3, 2]\n",
      "episode 3878, reward 932.0, memory_length 2000, epsilon 0.03041948450453002, time 731.0, rides 147\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 3879, reward 805.0, memory_length 2000, epsilon 0.03039210696847594, time 735.0, rides 131\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 3880, reward 771.0, memory_length 2000, epsilon 0.030364754072204313, time 731.0, rides 133\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 3881, reward 821.0, memory_length 2000, epsilon 0.03033742579353933, time 724.0, rides 139\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3882, reward 892.0, memory_length 2000, epsilon 0.030310122110325143, time 729.0, rides 129\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 3883, reward 798.0, memory_length 2000, epsilon 0.03028284300042585, time 730.0, rides 132\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 3884, reward 744.0, memory_length 2000, epsilon 0.03025558844172547, time 731.0, rides 142\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 3885, reward 641.0, memory_length 2000, epsilon 0.030228358412127915, time 722.0, rides 137\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 3886, reward 994.0, memory_length 2000, epsilon 0.030201152889557, time 725.0, rides 134\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 3887, reward 795.0, memory_length 2000, epsilon 0.0301739718519564, time 725.0, rides 139\n",
      "Initial State is  [1, 11, 1]\n",
      "episode 3888, reward 983.0, memory_length 2000, epsilon 0.030146815277289636, time 735.0, rides 140\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3889, reward 902.0, memory_length 2000, epsilon 0.030119683143540073, time 729.0, rides 137\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 3890, reward 928.0, memory_length 2000, epsilon 0.030092575428710886, time 729.0, rides 134\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 3891, reward 820.0, memory_length 2000, epsilon 0.030065492110825046, time 725.0, rides 148\n",
      "Initial State is  [1, 0, 2]\n",
      "episode 3892, reward 666.0, memory_length 2000, epsilon 0.030038433167925302, time 727.0, rides 129\n",
      "Initial State is  [0, 2, 1]\n",
      "episode 3893, reward 813.0, memory_length 2000, epsilon 0.03001139857807417, time 731.0, rides 148\n",
      "Initial State is  [4, 19, 5]\n",
      "episode 3894, reward 814.0, memory_length 2000, epsilon 0.0299843883193539, time 726.0, rides 129\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 3895, reward 1025.0, memory_length 2000, epsilon 0.029957402369866482, time 723.0, rides 136\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 3896, reward 810.0, memory_length 2000, epsilon 0.029930440707733603, time 734.0, rides 123\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 3897, reward 859.0, memory_length 2000, epsilon 0.029903503311096643, time 731.0, rides 131\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 3898, reward 807.0, memory_length 2000, epsilon 0.029876590158116657, time 726.0, rides 139\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 3899, reward 777.0, memory_length 2000, epsilon 0.029849701226974352, time 727.0, rides 136\n",
      "Initial State is  [1, 16, 4]\n",
      "episode 3900, reward 645.0, memory_length 2000, epsilon 0.029822836495870076, time 729.0, rides 130\n",
      "Initial State is  [2, 19, 1]\n",
      "episode 3901, reward 984.0, memory_length 2000, epsilon 0.029795995943023793, time 729.0, rides 147\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 3902, reward 579.0, memory_length 2000, epsilon 0.029769179546675073, time 735.0, rides 139\n",
      "Initial State is  [0, 8, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3903, reward 789.0, memory_length 2000, epsilon 0.029742387285083063, time 727.0, rides 141\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 3904, reward 1060.0, memory_length 2000, epsilon 0.029715619136526487, time 729.0, rides 128\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 3905, reward 1035.0, memory_length 2000, epsilon 0.029688875079303612, time 725.0, rides 148\n",
      "Initial State is  [3, 12, 6]\n",
      "episode 3906, reward 869.0, memory_length 2000, epsilon 0.02966215509173224, time 723.0, rides 138\n",
      "Initial State is  [1, 7, 6]\n",
      "episode 3907, reward 864.0, memory_length 2000, epsilon 0.02963545915214968, time 724.0, rides 137\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 3908, reward 999.0, memory_length 2000, epsilon 0.029608787238912745, time 729.0, rides 129\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 3909, reward 642.0, memory_length 2000, epsilon 0.029582139330397723, time 736.0, rides 135\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 3910, reward 788.0, memory_length 2000, epsilon 0.029555515405000364, time 725.0, rides 141\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 3911, reward 1021.0, memory_length 2000, epsilon 0.029528915441135863, time 730.0, rides 140\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 3912, reward 478.0, memory_length 2000, epsilon 0.02950233941723884, time 726.0, rides 126\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 3913, reward 988.0, memory_length 2000, epsilon 0.029475787311763323, time 731.0, rides 133\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 3914, reward 1019.0, memory_length 2000, epsilon 0.029449259103182735, time 727.0, rides 145\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 3915, reward 1013.0, memory_length 2000, epsilon 0.02942275476998987, time 730.0, rides 124\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 3916, reward 724.0, memory_length 2000, epsilon 0.02939627429069688, time 724.0, rides 158\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3917, reward 776.0, memory_length 2000, epsilon 0.029369817643835252, time 720.0, rides 152\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 3918, reward 934.0, memory_length 2000, epsilon 0.0293433848079558, time 733.0, rides 130\n",
      "Initial State is  [0, 2, 0]\n",
      "episode 3919, reward 864.0, memory_length 2000, epsilon 0.02931697576162864, time 727.0, rides 130\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 3920, reward 988.0, memory_length 2000, epsilon 0.029290590483443176, time 730.0, rides 128\n",
      "Initial State is  [4, 2, 5]\n",
      "episode 3921, reward 1108.0, memory_length 2000, epsilon 0.029264228952008076, time 723.0, rides 137\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 3922, reward 624.0, memory_length 2000, epsilon 0.02923789114595127, time 727.0, rides 131\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 3923, reward 1069.0, memory_length 2000, epsilon 0.029211577043919915, time 724.0, rides 137\n",
      "Initial State is  [1, 23, 1]\n",
      "episode 3924, reward 855.0, memory_length 2000, epsilon 0.02918528662458039, time 729.0, rides 130\n",
      "Initial State is  [3, 11, 6]\n",
      "episode 3925, reward 786.0, memory_length 2000, epsilon 0.029159019866618265, time 722.0, rides 138\n",
      "Initial State is  [4, 10, 1]\n",
      "episode 3926, reward 759.0, memory_length 2000, epsilon 0.029132776748738307, time 736.0, rides 156\n",
      "Initial State is  [3, 10, 0]\n",
      "episode 3927, reward 452.0, memory_length 2000, epsilon 0.02910655724966444, time 725.0, rides 131\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 3928, reward 685.0, memory_length 2000, epsilon 0.029080361348139742, time 724.0, rides 136\n",
      "Initial State is  [1, 9, 0]\n",
      "episode 3929, reward 587.0, memory_length 2000, epsilon 0.029054189022926415, time 729.0, rides 140\n",
      "Initial State is  [3, 17, 0]\n",
      "episode 3930, reward 892.0, memory_length 2000, epsilon 0.02902804025280578, time 725.0, rides 149\n",
      "Initial State is  [0, 15, 3]\n",
      "episode 3931, reward 641.0, memory_length 2000, epsilon 0.029001915016578256, time 729.0, rides 131\n",
      "Initial State is  [4, 20, 6]\n",
      "episode 3932, reward 715.0, memory_length 2000, epsilon 0.028975813293063334, time 724.0, rides 143\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 3933, reward 1185.0, memory_length 2000, epsilon 0.028949735061099578, time 723.0, rides 132\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 3934, reward 772.0, memory_length 2000, epsilon 0.028923680299544587, time 727.0, rides 130\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 3935, reward 799.0, memory_length 2000, epsilon 0.028897648987274996, time 729.0, rides 144\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 3936, reward 918.0, memory_length 2000, epsilon 0.02887164110318645, time 734.0, rides 137\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 3937, reward 642.0, memory_length 2000, epsilon 0.02884565662619358, time 725.0, rides 137\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 3938, reward 748.0, memory_length 2000, epsilon 0.028819695535230005, time 724.0, rides 136\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 3939, reward 1141.0, memory_length 2000, epsilon 0.028793757809248297, time 733.0, rides 136\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 3940, reward 590.0, memory_length 2000, epsilon 0.028767843427219972, time 728.0, rides 126\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 3941, reward 727.0, memory_length 2000, epsilon 0.028741952368135475, time 731.0, rides 139\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 3942, reward 893.0, memory_length 2000, epsilon 0.028716084611004153, time 734.0, rides 143\n",
      "Initial State is  [2, 18, 3]\n",
      "episode 3943, reward 492.0, memory_length 2000, epsilon 0.028690240134854248, time 728.0, rides 138\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 3944, reward 611.0, memory_length 2000, epsilon 0.02866441891873288, time 728.0, rides 129\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 3945, reward 948.0, memory_length 2000, epsilon 0.02863862094170602, time 730.0, rides 127\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 3946, reward 1071.0, memory_length 2000, epsilon 0.028612846182858483, time 732.0, rides 140\n",
      "Initial State is  [0, 6, 1]\n",
      "episode 3947, reward 734.0, memory_length 2000, epsilon 0.02858709462129391, time 732.0, rides 134\n",
      "Initial State is  [3, 20, 2]\n",
      "episode 3948, reward 675.0, memory_length 2000, epsilon 0.028561366236134745, time 729.0, rides 134\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 3949, reward 824.0, memory_length 2000, epsilon 0.028535661006522224, time 725.0, rides 141\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 3950, reward 1058.0, memory_length 2000, epsilon 0.028509978911616354, time 726.0, rides 145\n",
      "Initial State is  [0, 3, 2]\n",
      "episode 3951, reward 887.0, memory_length 2000, epsilon 0.0284843199305959, time 731.0, rides 139\n",
      "Initial State is  [2, 9, 6]\n",
      "episode 3952, reward 582.0, memory_length 2000, epsilon 0.028458684042658364, time 728.0, rides 126\n",
      "Initial State is  [4, 15, 0]\n",
      "episode 3953, reward 655.0, memory_length 2000, epsilon 0.028433071227019973, time 726.0, rides 145\n",
      "Initial State is  [4, 5, 0]\n",
      "episode 3954, reward 958.0, memory_length 2000, epsilon 0.028407481462915656, time 723.0, rides 132\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 3955, reward 753.0, memory_length 2000, epsilon 0.02838191472959903, time 726.0, rides 145\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 3956, reward 636.0, memory_length 2000, epsilon 0.028356371006342394, time 730.0, rides 131\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 3957, reward 659.0, memory_length 2000, epsilon 0.028330850272436685, time 729.0, rides 137\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 3958, reward 893.0, memory_length 2000, epsilon 0.028305352507191493, time 729.0, rides 126\n",
      "Initial State is  [1, 18, 6]\n",
      "episode 3959, reward 1237.0, memory_length 2000, epsilon 0.02827987768993502, time 727.0, rides 126\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 3960, reward 925.0, memory_length 2000, epsilon 0.02825442580001408, time 732.0, rides 124\n",
      "Initial State is  [1, 22, 3]\n",
      "episode 3961, reward 750.0, memory_length 2000, epsilon 0.028228996816794066, time 725.0, rides 134\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 3962, reward 987.0, memory_length 2000, epsilon 0.02820359071965895, time 722.0, rides 145\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 3963, reward 918.0, memory_length 2000, epsilon 0.028178207488011257, time 728.0, rides 127\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 3964, reward 907.0, memory_length 2000, epsilon 0.028152847101272045, time 731.0, rides 148\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 3965, reward 682.0, memory_length 2000, epsilon 0.0281275095388809, time 722.0, rides 130\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 3966, reward 520.0, memory_length 2000, epsilon 0.028102194780295908, time 725.0, rides 134\n",
      "Initial State is  [3, 12, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3967, reward 687.0, memory_length 2000, epsilon 0.028076902804993642, time 722.0, rides 132\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 3968, reward 1011.0, memory_length 2000, epsilon 0.028051633592469146, time 736.0, rides 149\n",
      "Initial State is  [2, 5, 5]\n",
      "episode 3969, reward 922.0, memory_length 2000, epsilon 0.028026387122235923, time 726.0, rides 127\n",
      "Initial State is  [0, 9, 3]\n",
      "episode 3970, reward 792.0, memory_length 2000, epsilon 0.02800116337382591, time 727.0, rides 135\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 3971, reward 872.0, memory_length 2000, epsilon 0.027975962326789467, time 728.0, rides 123\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 3972, reward 617.0, memory_length 2000, epsilon 0.027950783960695356, time 732.0, rides 136\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 3973, reward 655.0, memory_length 2000, epsilon 0.02792562825513073, time 729.0, rides 128\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 3974, reward 815.0, memory_length 2000, epsilon 0.027900495189701113, time 725.0, rides 127\n",
      "Initial State is  [0, 17, 1]\n",
      "episode 3975, reward 704.0, memory_length 2000, epsilon 0.027875384744030382, time 727.0, rides 128\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 3976, reward 721.0, memory_length 2000, epsilon 0.027850296897760755, time 738.0, rides 127\n",
      "Initial State is  [3, 17, 3]\n",
      "episode 3977, reward 853.0, memory_length 2000, epsilon 0.02782523163055277, time 730.0, rides 127\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 3978, reward 676.0, memory_length 2000, epsilon 0.02780018892208527, time 731.0, rides 126\n",
      "Initial State is  [2, 14, 5]\n",
      "episode 3979, reward 660.0, memory_length 2000, epsilon 0.027775168752055393, time 727.0, rides 125\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 3980, reward 563.0, memory_length 2000, epsilon 0.027750171100178543, time 732.0, rides 119\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 3981, reward 756.0, memory_length 2000, epsilon 0.027725195946188382, time 728.0, rides 120\n",
      "Initial State is  [4, 11, 3]\n",
      "episode 3982, reward 1089.0, memory_length 2000, epsilon 0.027700243269836812, time 728.0, rides 135\n",
      "Initial State is  [3, 8, 1]\n",
      "episode 3983, reward 894.0, memory_length 2000, epsilon 0.02767531305089396, time 728.0, rides 137\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 3984, reward 810.0, memory_length 2000, epsilon 0.027650405269148155, time 732.0, rides 134\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 3985, reward 1037.0, memory_length 2000, epsilon 0.02762551990440592, time 730.0, rides 120\n",
      "Initial State is  [4, 14, 1]\n",
      "episode 3986, reward 724.0, memory_length 2000, epsilon 0.027600656936491955, time 723.0, rides 146\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 3987, reward 921.0, memory_length 2000, epsilon 0.02757581634524911, time 723.0, rides 135\n",
      "Initial State is  [0, 8, 0]\n",
      "episode 3988, reward 926.0, memory_length 2000, epsilon 0.027550998110538384, time 727.0, rides 135\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 3989, reward 1041.0, memory_length 2000, epsilon 0.0275262022122389, time 735.0, rides 133\n",
      "Initial State is  [2, 7, 6]\n",
      "episode 3990, reward 890.0, memory_length 2000, epsilon 0.027501428630247883, time 730.0, rides 136\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 3991, reward 892.0, memory_length 2000, epsilon 0.02747667734448066, time 725.0, rides 140\n",
      "Initial State is  [0, 17, 4]\n",
      "episode 3992, reward 822.0, memory_length 2000, epsilon 0.027451948334870628, time 729.0, rides 149\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 3993, reward 759.0, memory_length 2000, epsilon 0.027427241581369242, time 732.0, rides 134\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 3994, reward 835.0, memory_length 2000, epsilon 0.02740255706394601, time 726.0, rides 134\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 3995, reward 861.0, memory_length 2000, epsilon 0.027377894762588458, time 733.0, rides 134\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 3996, reward 1153.0, memory_length 2000, epsilon 0.027353254657302126, time 728.0, rides 141\n",
      "Initial State is  [0, 15, 5]\n",
      "episode 3997, reward 683.0, memory_length 2000, epsilon 0.027328636728110554, time 732.0, rides 151\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 3998, reward 1243.0, memory_length 2000, epsilon 0.027304040955055255, time 731.0, rides 137\n",
      "Initial State is  [1, 4, 5]\n",
      "episode 3999, reward 870.0, memory_length 2000, epsilon 0.027279467318195704, time 730.0, rides 136\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 4000, reward 959.0, memory_length 2000, epsilon 0.027254915797609327, time 727.0, rides 144\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 4001, reward 928.0, memory_length 2000, epsilon 0.02723038637339148, time 737.0, rides 149\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 4002, reward 783.0, memory_length 2000, epsilon 0.027205879025655428, time 724.0, rides 143\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 4003, reward 850.0, memory_length 2000, epsilon 0.02718139373453234, time 726.0, rides 127\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 4004, reward 681.0, memory_length 2000, epsilon 0.02715693048017126, time 727.0, rides 127\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 4005, reward 1014.0, memory_length 2000, epsilon 0.027132489242739106, time 727.0, rides 151\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 4006, reward 998.0, memory_length 2000, epsilon 0.02710807000242064, time 728.0, rides 136\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 4007, reward 805.0, memory_length 2000, epsilon 0.027083672739418464, time 721.0, rides 138\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 4008, reward 741.0, memory_length 2000, epsilon 0.027059297433952988, time 726.0, rides 132\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4009, reward 961.0, memory_length 2000, epsilon 0.02703494406626243, time 724.0, rides 124\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 4010, reward 877.0, memory_length 2000, epsilon 0.027010612616602796, time 731.0, rides 137\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 4011, reward 621.0, memory_length 2000, epsilon 0.026986303065247852, time 731.0, rides 130\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 4012, reward 771.0, memory_length 2000, epsilon 0.026962015392489127, time 726.0, rides 129\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 4013, reward 969.0, memory_length 2000, epsilon 0.026937749578635886, time 730.0, rides 139\n",
      "Initial State is  [2, 9, 4]\n",
      "episode 4014, reward 919.0, memory_length 2000, epsilon 0.026913505604015113, time 725.0, rides 148\n",
      "Initial State is  [1, 14, 0]\n",
      "episode 4015, reward 628.0, memory_length 2000, epsilon 0.0268892834489715, time 734.0, rides 134\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 4016, reward 1217.0, memory_length 2000, epsilon 0.026865083093867426, time 729.0, rides 133\n",
      "Initial State is  [4, 6, 2]\n",
      "episode 4017, reward 783.0, memory_length 2000, epsilon 0.026840904519082946, time 726.0, rides 125\n",
      "Initial State is  [4, 21, 2]\n",
      "episode 4018, reward 852.0, memory_length 2000, epsilon 0.02681674770501577, time 729.0, rides 140\n",
      "Initial State is  [4, 21, 0]\n",
      "episode 4019, reward 683.0, memory_length 2000, epsilon 0.026792612632081256, time 733.0, rides 133\n",
      "Initial State is  [3, 22, 6]\n",
      "episode 4020, reward 809.0, memory_length 2000, epsilon 0.02676849928071238, time 731.0, rides 145\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 4021, reward 801.0, memory_length 2000, epsilon 0.02674440763135974, time 724.0, rides 122\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 4022, reward 862.0, memory_length 2000, epsilon 0.026720337664491518, time 724.0, rides 120\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 4023, reward 408.0, memory_length 2000, epsilon 0.026696289360593473, time 730.0, rides 133\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 4024, reward 853.0, memory_length 2000, epsilon 0.02667226270016894, time 730.0, rides 123\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 4025, reward 1009.0, memory_length 2000, epsilon 0.026648257663738788, time 735.0, rides 146\n",
      "Initial State is  [2, 12, 4]\n",
      "episode 4026, reward 830.0, memory_length 2000, epsilon 0.02662427423184142, time 728.0, rides 131\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 4027, reward 809.0, memory_length 2000, epsilon 0.026600312385032764, time 723.0, rides 140\n",
      "Initial State is  [4, 8, 4]\n",
      "episode 4028, reward 689.0, memory_length 2000, epsilon 0.026576372103886234, time 731.0, rides 150\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 4029, reward 939.0, memory_length 2000, epsilon 0.026552453368992736, time 724.0, rides 128\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 4030, reward 996.0, memory_length 2000, epsilon 0.026528556160960642, time 725.0, rides 130\n",
      "Initial State is  [0, 0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4031, reward 1393.0, memory_length 2000, epsilon 0.026504680460415778, time 730.0, rides 128\n",
      "Initial State is  [4, 4, 5]\n",
      "episode 4032, reward 873.0, memory_length 2000, epsilon 0.026480826248001403, time 734.0, rides 126\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 4033, reward 674.0, memory_length 2000, epsilon 0.0264569935043782, time 732.0, rides 132\n",
      "Initial State is  [3, 13, 3]\n",
      "episode 4034, reward 671.0, memory_length 2000, epsilon 0.02643318221022426, time 729.0, rides 132\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 4035, reward 902.0, memory_length 2000, epsilon 0.02640939234623506, time 726.0, rides 133\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 4036, reward 1381.0, memory_length 2000, epsilon 0.026385623893123447, time 729.0, rides 138\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 4037, reward 712.0, memory_length 2000, epsilon 0.026361876831619637, time 728.0, rides 140\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 4038, reward 859.0, memory_length 2000, epsilon 0.026338151142471178, time 725.0, rides 131\n",
      "Initial State is  [0, 0, 6]\n",
      "episode 4039, reward 851.0, memory_length 2000, epsilon 0.026314446806442952, time 731.0, rides 129\n",
      "Initial State is  [1, 17, 3]\n",
      "episode 4040, reward 658.0, memory_length 2000, epsilon 0.026290763804317153, time 726.0, rides 124\n",
      "Initial State is  [3, 20, 0]\n",
      "episode 4041, reward 826.0, memory_length 2000, epsilon 0.026267102116893266, time 727.0, rides 137\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4042, reward 1172.0, memory_length 2000, epsilon 0.026243461724988062, time 734.0, rides 145\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 4043, reward 961.0, memory_length 2000, epsilon 0.02621984260943557, time 723.0, rides 143\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 4044, reward 767.0, memory_length 2000, epsilon 0.02619624475108708, time 730.0, rides 127\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 4045, reward 540.0, memory_length 2000, epsilon 0.0261726681308111, time 723.0, rides 137\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 4046, reward 875.0, memory_length 2000, epsilon 0.02614911272949337, time 724.0, rides 129\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 4047, reward 447.0, memory_length 2000, epsilon 0.026125578528036826, time 726.0, rides 129\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 4048, reward 734.0, memory_length 2000, epsilon 0.02610206550736159, time 733.0, rides 126\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 4049, reward 1068.0, memory_length 2000, epsilon 0.026078573648404966, time 734.0, rides 145\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 4050, reward 750.0, memory_length 2000, epsilon 0.0260551029321214, time 722.0, rides 130\n",
      "Initial State is  [1, 16, 5]\n",
      "episode 4051, reward 730.0, memory_length 2000, epsilon 0.026031653339482493, time 726.0, rides 136\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4052, reward 526.0, memory_length 2000, epsilon 0.02600822485147696, time 723.0, rides 135\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 4053, reward 938.0, memory_length 2000, epsilon 0.025984817449110627, time 723.0, rides 145\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 4054, reward 759.0, memory_length 2000, epsilon 0.025961431113406427, time 729.0, rides 132\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 4055, reward 740.0, memory_length 2000, epsilon 0.02593806582540436, time 727.0, rides 135\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 4056, reward 911.0, memory_length 2000, epsilon 0.025914721566161494, time 732.0, rides 131\n",
      "Initial State is  [0, 13, 6]\n",
      "episode 4057, reward 892.0, memory_length 2000, epsilon 0.025891398316751947, time 733.0, rides 136\n",
      "Initial State is  [2, 1, 0]\n",
      "episode 4058, reward 610.0, memory_length 2000, epsilon 0.02586809605826687, time 726.0, rides 124\n",
      "Initial State is  [4, 4, 0]\n",
      "episode 4059, reward 689.0, memory_length 2000, epsilon 0.02584481477181443, time 730.0, rides 134\n",
      "Initial State is  [1, 23, 4]\n",
      "episode 4060, reward 997.0, memory_length 2000, epsilon 0.025821554438519797, time 729.0, rides 126\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 4061, reward 906.0, memory_length 2000, epsilon 0.02579831503952513, time 724.0, rides 139\n",
      "Initial State is  [3, 12, 4]\n",
      "episode 4062, reward 739.0, memory_length 2000, epsilon 0.025775096555989554, time 732.0, rides 139\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 4063, reward 941.0, memory_length 2000, epsilon 0.025751898969089162, time 730.0, rides 126\n",
      "Initial State is  [4, 4, 3]\n",
      "episode 4064, reward 692.0, memory_length 2000, epsilon 0.025728722260016983, time 723.0, rides 142\n",
      "Initial State is  [2, 23, 6]\n",
      "episode 4065, reward 826.0, memory_length 2000, epsilon 0.025705566409982967, time 727.0, rides 143\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 4066, reward 1055.0, memory_length 2000, epsilon 0.025682431400213982, time 730.0, rides 138\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 4067, reward 800.0, memory_length 2000, epsilon 0.02565931721195379, time 739.0, rides 144\n",
      "Initial State is  [4, 2, 4]\n",
      "episode 4068, reward 677.0, memory_length 2000, epsilon 0.02563622382646303, time 730.0, rides 141\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 4069, reward 835.0, memory_length 2000, epsilon 0.025613151225019212, time 725.0, rides 124\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 4070, reward 744.0, memory_length 2000, epsilon 0.025590099388916696, time 730.0, rides 147\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 4071, reward 1089.0, memory_length 2000, epsilon 0.02556706829946667, time 728.0, rides 134\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 4072, reward 795.0, memory_length 2000, epsilon 0.025544057937997147, time 737.0, rides 132\n",
      "Initial State is  [2, 9, 1]\n",
      "episode 4073, reward 708.0, memory_length 2000, epsilon 0.02552106828585295, time 730.0, rides 139\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 4074, reward 807.0, memory_length 2000, epsilon 0.02549809932439568, time 729.0, rides 126\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 4075, reward 630.0, memory_length 2000, epsilon 0.025475151035003724, time 729.0, rides 135\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 4076, reward 523.0, memory_length 2000, epsilon 0.02545222339907222, time 737.0, rides 149\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 4077, reward 547.0, memory_length 2000, epsilon 0.025429316398013053, time 723.0, rides 137\n",
      "Initial State is  [0, 5, 0]\n",
      "episode 4078, reward 747.0, memory_length 2000, epsilon 0.025406430013254842, time 729.0, rides 141\n",
      "Initial State is  [1, 13, 1]\n",
      "episode 4079, reward 802.0, memory_length 2000, epsilon 0.025383564226242914, time 723.0, rides 144\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 4080, reward 499.0, memory_length 2000, epsilon 0.025360719018439296, time 730.0, rides 136\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 4081, reward 445.0, memory_length 2000, epsilon 0.0253378943713227, time 736.0, rides 138\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 4082, reward 1157.0, memory_length 2000, epsilon 0.02531509026638851, time 729.0, rides 139\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 4083, reward 807.0, memory_length 2000, epsilon 0.02529230668514876, time 730.0, rides 133\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 4084, reward 653.0, memory_length 2000, epsilon 0.02526954360913213, time 731.0, rides 130\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 4085, reward 1000.0, memory_length 2000, epsilon 0.02524680101988391, time 738.0, rides 138\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 4086, reward 526.0, memory_length 2000, epsilon 0.025224078898966013, time 729.0, rides 131\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4087, reward 773.0, memory_length 2000, epsilon 0.025201377227956942, time 731.0, rides 130\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 4088, reward 907.0, memory_length 2000, epsilon 0.02517869598845178, time 734.0, rides 128\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 4089, reward 1118.0, memory_length 2000, epsilon 0.025156035162062173, time 725.0, rides 150\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 4090, reward 1031.0, memory_length 2000, epsilon 0.025133394730416318, time 727.0, rides 162\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4091, reward 828.0, memory_length 2000, epsilon 0.02511077467515894, time 731.0, rides 146\n",
      "Initial State is  [3, 14, 1]\n",
      "episode 4092, reward 1028.0, memory_length 2000, epsilon 0.025088174977951298, time 726.0, rides 151\n",
      "Initial State is  [3, 18, 4]\n",
      "episode 4093, reward 969.0, memory_length 2000, epsilon 0.025065595620471143, time 730.0, rides 129\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 4094, reward 957.0, memory_length 2000, epsilon 0.025043036584412717, time 734.0, rides 132\n",
      "Initial State is  [1, 23, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4095, reward 515.0, memory_length 2000, epsilon 0.025020497851486745, time 729.0, rides 139\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 4096, reward 1061.0, memory_length 2000, epsilon 0.024997979403420408, time 725.0, rides 144\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 4097, reward 891.0, memory_length 2000, epsilon 0.02497548122195733, time 728.0, rides 130\n",
      "Initial State is  [4, 7, 3]\n",
      "episode 4098, reward 537.0, memory_length 2000, epsilon 0.024953003288857568, time 741.0, rides 145\n",
      "Initial State is  [2, 1, 6]\n",
      "episode 4099, reward 779.0, memory_length 2000, epsilon 0.024930545585897596, time 736.0, rides 135\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 4100, reward 849.0, memory_length 2000, epsilon 0.024908108094870287, time 724.0, rides 141\n",
      "Initial State is  [0, 11, 2]\n",
      "episode 4101, reward 633.0, memory_length 2000, epsilon 0.024885690797584903, time 729.0, rides 146\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 4102, reward 732.0, memory_length 2000, epsilon 0.024863293675867076, time 731.0, rides 134\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 4103, reward 866.0, memory_length 2000, epsilon 0.024840916711558796, time 723.0, rides 138\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 4104, reward 665.0, memory_length 2000, epsilon 0.024818559886518394, time 735.0, rides 153\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 4105, reward 715.0, memory_length 2000, epsilon 0.024796223182620526, time 731.0, rides 138\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 4106, reward 875.0, memory_length 2000, epsilon 0.024773906581756166, time 727.0, rides 126\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 4107, reward 1031.0, memory_length 2000, epsilon 0.024751610065832586, time 722.0, rides 149\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 4108, reward 796.0, memory_length 2000, epsilon 0.024729333616773336, time 724.0, rides 133\n",
      "Initial State is  [0, 18, 2]\n",
      "episode 4109, reward 896.0, memory_length 2000, epsilon 0.024707077216518242, time 726.0, rides 148\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 4110, reward 953.0, memory_length 2000, epsilon 0.024684840847023375, time 732.0, rides 129\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 4111, reward 768.0, memory_length 2000, epsilon 0.02466262449026105, time 723.0, rides 142\n",
      "Initial State is  [0, 2, 5]\n",
      "episode 4112, reward 763.0, memory_length 2000, epsilon 0.024640428128219816, time 732.0, rides 145\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 4113, reward 990.0, memory_length 2000, epsilon 0.02461825174290442, time 724.0, rides 140\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 4114, reward 1015.0, memory_length 2000, epsilon 0.024596095316335807, time 725.0, rides 136\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 4115, reward 874.0, memory_length 2000, epsilon 0.024573958830551103, time 723.0, rides 152\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 4116, reward 903.0, memory_length 2000, epsilon 0.024551842267603607, time 729.0, rides 139\n",
      "Initial State is  [1, 2, 3]\n",
      "episode 4117, reward 646.0, memory_length 2000, epsilon 0.024529745609562763, time 735.0, rides 130\n",
      "Initial State is  [1, 7, 0]\n",
      "episode 4118, reward 1113.0, memory_length 2000, epsilon 0.024507668838514157, time 720.0, rides 142\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 4119, reward 1161.0, memory_length 2000, epsilon 0.024485611936559494, time 727.0, rides 148\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 4120, reward 678.0, memory_length 2000, epsilon 0.02446357488581659, time 727.0, rides 145\n",
      "Initial State is  [0, 10, 5]\n",
      "episode 4121, reward 624.0, memory_length 2000, epsilon 0.024441557668419354, time 728.0, rides 131\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4122, reward 924.0, memory_length 2000, epsilon 0.024419560266517776, time 723.0, rides 149\n",
      "Initial State is  [0, 11, 6]\n",
      "episode 4123, reward 897.0, memory_length 2000, epsilon 0.02439758266227791, time 725.0, rides 134\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 4124, reward 694.0, memory_length 2000, epsilon 0.02437562483788186, time 730.0, rides 141\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 4125, reward 829.0, memory_length 2000, epsilon 0.024353686775527766, time 731.0, rides 144\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 4126, reward 887.0, memory_length 2000, epsilon 0.024331768457429792, time 726.0, rides 134\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 4127, reward 659.0, memory_length 2000, epsilon 0.024309869865818106, time 731.0, rides 128\n",
      "Initial State is  [3, 7, 2]\n",
      "episode 4128, reward 686.0, memory_length 2000, epsilon 0.024287990982938868, time 727.0, rides 139\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 4129, reward 1038.0, memory_length 2000, epsilon 0.024266131791054222, time 729.0, rides 123\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 4130, reward 842.0, memory_length 2000, epsilon 0.024244292272442274, time 727.0, rides 135\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 4131, reward 926.0, memory_length 2000, epsilon 0.024222472409397077, time 726.0, rides 146\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 4132, reward 929.0, memory_length 2000, epsilon 0.024200672184228618, time 732.0, rides 143\n",
      "Initial State is  [3, 13, 6]\n",
      "episode 4133, reward 941.0, memory_length 2000, epsilon 0.024178891579262812, time 723.0, rides 137\n",
      "Initial State is  [3, 22, 5]\n",
      "episode 4134, reward 1037.0, memory_length 2000, epsilon 0.024157130576841476, time 736.0, rides 137\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 4135, reward 986.0, memory_length 2000, epsilon 0.02413538915932232, time 727.0, rides 139\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 4136, reward 1003.0, memory_length 2000, epsilon 0.024113667309078927, time 728.0, rides 149\n",
      "Initial State is  [4, 1, 2]\n",
      "episode 4137, reward 831.0, memory_length 2000, epsilon 0.024091965008500756, time 732.0, rides 139\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 4138, reward 749.0, memory_length 2000, epsilon 0.024070282239993104, time 732.0, rides 140\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 4139, reward 605.0, memory_length 2000, epsilon 0.02404861898597711, time 725.0, rides 137\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 4140, reward 972.0, memory_length 2000, epsilon 0.02402697522888973, time 727.0, rides 144\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 4141, reward 859.0, memory_length 2000, epsilon 0.024005350951183727, time 729.0, rides 153\n",
      "Initial State is  [3, 19, 2]\n",
      "episode 4142, reward 700.0, memory_length 2000, epsilon 0.023983746135327663, time 729.0, rides 130\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4143, reward 1021.0, memory_length 2000, epsilon 0.02396216076380587, time 730.0, rides 141\n",
      "Initial State is  [2, 21, 4]\n",
      "episode 4144, reward 819.0, memory_length 2000, epsilon 0.023940594819118442, time 735.0, rides 154\n",
      "Initial State is  [2, 6, 4]\n",
      "episode 4145, reward 859.0, memory_length 2000, epsilon 0.023919048283781236, time 729.0, rides 127\n",
      "Initial State is  [3, 20, 6]\n",
      "episode 4146, reward 913.0, memory_length 2000, epsilon 0.023897521140325832, time 731.0, rides 151\n",
      "Initial State is  [0, 20, 4]\n",
      "episode 4147, reward 798.0, memory_length 2000, epsilon 0.023876013371299538, time 723.0, rides 129\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4148, reward 790.0, memory_length 2000, epsilon 0.023854524959265367, time 734.0, rides 140\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4149, reward 869.0, memory_length 2000, epsilon 0.023833055886802026, time 736.0, rides 135\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4150, reward 1042.0, memory_length 2000, epsilon 0.023811606136503904, time 732.0, rides 138\n",
      "Initial State is  [3, 23, 6]\n",
      "episode 4151, reward 866.0, memory_length 2000, epsilon 0.02379017569098105, time 726.0, rides 124\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 4152, reward 817.0, memory_length 2000, epsilon 0.023768764532859168, time 726.0, rides 136\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 4153, reward 840.0, memory_length 2000, epsilon 0.023747372644779594, time 733.0, rides 142\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 4154, reward 930.0, memory_length 2000, epsilon 0.02372600000939929, time 730.0, rides 150\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 4155, reward 851.0, memory_length 2000, epsilon 0.023704646609390832, time 733.0, rides 133\n",
      "Initial State is  [2, 8, 2]\n",
      "episode 4156, reward 766.0, memory_length 2000, epsilon 0.02368331242744238, time 734.0, rides 134\n",
      "Initial State is  [4, 0, 2]\n",
      "episode 4157, reward 1013.0, memory_length 2000, epsilon 0.023661997446257684, time 727.0, rides 127\n",
      "Initial State is  [1, 12, 4]\n",
      "episode 4158, reward 691.0, memory_length 2000, epsilon 0.023640701648556053, time 727.0, rides 138\n",
      "Initial State is  [0, 16, 6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4159, reward 933.0, memory_length 2000, epsilon 0.023619425017072353, time 731.0, rides 141\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 4160, reward 898.0, memory_length 2000, epsilon 0.02359816753455699, time 721.0, rides 139\n",
      "Initial State is  [3, 19, 1]\n",
      "episode 4161, reward 994.0, memory_length 2000, epsilon 0.023576929183775887, time 726.0, rides 144\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 4162, reward 806.0, memory_length 2000, epsilon 0.023555709947510488, time 725.0, rides 133\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 4163, reward 1016.0, memory_length 2000, epsilon 0.02353450980855773, time 729.0, rides 147\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 4164, reward 735.0, memory_length 2000, epsilon 0.023513328749730028, time 726.0, rides 143\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 4165, reward 752.0, memory_length 2000, epsilon 0.02349216675385527, time 727.0, rides 139\n",
      "Initial State is  [0, 6, 4]\n",
      "episode 4166, reward 601.0, memory_length 2000, epsilon 0.023471023803776803, time 733.0, rides 126\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 4167, reward 850.0, memory_length 2000, epsilon 0.023449899882353402, time 727.0, rides 133\n",
      "Initial State is  [1, 6, 1]\n",
      "episode 4168, reward 1142.0, memory_length 2000, epsilon 0.023428794972459283, time 729.0, rides 129\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4169, reward 832.0, memory_length 2000, epsilon 0.02340770905698407, time 737.0, rides 136\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 4170, reward 864.0, memory_length 2000, epsilon 0.023386642118832783, time 735.0, rides 143\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 4171, reward 901.0, memory_length 2000, epsilon 0.023365594140925833, time 731.0, rides 121\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 4172, reward 670.0, memory_length 2000, epsilon 0.023344565106199, time 725.0, rides 119\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 4173, reward 812.0, memory_length 2000, epsilon 0.02332355499760342, time 722.0, rides 147\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 4174, reward 912.0, memory_length 2000, epsilon 0.023302563798105577, time 722.0, rides 133\n",
      "Initial State is  [1, 18, 2]\n",
      "episode 4175, reward 1022.0, memory_length 2000, epsilon 0.02328159149068728, time 725.0, rides 134\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 4176, reward 702.0, memory_length 2000, epsilon 0.023260638058345662, time 728.0, rides 145\n",
      "Initial State is  [2, 21, 0]\n",
      "episode 4177, reward 997.0, memory_length 2000, epsilon 0.02323970348409315, time 727.0, rides 137\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 4178, reward 741.0, memory_length 2000, epsilon 0.023218787750957467, time 724.0, rides 128\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 4179, reward 881.0, memory_length 2000, epsilon 0.023197890841981605, time 734.0, rides 127\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 4180, reward 825.0, memory_length 2000, epsilon 0.02317701274022382, time 730.0, rides 129\n",
      "Initial State is  [2, 8, 5]\n",
      "episode 4181, reward 1014.0, memory_length 2000, epsilon 0.02315615342875762, time 723.0, rides 135\n",
      "Initial State is  [2, 18, 4]\n",
      "episode 4182, reward 856.0, memory_length 2000, epsilon 0.023135312890671736, time 731.0, rides 137\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4183, reward 982.0, memory_length 2000, epsilon 0.023114491109070132, time 737.0, rides 147\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 4184, reward 833.0, memory_length 2000, epsilon 0.02309368806707197, time 731.0, rides 125\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 4185, reward 1056.0, memory_length 2000, epsilon 0.023072903747811607, time 725.0, rides 145\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 4186, reward 828.0, memory_length 2000, epsilon 0.023052138134438575, time 723.0, rides 133\n",
      "Initial State is  [4, 16, 0]\n",
      "episode 4187, reward 759.0, memory_length 2000, epsilon 0.02303139121011758, time 730.0, rides 138\n",
      "Initial State is  [1, 2, 2]\n",
      "episode 4188, reward 1044.0, memory_length 2000, epsilon 0.023010662958028474, time 736.0, rides 138\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 4189, reward 1017.0, memory_length 2000, epsilon 0.022989953361366246, time 722.0, rides 145\n",
      "Initial State is  [2, 2, 0]\n",
      "episode 4190, reward 898.0, memory_length 2000, epsilon 0.022969262403341018, time 725.0, rides 138\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 4191, reward 895.0, memory_length 2000, epsilon 0.02294859006717801, time 733.0, rides 152\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 4192, reward 650.0, memory_length 2000, epsilon 0.02292793633611755, time 738.0, rides 149\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 4193, reward 963.0, memory_length 2000, epsilon 0.022907301193415042, time 732.0, rides 137\n",
      "Initial State is  [3, 20, 3]\n",
      "episode 4194, reward 572.0, memory_length 2000, epsilon 0.022886684622340968, time 731.0, rides 145\n",
      "Initial State is  [1, 17, 4]\n",
      "episode 4195, reward 702.0, memory_length 2000, epsilon 0.02286608660618086, time 738.0, rides 136\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 4196, reward 608.0, memory_length 2000, epsilon 0.0228455071282353, time 727.0, rides 147\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 4197, reward 684.0, memory_length 2000, epsilon 0.022824946171819887, time 730.0, rides 147\n",
      "Initial State is  [1, 12, 5]\n",
      "episode 4198, reward 939.0, memory_length 2000, epsilon 0.022804403720265248, time 727.0, rides 136\n",
      "Initial State is  [0, 16, 6]\n",
      "episode 4199, reward 909.0, memory_length 2000, epsilon 0.022783879756917008, time 732.0, rides 133\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 4200, reward 785.0, memory_length 2000, epsilon 0.022763374265135784, time 726.0, rides 144\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 4201, reward 1003.0, memory_length 2000, epsilon 0.022742887228297162, time 733.0, rides 131\n",
      "Initial State is  [3, 6, 4]\n",
      "episode 4202, reward 836.0, memory_length 2000, epsilon 0.022722418629791696, time 727.0, rides 127\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 4203, reward 1138.0, memory_length 2000, epsilon 0.022701968453024884, time 723.0, rides 137\n",
      "Initial State is  [0, 3, 5]\n",
      "episode 4204, reward 652.0, memory_length 2000, epsilon 0.02268153668141716, time 727.0, rides 135\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 4205, reward 823.0, memory_length 2000, epsilon 0.022661123298403887, time 730.0, rides 132\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4206, reward 862.0, memory_length 2000, epsilon 0.022640728287435324, time 724.0, rides 138\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 4207, reward 900.0, memory_length 2000, epsilon 0.02262035163197663, time 723.0, rides 155\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 4208, reward 853.0, memory_length 2000, epsilon 0.02259999331550785, time 728.0, rides 158\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 4209, reward 876.0, memory_length 2000, epsilon 0.022579653321523892, time 729.0, rides 153\n",
      "Initial State is  [3, 23, 5]\n",
      "episode 4210, reward 982.0, memory_length 2000, epsilon 0.022559331633534522, time 724.0, rides 146\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4211, reward 648.0, memory_length 2000, epsilon 0.022539028235064342, time 723.0, rides 146\n",
      "Initial State is  [0, 9, 5]\n",
      "episode 4212, reward 691.0, memory_length 2000, epsilon 0.022518743109652784, time 728.0, rides 150\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4213, reward 768.0, memory_length 2000, epsilon 0.022498476240854097, time 730.0, rides 135\n",
      "Initial State is  [1, 0, 3]\n",
      "episode 4214, reward 736.0, memory_length 2000, epsilon 0.022478227612237327, time 723.0, rides 128\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 4215, reward 912.0, memory_length 2000, epsilon 0.022457997207386313, time 731.0, rides 136\n",
      "Initial State is  [4, 6, 4]\n",
      "episode 4216, reward 908.0, memory_length 2000, epsilon 0.022437785009899666, time 729.0, rides 154\n",
      "Initial State is  [1, 19, 4]\n",
      "episode 4217, reward 933.0, memory_length 2000, epsilon 0.022417591003390757, time 724.0, rides 125\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 4218, reward 972.0, memory_length 2000, epsilon 0.022397415171487706, time 723.0, rides 143\n",
      "Initial State is  [2, 0, 1]\n",
      "episode 4219, reward 720.0, memory_length 2000, epsilon 0.022377257497833366, time 725.0, rides 141\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 4220, reward 602.0, memory_length 2000, epsilon 0.022357117966085315, time 727.0, rides 134\n",
      "Initial State is  [2, 16, 3]\n",
      "episode 4221, reward 883.0, memory_length 2000, epsilon 0.022336996559915837, time 725.0, rides 136\n",
      "Initial State is  [0, 23, 6]\n",
      "episode 4222, reward 677.0, memory_length 2000, epsilon 0.02231689326301191, time 732.0, rides 133\n",
      "Initial State is  [3, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4223, reward 837.0, memory_length 2000, epsilon 0.0222968080590752, time 725.0, rides 120\n",
      "Initial State is  [4, 4, 6]\n",
      "episode 4224, reward 772.0, memory_length 2000, epsilon 0.022276740931822032, time 727.0, rides 140\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 4225, reward 962.0, memory_length 2000, epsilon 0.022256691864983393, time 731.0, rides 130\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 4226, reward 798.0, memory_length 2000, epsilon 0.022236660842304908, time 723.0, rides 155\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4227, reward 1172.0, memory_length 2000, epsilon 0.022216647847546834, time 729.0, rides 144\n",
      "Initial State is  [4, 7, 5]\n",
      "episode 4228, reward 646.0, memory_length 2000, epsilon 0.02219665286448404, time 736.0, rides 128\n",
      "Initial State is  [3, 5, 6]\n",
      "episode 4229, reward 841.0, memory_length 2000, epsilon 0.022176675876906006, time 726.0, rides 140\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 4230, reward 888.0, memory_length 2000, epsilon 0.02215671686861679, time 727.0, rides 147\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 4231, reward 875.0, memory_length 2000, epsilon 0.022136775823435033, time 724.0, rides 136\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4232, reward 705.0, memory_length 2000, epsilon 0.022116852725193942, time 724.0, rides 132\n",
      "Initial State is  [4, 23, 6]\n",
      "episode 4233, reward 1034.0, memory_length 2000, epsilon 0.02209694755774127, time 729.0, rides 144\n",
      "Initial State is  [4, 19, 6]\n",
      "episode 4234, reward 992.0, memory_length 2000, epsilon 0.022077060304939302, time 720.0, rides 151\n",
      "Initial State is  [4, 7, 4]\n",
      "episode 4235, reward 1017.0, memory_length 2000, epsilon 0.022057190950664857, time 727.0, rides 145\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 4236, reward 569.0, memory_length 2000, epsilon 0.02203733947880926, time 728.0, rides 142\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 4237, reward 775.0, memory_length 2000, epsilon 0.02201750587327833, time 729.0, rides 138\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 4238, reward 799.0, memory_length 2000, epsilon 0.02199769011799238, time 729.0, rides 138\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 4239, reward 800.0, memory_length 2000, epsilon 0.021977892196886187, time 730.0, rides 150\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4240, reward 939.0, memory_length 2000, epsilon 0.02195811209390899, time 731.0, rides 139\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 4241, reward 1210.0, memory_length 2000, epsilon 0.021938349793024472, time 726.0, rides 144\n",
      "Initial State is  [3, 10, 6]\n",
      "episode 4242, reward 690.0, memory_length 2000, epsilon 0.02191860527821075, time 726.0, rides 140\n",
      "Initial State is  [4, 8, 6]\n",
      "episode 4243, reward 1004.0, memory_length 2000, epsilon 0.02189887853346036, time 724.0, rides 144\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 4244, reward 912.0, memory_length 2000, epsilon 0.021879169542780245, time 727.0, rides 152\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 4245, reward 828.0, memory_length 2000, epsilon 0.021859478290191744, time 727.0, rides 129\n",
      "Initial State is  [4, 15, 2]\n",
      "episode 4246, reward 995.0, memory_length 2000, epsilon 0.02183980475973057, time 726.0, rides 132\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 4247, reward 1153.0, memory_length 2000, epsilon 0.021820148935446815, time 727.0, rides 141\n",
      "Initial State is  [0, 4, 5]\n",
      "episode 4248, reward 707.0, memory_length 2000, epsilon 0.021800510801404913, time 731.0, rides 132\n",
      "Initial State is  [0, 3, 4]\n",
      "episode 4249, reward 919.0, memory_length 2000, epsilon 0.021780890341683647, time 724.0, rides 144\n",
      "Initial State is  [1, 0, 6]\n",
      "episode 4250, reward 816.0, memory_length 2000, epsilon 0.021761287540376133, time 729.0, rides 128\n",
      "Initial State is  [4, 20, 2]\n",
      "episode 4251, reward 834.0, memory_length 2000, epsilon 0.021741702381589796, time 727.0, rides 142\n",
      "Initial State is  [0, 8, 1]\n",
      "episode 4252, reward 964.0, memory_length 2000, epsilon 0.021722134849446365, time 726.0, rides 142\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4253, reward 869.0, memory_length 2000, epsilon 0.021702584928081862, time 732.0, rides 146\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 4254, reward 524.0, memory_length 2000, epsilon 0.021683052601646588, time 730.0, rides 157\n",
      "Initial State is  [4, 1, 5]\n",
      "episode 4255, reward 924.0, memory_length 2000, epsilon 0.021663537854305106, time 722.0, rides 138\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 4256, reward 778.0, memory_length 2000, epsilon 0.02164404067023623, time 728.0, rides 132\n",
      "Initial State is  [3, 0, 2]\n",
      "episode 4257, reward 902.0, memory_length 2000, epsilon 0.021624561033633017, time 733.0, rides 137\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 4258, reward 777.0, memory_length 2000, epsilon 0.021605098928702746, time 721.0, rides 147\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 4259, reward 1032.0, memory_length 2000, epsilon 0.021585654339666912, time 726.0, rides 125\n",
      "Initial State is  [0, 12, 1]\n",
      "episode 4260, reward 907.0, memory_length 2000, epsilon 0.02156622725076121, time 726.0, rides 157\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4261, reward 803.0, memory_length 2000, epsilon 0.021546817646235526, time 726.0, rides 134\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 4262, reward 1089.0, memory_length 2000, epsilon 0.021527425510353915, time 728.0, rides 140\n",
      "Initial State is  [0, 0, 1]\n",
      "episode 4263, reward 772.0, memory_length 2000, epsilon 0.021508050827394595, time 732.0, rides 140\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4264, reward 876.0, memory_length 2000, epsilon 0.02148869358164994, time 734.0, rides 123\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 4265, reward 514.0, memory_length 2000, epsilon 0.021469353757426455, time 729.0, rides 167\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 4266, reward 576.0, memory_length 2000, epsilon 0.02145003133904477, time 726.0, rides 135\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4267, reward 1003.0, memory_length 2000, epsilon 0.02143072631083963, time 737.0, rides 150\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 4268, reward 928.0, memory_length 2000, epsilon 0.021411438657159873, time 730.0, rides 142\n",
      "Initial State is  [2, 6, 6]\n",
      "episode 4269, reward 1001.0, memory_length 2000, epsilon 0.02139216836236843, time 727.0, rides 146\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 4270, reward 1088.0, memory_length 2000, epsilon 0.021372915410842297, time 729.0, rides 162\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 4271, reward 716.0, memory_length 2000, epsilon 0.02135367978697254, time 728.0, rides 143\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 4272, reward 686.0, memory_length 2000, epsilon 0.021334461475164265, time 731.0, rides 148\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 4273, reward 801.0, memory_length 2000, epsilon 0.021315260459836616, time 720.0, rides 133\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 4274, reward 939.0, memory_length 2000, epsilon 0.021296076725422764, time 724.0, rides 146\n",
      "Initial State is  [2, 0, 6]\n",
      "episode 4275, reward 853.0, memory_length 2000, epsilon 0.021276910256369883, time 722.0, rides 138\n",
      "Initial State is  [4, 20, 1]\n",
      "episode 4276, reward 883.0, memory_length 2000, epsilon 0.02125776103713915, time 733.0, rides 130\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 4277, reward 902.0, memory_length 2000, epsilon 0.021238629052205724, time 732.0, rides 134\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 4278, reward 822.0, memory_length 2000, epsilon 0.021219514286058738, time 731.0, rides 138\n",
      "Initial State is  [1, 6, 3]\n",
      "episode 4279, reward 800.0, memory_length 2000, epsilon 0.021200416723201283, time 729.0, rides 133\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 4280, reward 1195.0, memory_length 2000, epsilon 0.021181336348150403, time 720.0, rides 141\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 4281, reward 920.0, memory_length 2000, epsilon 0.021162273145437067, time 729.0, rides 142\n",
      "Initial State is  [0, 0, 5]\n",
      "episode 4282, reward 884.0, memory_length 2000, epsilon 0.021143227099606175, time 733.0, rides 129\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 4283, reward 843.0, memory_length 2000, epsilon 0.021124198195216527, time 721.0, rides 130\n",
      "Initial State is  [1, 8, 4]\n",
      "episode 4284, reward 954.0, memory_length 2000, epsilon 0.02110518641684083, time 728.0, rides 148\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 4285, reward 747.0, memory_length 2000, epsilon 0.021086191749065675, time 732.0, rides 137\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 4286, reward 865.0, memory_length 2000, epsilon 0.021067214176491517, time 728.0, rides 121\n",
      "Initial State is  [4, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4287, reward 721.0, memory_length 2000, epsilon 0.021048253683732674, time 723.0, rides 136\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 4288, reward 1014.0, memory_length 2000, epsilon 0.021029310255417315, time 729.0, rides 147\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 4289, reward 728.0, memory_length 2000, epsilon 0.02101038387618744, time 728.0, rides 136\n",
      "Initial State is  [1, 23, 6]\n",
      "episode 4290, reward 663.0, memory_length 2000, epsilon 0.02099147453069887, time 726.0, rides 141\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 4291, reward 723.0, memory_length 2000, epsilon 0.02097258220362124, time 736.0, rides 126\n",
      "Initial State is  [3, 16, 6]\n",
      "episode 4292, reward 621.0, memory_length 2000, epsilon 0.020953706879637983, time 724.0, rides 139\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4293, reward 950.0, memory_length 2000, epsilon 0.020934848543446308, time 727.0, rides 135\n",
      "Initial State is  [2, 17, 6]\n",
      "episode 4294, reward 781.0, memory_length 2000, epsilon 0.020916007179757206, time 724.0, rides 146\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 4295, reward 641.0, memory_length 2000, epsilon 0.020897182773295424, time 729.0, rides 145\n",
      "Initial State is  [3, 10, 1]\n",
      "episode 4296, reward 790.0, memory_length 2000, epsilon 0.02087837530879946, time 734.0, rides 131\n",
      "Initial State is  [0, 7, 1]\n",
      "episode 4297, reward 974.0, memory_length 2000, epsilon 0.02085958477102154, time 728.0, rides 128\n",
      "Initial State is  [1, 13, 4]\n",
      "episode 4298, reward 959.0, memory_length 2000, epsilon 0.02084081114472762, time 732.0, rides 146\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 4299, reward 897.0, memory_length 2000, epsilon 0.020822054414697363, time 727.0, rides 151\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 4300, reward 852.0, memory_length 2000, epsilon 0.020803314565724134, time 724.0, rides 127\n",
      "Initial State is  [3, 15, 6]\n",
      "episode 4301, reward 739.0, memory_length 2000, epsilon 0.020784591582614982, time 728.0, rides 143\n",
      "Initial State is  [1, 19, 3]\n",
      "episode 4302, reward 688.0, memory_length 2000, epsilon 0.02076588545019063, time 730.0, rides 131\n",
      "Initial State is  [2, 17, 1]\n",
      "episode 4303, reward 911.0, memory_length 2000, epsilon 0.020747196153285456, time 728.0, rides 141\n",
      "Initial State is  [0, 10, 4]\n",
      "episode 4304, reward 456.0, memory_length 2000, epsilon 0.0207285236767475, time 727.0, rides 143\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 4305, reward 955.0, memory_length 2000, epsilon 0.020709868005438427, time 728.0, rides 138\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 4306, reward 999.0, memory_length 2000, epsilon 0.02069122912423353, time 725.0, rides 133\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 4307, reward 1027.0, memory_length 2000, epsilon 0.020672607018021722, time 733.0, rides 138\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4308, reward 886.0, memory_length 2000, epsilon 0.020654001671705502, time 726.0, rides 142\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4309, reward 1034.0, memory_length 2000, epsilon 0.02063541307020097, time 725.0, rides 147\n",
      "Initial State is  [1, 9, 3]\n",
      "episode 4310, reward 1134.0, memory_length 2000, epsilon 0.020616841198437787, time 727.0, rides 145\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 4311, reward 1005.0, memory_length 2000, epsilon 0.020598286041359194, time 724.0, rides 155\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 4312, reward 638.0, memory_length 2000, epsilon 0.020579747583921972, time 728.0, rides 135\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 4313, reward 636.0, memory_length 2000, epsilon 0.020561225811096442, time 734.0, rides 146\n",
      "Initial State is  [2, 17, 3]\n",
      "episode 4314, reward 1098.0, memory_length 2000, epsilon 0.020542720707866457, time 728.0, rides 137\n",
      "Initial State is  [1, 5, 2]\n",
      "episode 4315, reward 1221.0, memory_length 2000, epsilon 0.020524232259229377, time 723.0, rides 133\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 4316, reward 543.0, memory_length 2000, epsilon 0.02050576045019607, time 724.0, rides 139\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4317, reward 844.0, memory_length 2000, epsilon 0.020487305265790894, time 733.0, rides 146\n",
      "Initial State is  [3, 14, 2]\n",
      "episode 4318, reward 782.0, memory_length 2000, epsilon 0.02046886669105168, time 729.0, rides 129\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 4319, reward 899.0, memory_length 2000, epsilon 0.020450444711029733, time 731.0, rides 141\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 4320, reward 597.0, memory_length 2000, epsilon 0.020432039310789806, time 739.0, rides 120\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 4321, reward 865.0, memory_length 2000, epsilon 0.020413650475410095, time 722.0, rides 130\n",
      "Initial State is  [2, 23, 5]\n",
      "episode 4322, reward 1020.0, memory_length 2000, epsilon 0.020395278189982227, time 730.0, rides 142\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4323, reward 955.0, memory_length 2000, epsilon 0.020376922439611242, time 725.0, rides 128\n",
      "Initial State is  [0, 16, 1]\n",
      "episode 4324, reward 711.0, memory_length 2000, epsilon 0.020358583209415592, time 730.0, rides 155\n",
      "Initial State is  [4, 12, 0]\n",
      "episode 4325, reward 847.0, memory_length 2000, epsilon 0.02034026048452712, time 728.0, rides 127\n",
      "Initial State is  [1, 1, 1]\n",
      "episode 4326, reward 1063.0, memory_length 2000, epsilon 0.020321954250091045, time 726.0, rides 134\n",
      "Initial State is  [2, 22, 4]\n",
      "episode 4327, reward 838.0, memory_length 2000, epsilon 0.02030366449126596, time 722.0, rides 131\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 4328, reward 828.0, memory_length 2000, epsilon 0.020285391193223822, time 735.0, rides 141\n",
      "Initial State is  [4, 11, 2]\n",
      "episode 4329, reward 1002.0, memory_length 2000, epsilon 0.02026713434114992, time 725.0, rides 134\n",
      "Initial State is  [2, 14, 1]\n",
      "episode 4330, reward 1000.0, memory_length 2000, epsilon 0.020248893920242886, time 727.0, rides 145\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 4331, reward 775.0, memory_length 2000, epsilon 0.020230669915714667, time 720.0, rides 147\n",
      "Initial State is  [4, 16, 1]\n",
      "episode 4332, reward 863.0, memory_length 2000, epsilon 0.020212462312790523, time 726.0, rides 144\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 4333, reward 1036.0, memory_length 2000, epsilon 0.020194271096709012, time 730.0, rides 148\n",
      "Initial State is  [3, 8, 3]\n",
      "episode 4334, reward 854.0, memory_length 2000, epsilon 0.020176096252721973, time 733.0, rides 139\n",
      "Initial State is  [0, 20, 2]\n",
      "episode 4335, reward 983.0, memory_length 2000, epsilon 0.020157937766094522, time 731.0, rides 134\n",
      "Initial State is  [2, 0, 3]\n",
      "episode 4336, reward 641.0, memory_length 2000, epsilon 0.020139795622105036, time 733.0, rides 162\n",
      "Initial State is  [1, 13, 5]\n",
      "episode 4337, reward 893.0, memory_length 2000, epsilon 0.020121669806045142, time 728.0, rides 138\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 4338, reward 855.0, memory_length 2000, epsilon 0.020103560303219702, time 733.0, rides 126\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 4339, reward 836.0, memory_length 2000, epsilon 0.020085467098946805, time 733.0, rides 140\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 4340, reward 1032.0, memory_length 2000, epsilon 0.020067390178557753, time 730.0, rides 138\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 4341, reward 883.0, memory_length 2000, epsilon 0.02004932952739705, time 730.0, rides 145\n",
      "Initial State is  [4, 10, 3]\n",
      "episode 4342, reward 857.0, memory_length 2000, epsilon 0.020031285130822394, time 725.0, rides 145\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 4343, reward 846.0, memory_length 2000, epsilon 0.020013256974204655, time 732.0, rides 143\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4344, reward 837.0, memory_length 2000, epsilon 0.01999524504292787, time 727.0, rides 122\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 4345, reward 786.0, memory_length 2000, epsilon 0.019977249322389236, time 735.0, rides 152\n",
      "Initial State is  [3, 13, 1]\n",
      "episode 4346, reward 1080.0, memory_length 2000, epsilon 0.019959269797999085, time 731.0, rides 136\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4347, reward 835.0, memory_length 2000, epsilon 0.019941306455180885, time 737.0, rides 129\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 4348, reward 1093.0, memory_length 2000, epsilon 0.019923359279371222, time 728.0, rides 136\n",
      "Initial State is  [4, 23, 3]\n",
      "episode 4349, reward 890.0, memory_length 2000, epsilon 0.019905428256019788, time 729.0, rides 135\n",
      "Initial State is  [2, 21, 6]\n",
      "episode 4350, reward 748.0, memory_length 2000, epsilon 0.01988751337058937, time 728.0, rides 137\n",
      "Initial State is  [3, 7, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4351, reward 741.0, memory_length 2000, epsilon 0.01986961460855584, time 739.0, rides 120\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 4352, reward 748.0, memory_length 2000, epsilon 0.01985173195540814, time 723.0, rides 141\n",
      "Initial State is  [1, 10, 0]\n",
      "episode 4353, reward 931.0, memory_length 2000, epsilon 0.019833865396648272, time 724.0, rides 147\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 4354, reward 624.0, memory_length 2000, epsilon 0.019816014917791287, time 729.0, rides 139\n",
      "Initial State is  [3, 15, 2]\n",
      "episode 4355, reward 1056.0, memory_length 2000, epsilon 0.019798180504365274, time 730.0, rides 145\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 4356, reward 972.0, memory_length 2000, epsilon 0.019780362141911347, time 728.0, rides 146\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 4357, reward 700.0, memory_length 2000, epsilon 0.019762559815983627, time 726.0, rides 132\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 4358, reward 812.0, memory_length 2000, epsilon 0.01974477351214924, time 727.0, rides 150\n",
      "Initial State is  [1, 0, 0]\n",
      "episode 4359, reward 982.0, memory_length 2000, epsilon 0.019727003215988307, time 733.0, rides 148\n",
      "Initial State is  [2, 6, 3]\n",
      "episode 4360, reward 889.0, memory_length 2000, epsilon 0.01970924891309392, time 725.0, rides 142\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 4361, reward 1039.0, memory_length 2000, epsilon 0.019691510589072134, time 726.0, rides 153\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 4362, reward 919.0, memory_length 2000, epsilon 0.01967378822954197, time 729.0, rides 155\n",
      "Initial State is  [2, 20, 2]\n",
      "episode 4363, reward 847.0, memory_length 2000, epsilon 0.019656081820135382, time 732.0, rides 149\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 4364, reward 873.0, memory_length 2000, epsilon 0.01963839134649726, time 724.0, rides 133\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 4365, reward 712.0, memory_length 2000, epsilon 0.01962071679428541, time 727.0, rides 125\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 4366, reward 965.0, memory_length 2000, epsilon 0.019603058149170554, time 726.0, rides 134\n",
      "Initial State is  [3, 4, 6]\n",
      "episode 4367, reward 767.0, memory_length 2000, epsilon 0.019585415396836302, time 726.0, rides 145\n",
      "Initial State is  [0, 16, 3]\n",
      "episode 4368, reward 926.0, memory_length 2000, epsilon 0.01956778852297915, time 732.0, rides 144\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 4369, reward 1153.0, memory_length 2000, epsilon 0.019550177513308467, time 726.0, rides 146\n",
      "Initial State is  [3, 1, 0]\n",
      "episode 4370, reward 667.0, memory_length 2000, epsilon 0.01953258235354649, time 725.0, rides 145\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 4371, reward 899.0, memory_length 2000, epsilon 0.0195150030294283, time 725.0, rides 147\n",
      "Initial State is  [0, 10, 6]\n",
      "episode 4372, reward 766.0, memory_length 2000, epsilon 0.019497439526701812, time 726.0, rides 134\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 4373, reward 901.0, memory_length 2000, epsilon 0.01947989183112778, time 726.0, rides 140\n",
      "Initial State is  [0, 11, 4]\n",
      "episode 4374, reward 900.0, memory_length 2000, epsilon 0.019462359928479764, time 731.0, rides 136\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 4375, reward 846.0, memory_length 2000, epsilon 0.019444843804544133, time 730.0, rides 131\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4376, reward 864.0, memory_length 2000, epsilon 0.01942734344512004, time 729.0, rides 142\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 4377, reward 721.0, memory_length 2000, epsilon 0.019409858836019433, time 726.0, rides 153\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 4378, reward 1095.0, memory_length 2000, epsilon 0.019392389963067014, time 724.0, rides 144\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 4379, reward 925.0, memory_length 2000, epsilon 0.019374936812100254, time 723.0, rides 134\n",
      "Initial State is  [0, 6, 3]\n",
      "episode 4380, reward 831.0, memory_length 2000, epsilon 0.019357499368969362, time 734.0, rides 149\n",
      "Initial State is  [2, 14, 4]\n",
      "episode 4381, reward 765.0, memory_length 2000, epsilon 0.01934007761953729, time 723.0, rides 143\n",
      "Initial State is  [2, 11, 1]\n",
      "episode 4382, reward 993.0, memory_length 2000, epsilon 0.019322671549679708, time 727.0, rides 126\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 4383, reward 827.0, memory_length 2000, epsilon 0.019305281145284996, time 731.0, rides 160\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 4384, reward 1024.0, memory_length 2000, epsilon 0.01928790639225424, time 723.0, rides 136\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 4385, reward 890.0, memory_length 2000, epsilon 0.01927054727650121, time 728.0, rides 139\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 4386, reward 891.0, memory_length 2000, epsilon 0.019253203783952358, time 732.0, rides 130\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4387, reward 803.0, memory_length 2000, epsilon 0.0192358759005468, time 730.0, rides 149\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 4388, reward 1028.0, memory_length 2000, epsilon 0.019218563612236308, time 723.0, rides 129\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4389, reward 942.0, memory_length 2000, epsilon 0.019201266904985297, time 728.0, rides 144\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 4390, reward 823.0, memory_length 2000, epsilon 0.01918398576477081, time 728.0, rides 144\n",
      "Initial State is  [3, 21, 0]\n",
      "episode 4391, reward 603.0, memory_length 2000, epsilon 0.019166720177582516, time 731.0, rides 153\n",
      "Initial State is  [4, 0, 4]\n",
      "episode 4392, reward 960.0, memory_length 2000, epsilon 0.019149470129422693, time 724.0, rides 130\n",
      "Initial State is  [0, 8, 2]\n",
      "episode 4393, reward 662.0, memory_length 2000, epsilon 0.01913223560630621, time 729.0, rides 139\n",
      "Initial State is  [3, 17, 2]\n",
      "episode 4394, reward 878.0, memory_length 2000, epsilon 0.019115016594260535, time 725.0, rides 150\n",
      "Initial State is  [4, 21, 3]\n",
      "episode 4395, reward 754.0, memory_length 2000, epsilon 0.0190978130793257, time 731.0, rides 144\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 4396, reward 1095.0, memory_length 2000, epsilon 0.019080625047554308, time 725.0, rides 142\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4397, reward 1306.0, memory_length 2000, epsilon 0.019063452485011508, time 726.0, rides 156\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 4398, reward 871.0, memory_length 2000, epsilon 0.019046295377774997, time 728.0, rides 134\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 4399, reward 894.0, memory_length 2000, epsilon 0.019029153711935, time 724.0, rides 140\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 4400, reward 863.0, memory_length 2000, epsilon 0.01901202747359426, time 722.0, rides 136\n",
      "Initial State is  [1, 14, 1]\n",
      "episode 4401, reward 889.0, memory_length 2000, epsilon 0.018994916648868022, time 732.0, rides 144\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4402, reward 964.0, memory_length 2000, epsilon 0.018977821223884042, time 724.0, rides 139\n",
      "Initial State is  [2, 10, 2]\n",
      "episode 4403, reward 803.0, memory_length 2000, epsilon 0.018960741184782547, time 720.0, rides 131\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 4404, reward 734.0, memory_length 2000, epsilon 0.01894367651771624, time 733.0, rides 140\n",
      "Initial State is  [3, 12, 5]\n",
      "episode 4405, reward 1204.0, memory_length 2000, epsilon 0.018926627208850296, time 726.0, rides 146\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 4406, reward 816.0, memory_length 2000, epsilon 0.01890959324436233, time 725.0, rides 135\n",
      "Initial State is  [2, 13, 1]\n",
      "episode 4407, reward 665.0, memory_length 2000, epsilon 0.018892574610442404, time 731.0, rides 137\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4408, reward 821.0, memory_length 2000, epsilon 0.018875571293293005, time 729.0, rides 137\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4409, reward 701.0, memory_length 2000, epsilon 0.01885858327912904, time 725.0, rides 128\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 4410, reward 551.0, memory_length 2000, epsilon 0.018841610554177823, time 726.0, rides 125\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 4411, reward 648.0, memory_length 2000, epsilon 0.018824653104679064, time 726.0, rides 139\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 4412, reward 794.0, memory_length 2000, epsilon 0.018807710916884855, time 740.0, rides 144\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 4413, reward 843.0, memory_length 2000, epsilon 0.018790783977059657, time 736.0, rides 138\n",
      "Initial State is  [4, 10, 5]\n",
      "episode 4414, reward 890.0, memory_length 2000, epsilon 0.018773872271480304, time 729.0, rides 146\n",
      "Initial State is  [3, 11, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4415, reward 779.0, memory_length 2000, epsilon 0.01875697578643597, time 725.0, rides 134\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 4416, reward 693.0, memory_length 2000, epsilon 0.018740094508228177, time 729.0, rides 140\n",
      "Initial State is  [3, 11, 1]\n",
      "episode 4417, reward 1242.0, memory_length 2000, epsilon 0.01872322842317077, time 725.0, rides 147\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 4418, reward 744.0, memory_length 2000, epsilon 0.018706377517589915, time 722.0, rides 138\n",
      "Initial State is  [1, 3, 1]\n",
      "episode 4419, reward 740.0, memory_length 2000, epsilon 0.018689541777824083, time 726.0, rides 132\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4420, reward 570.0, memory_length 2000, epsilon 0.01867272119022404, time 731.0, rides 126\n",
      "Initial State is  [2, 22, 0]\n",
      "episode 4421, reward 682.0, memory_length 2000, epsilon 0.01865591574115284, time 726.0, rides 137\n",
      "Initial State is  [2, 16, 6]\n",
      "episode 4422, reward 834.0, memory_length 2000, epsilon 0.018639125416985803, time 728.0, rides 154\n",
      "Initial State is  [1, 13, 6]\n",
      "episode 4423, reward 824.0, memory_length 2000, epsilon 0.018622350204110516, time 723.0, rides 136\n",
      "Initial State is  [0, 16, 2]\n",
      "episode 4424, reward 831.0, memory_length 2000, epsilon 0.018605590088926816, time 729.0, rides 155\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 4425, reward 927.0, memory_length 2000, epsilon 0.018588845057846783, time 726.0, rides 141\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 4426, reward 567.0, memory_length 2000, epsilon 0.01857211509729472, time 729.0, rides 144\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 4427, reward 874.0, memory_length 2000, epsilon 0.018555400193707154, time 729.0, rides 136\n",
      "Initial State is  [1, 22, 1]\n",
      "episode 4428, reward 965.0, memory_length 2000, epsilon 0.018538700333532818, time 723.0, rides 142\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4429, reward 968.0, memory_length 2000, epsilon 0.01852201550323264, time 726.0, rides 132\n",
      "Initial State is  [4, 12, 3]\n",
      "episode 4430, reward 1068.0, memory_length 2000, epsilon 0.01850534568927973, time 724.0, rides 151\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 4431, reward 765.0, memory_length 2000, epsilon 0.018488690878159377, time 726.0, rides 132\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 4432, reward 820.0, memory_length 2000, epsilon 0.018472051056369034, time 722.0, rides 131\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 4433, reward 683.0, memory_length 2000, epsilon 0.0184554262104183, time 727.0, rides 141\n",
      "Initial State is  [3, 16, 1]\n",
      "episode 4434, reward 651.0, memory_length 2000, epsilon 0.018438816326828925, time 729.0, rides 144\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 4435, reward 880.0, memory_length 2000, epsilon 0.01842222139213478, time 726.0, rides 142\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 4436, reward 860.0, memory_length 2000, epsilon 0.018405641392881856, time 727.0, rides 128\n",
      "Initial State is  [2, 15, 2]\n",
      "episode 4437, reward 1094.0, memory_length 2000, epsilon 0.01838907631562826, time 733.0, rides 131\n",
      "Initial State is  [4, 10, 2]\n",
      "episode 4438, reward 913.0, memory_length 2000, epsilon 0.018372526146944193, time 735.0, rides 134\n",
      "Initial State is  [0, 10, 2]\n",
      "episode 4439, reward 1076.0, memory_length 2000, epsilon 0.018355990873411943, time 729.0, rides 130\n",
      "Initial State is  [3, 18, 5]\n",
      "episode 4440, reward 592.0, memory_length 2000, epsilon 0.01833947048162587, time 723.0, rides 131\n",
      "Initial State is  [1, 17, 6]\n",
      "episode 4441, reward 963.0, memory_length 2000, epsilon 0.018322964958192408, time 722.0, rides 122\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4442, reward 1051.0, memory_length 2000, epsilon 0.018306474289730035, time 734.0, rides 144\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 4443, reward 986.0, memory_length 2000, epsilon 0.018289998462869276, time 732.0, rides 151\n",
      "Initial State is  [2, 13, 6]\n",
      "episode 4444, reward 943.0, memory_length 2000, epsilon 0.018273537464252695, time 727.0, rides 133\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 4445, reward 1025.0, memory_length 2000, epsilon 0.018257091280534866, time 730.0, rides 140\n",
      "Initial State is  [2, 12, 3]\n",
      "episode 4446, reward 1102.0, memory_length 2000, epsilon 0.018240659898382385, time 728.0, rides 141\n",
      "Initial State is  [4, 9, 5]\n",
      "episode 4447, reward 935.0, memory_length 2000, epsilon 0.01822424330447384, time 726.0, rides 133\n",
      "Initial State is  [0, 23, 4]\n",
      "episode 4448, reward 863.0, memory_length 2000, epsilon 0.018207841485499813, time 733.0, rides 132\n",
      "Initial State is  [2, 12, 6]\n",
      "episode 4449, reward 888.0, memory_length 2000, epsilon 0.018191454428162862, time 727.0, rides 140\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 4450, reward 750.0, memory_length 2000, epsilon 0.018175082119177514, time 727.0, rides 133\n",
      "Initial State is  [1, 7, 4]\n",
      "episode 4451, reward 690.0, memory_length 2000, epsilon 0.018158724545270254, time 729.0, rides 138\n",
      "Initial State is  [4, 8, 2]\n",
      "episode 4452, reward 1121.0, memory_length 2000, epsilon 0.01814238169317951, time 728.0, rides 140\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 4453, reward 577.0, memory_length 2000, epsilon 0.018126053549655647, time 725.0, rides 140\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 4454, reward 1139.0, memory_length 2000, epsilon 0.018109740101460957, time 728.0, rides 152\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 4455, reward 869.0, memory_length 2000, epsilon 0.01809344133536964, time 727.0, rides 142\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 4456, reward 675.0, memory_length 2000, epsilon 0.01807715723816781, time 728.0, rides 149\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 4457, reward 1091.0, memory_length 2000, epsilon 0.018060887796653456, time 721.0, rides 137\n",
      "Initial State is  [3, 14, 0]\n",
      "episode 4458, reward 816.0, memory_length 2000, epsilon 0.018044632997636468, time 729.0, rides 156\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 4459, reward 1031.0, memory_length 2000, epsilon 0.018028392827938593, time 727.0, rides 138\n",
      "Initial State is  [4, 19, 2]\n",
      "episode 4460, reward 1009.0, memory_length 2000, epsilon 0.018012167274393448, time 726.0, rides 143\n",
      "Initial State is  [3, 11, 3]\n",
      "episode 4461, reward 939.0, memory_length 2000, epsilon 0.017995956323846495, time 726.0, rides 143\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 4462, reward 1165.0, memory_length 2000, epsilon 0.017979759963155033, time 724.0, rides 138\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 4463, reward 685.0, memory_length 2000, epsilon 0.017963578179188193, time 723.0, rides 143\n",
      "Initial State is  [3, 1, 3]\n",
      "episode 4464, reward 959.0, memory_length 2000, epsilon 0.017947410958826925, time 730.0, rides 139\n",
      "Initial State is  [0, 1, 1]\n",
      "episode 4465, reward 1109.0, memory_length 2000, epsilon 0.01793125828896398, time 725.0, rides 137\n",
      "Initial State is  [1, 3, 5]\n",
      "episode 4466, reward 602.0, memory_length 2000, epsilon 0.017915120156503914, time 728.0, rides 131\n",
      "Initial State is  [3, 9, 0]\n",
      "episode 4467, reward 563.0, memory_length 2000, epsilon 0.01789899654836306, time 723.0, rides 145\n",
      "Initial State is  [1, 11, 6]\n",
      "episode 4468, reward 983.0, memory_length 2000, epsilon 0.017882887451469532, time 731.0, rides 135\n",
      "Initial State is  [3, 3, 5]\n",
      "episode 4469, reward 921.0, memory_length 2000, epsilon 0.01786679285276321, time 726.0, rides 140\n",
      "Initial State is  [3, 14, 4]\n",
      "episode 4470, reward 631.0, memory_length 2000, epsilon 0.017850712739195723, time 723.0, rides 137\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 4471, reward 710.0, memory_length 2000, epsilon 0.017834647097730447, time 724.0, rides 136\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 4472, reward 933.0, memory_length 2000, epsilon 0.01781859591534249, time 731.0, rides 139\n",
      "Initial State is  [4, 8, 3]\n",
      "episode 4473, reward 595.0, memory_length 2000, epsilon 0.01780255917901868, time 732.0, rides 135\n",
      "Initial State is  [2, 2, 6]\n",
      "episode 4474, reward 1047.0, memory_length 2000, epsilon 0.017786536875757562, time 730.0, rides 138\n",
      "Initial State is  [4, 1, 1]\n",
      "episode 4475, reward 947.0, memory_length 2000, epsilon 0.01777052899256938, time 720.0, rides 141\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 4476, reward 857.0, memory_length 2000, epsilon 0.01775453551647607, time 731.0, rides 137\n",
      "Initial State is  [2, 13, 3]\n",
      "episode 4477, reward 946.0, memory_length 2000, epsilon 0.01773855643451124, time 727.0, rides 151\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 4478, reward 998.0, memory_length 2000, epsilon 0.017722591733720178, time 729.0, rides 144\n",
      "Initial State is  [3, 9, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4479, reward 942.0, memory_length 2000, epsilon 0.01770664140115983, time 733.0, rides 137\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4480, reward 828.0, memory_length 2000, epsilon 0.017690705423898785, time 727.0, rides 147\n",
      "Initial State is  [0, 13, 2]\n",
      "episode 4481, reward 820.0, memory_length 2000, epsilon 0.017674783789017275, time 738.0, rides 132\n",
      "Initial State is  [4, 21, 5]\n",
      "episode 4482, reward 937.0, memory_length 2000, epsilon 0.017658876483607158, time 724.0, rides 148\n",
      "Initial State is  [4, 2, 0]\n",
      "episode 4483, reward 832.0, memory_length 2000, epsilon 0.017642983494771912, time 724.0, rides 120\n",
      "Initial State is  [0, 15, 4]\n",
      "episode 4484, reward 834.0, memory_length 2000, epsilon 0.017627104809626617, time 735.0, rides 145\n",
      "Initial State is  [4, 21, 6]\n",
      "episode 4485, reward 409.0, memory_length 2000, epsilon 0.017611240415297953, time 727.0, rides 147\n",
      "Initial State is  [3, 0, 4]\n",
      "episode 4486, reward 696.0, memory_length 2000, epsilon 0.017595390298924183, time 730.0, rides 154\n",
      "Initial State is  [1, 0, 1]\n",
      "episode 4487, reward 1049.0, memory_length 2000, epsilon 0.01757955444765515, time 728.0, rides 138\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 4488, reward 781.0, memory_length 2000, epsilon 0.01756373284865226, time 731.0, rides 146\n",
      "Initial State is  [1, 14, 4]\n",
      "episode 4489, reward 791.0, memory_length 2000, epsilon 0.017547925489088474, time 724.0, rides 147\n",
      "Initial State is  [1, 15, 0]\n",
      "episode 4490, reward 893.0, memory_length 2000, epsilon 0.017532132356148294, time 726.0, rides 143\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4491, reward 704.0, memory_length 2000, epsilon 0.01751635343702776, time 726.0, rides 139\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 4492, reward 1132.0, memory_length 2000, epsilon 0.017500588718934434, time 729.0, rides 148\n",
      "Initial State is  [3, 4, 4]\n",
      "episode 4493, reward 607.0, memory_length 2000, epsilon 0.017484838189087394, time 724.0, rides 137\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 4494, reward 989.0, memory_length 2000, epsilon 0.017469101834717216, time 729.0, rides 139\n",
      "Initial State is  [1, 6, 4]\n",
      "episode 4495, reward 811.0, memory_length 2000, epsilon 0.01745337964306597, time 732.0, rides 130\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4496, reward 995.0, memory_length 2000, epsilon 0.01743767160138721, time 736.0, rides 157\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 4497, reward 1091.0, memory_length 2000, epsilon 0.01742197769694596, time 732.0, rides 160\n",
      "Initial State is  [1, 3, 3]\n",
      "episode 4498, reward 972.0, memory_length 2000, epsilon 0.01740629791701871, time 732.0, rides 130\n",
      "Initial State is  [0, 17, 2]\n",
      "episode 4499, reward 791.0, memory_length 2000, epsilon 0.01739063224889339, time 724.0, rides 133\n",
      "Initial State is  [0, 6, 6]\n",
      "episode 4500, reward 769.0, memory_length 2000, epsilon 0.017374980679869388, time 723.0, rides 151\n",
      "Initial State is  [4, 18, 0]\n",
      "episode 4501, reward 661.0, memory_length 2000, epsilon 0.017359343197257505, time 733.0, rides 132\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 4502, reward 890.0, memory_length 2000, epsilon 0.017343719788379973, time 730.0, rides 140\n",
      "Initial State is  [4, 5, 1]\n",
      "episode 4503, reward 1033.0, memory_length 2000, epsilon 0.01732811044057043, time 723.0, rides 134\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 4504, reward 620.0, memory_length 2000, epsilon 0.017312515141173917, time 728.0, rides 141\n",
      "Initial State is  [2, 4, 6]\n",
      "episode 4505, reward 909.0, memory_length 2000, epsilon 0.01729693387754686, time 723.0, rides 151\n",
      "Initial State is  [0, 19, 4]\n",
      "episode 4506, reward 737.0, memory_length 2000, epsilon 0.017281366637057066, time 731.0, rides 111\n",
      "Initial State is  [1, 1, 0]\n",
      "episode 4507, reward 992.0, memory_length 2000, epsilon 0.017265813407083715, time 724.0, rides 121\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 4508, reward 809.0, memory_length 2000, epsilon 0.01725027417501734, time 727.0, rides 136\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 4509, reward 933.0, memory_length 2000, epsilon 0.017234748928259824, time 722.0, rides 144\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4510, reward 749.0, memory_length 2000, epsilon 0.017219237654224392, time 722.0, rides 141\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 4511, reward 930.0, memory_length 2000, epsilon 0.017203740340335588, time 723.0, rides 141\n",
      "Initial State is  [1, 23, 0]\n",
      "episode 4512, reward 790.0, memory_length 2000, epsilon 0.017188256974029287, time 726.0, rides 132\n",
      "Initial State is  [1, 23, 2]\n",
      "episode 4513, reward 971.0, memory_length 2000, epsilon 0.01717278754275266, time 725.0, rides 126\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 4514, reward 814.0, memory_length 2000, epsilon 0.017157332033964183, time 727.0, rides 142\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 4515, reward 828.0, memory_length 2000, epsilon 0.017141890435133617, time 727.0, rides 135\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 4516, reward 1013.0, memory_length 2000, epsilon 0.017126462733741996, time 726.0, rides 143\n",
      "Initial State is  [0, 1, 5]\n",
      "episode 4517, reward 888.0, memory_length 2000, epsilon 0.01711104891728163, time 735.0, rides 121\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 4518, reward 786.0, memory_length 2000, epsilon 0.017095648973256074, time 727.0, rides 122\n",
      "Initial State is  [1, 13, 0]\n",
      "episode 4519, reward 918.0, memory_length 2000, epsilon 0.017080262889180145, time 735.0, rides 126\n",
      "Initial State is  [2, 2, 3]\n",
      "episode 4520, reward 890.0, memory_length 2000, epsilon 0.01706489065257988, time 734.0, rides 142\n",
      "Initial State is  [4, 0, 0]\n",
      "episode 4521, reward 718.0, memory_length 2000, epsilon 0.01704953225099256, time 729.0, rides 135\n",
      "Initial State is  [4, 17, 2]\n",
      "episode 4522, reward 896.0, memory_length 2000, epsilon 0.017034187671966666, time 720.0, rides 158\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4523, reward 792.0, memory_length 2000, epsilon 0.017018856903061895, time 730.0, rides 149\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 4524, reward 998.0, memory_length 2000, epsilon 0.017003539931849138, time 734.0, rides 134\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 4525, reward 890.0, memory_length 2000, epsilon 0.016988236745910473, time 728.0, rides 126\n",
      "Initial State is  [0, 7, 3]\n",
      "episode 4526, reward 531.0, memory_length 2000, epsilon 0.016972947332839154, time 731.0, rides 129\n",
      "Initial State is  [4, 5, 6]\n",
      "episode 4527, reward 865.0, memory_length 2000, epsilon 0.016957671680239598, time 728.0, rides 137\n",
      "Initial State is  [1, 10, 4]\n",
      "episode 4528, reward 660.0, memory_length 2000, epsilon 0.016942409775727384, time 725.0, rides 159\n",
      "Initial State is  [1, 18, 5]\n",
      "episode 4529, reward 618.0, memory_length 2000, epsilon 0.01692716160692923, time 739.0, rides 134\n",
      "Initial State is  [0, 6, 0]\n",
      "episode 4530, reward 1087.0, memory_length 2000, epsilon 0.016911927161482994, time 727.0, rides 143\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 4531, reward 910.0, memory_length 2000, epsilon 0.01689670642703766, time 728.0, rides 141\n",
      "Initial State is  [2, 4, 5]\n",
      "episode 4532, reward 689.0, memory_length 2000, epsilon 0.016881499391253326, time 725.0, rides 140\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 4533, reward 948.0, memory_length 2000, epsilon 0.0168663060418012, time 724.0, rides 135\n",
      "Initial State is  [0, 3, 3]\n",
      "episode 4534, reward 977.0, memory_length 2000, epsilon 0.01685112636636358, time 731.0, rides 153\n",
      "Initial State is  [1, 18, 3]\n",
      "episode 4535, reward 801.0, memory_length 2000, epsilon 0.016835960352633853, time 731.0, rides 146\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4536, reward 805.0, memory_length 2000, epsilon 0.01682080798831648, time 729.0, rides 138\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 4537, reward 1106.0, memory_length 2000, epsilon 0.016805669261126997, time 729.0, rides 134\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 4538, reward 996.0, memory_length 2000, epsilon 0.016790544158791984, time 729.0, rides 146\n",
      "Initial State is  [2, 5, 4]\n",
      "episode 4539, reward 686.0, memory_length 2000, epsilon 0.01677543266904907, time 727.0, rides 138\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 4540, reward 1031.0, memory_length 2000, epsilon 0.016760334779646925, time 725.0, rides 147\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 4541, reward 1018.0, memory_length 2000, epsilon 0.01674525047834524, time 727.0, rides 132\n",
      "Initial State is  [1, 16, 3]\n",
      "episode 4542, reward 818.0, memory_length 2000, epsilon 0.016730179752914732, time 727.0, rides 146\n",
      "Initial State is  [1, 15, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4543, reward 593.0, memory_length 2000, epsilon 0.016715122591137107, time 727.0, rides 133\n",
      "Initial State is  [2, 15, 4]\n",
      "episode 4544, reward 774.0, memory_length 2000, epsilon 0.016700078980805083, time 727.0, rides 133\n",
      "Initial State is  [1, 19, 1]\n",
      "episode 4545, reward 553.0, memory_length 2000, epsilon 0.016685048909722357, time 726.0, rides 132\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 4546, reward 319.0, memory_length 2000, epsilon 0.016670032365703608, time 728.0, rides 118\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4547, reward 1027.0, memory_length 2000, epsilon 0.016655029336574475, time 730.0, rides 132\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 4548, reward 950.0, memory_length 2000, epsilon 0.016640039810171557, time 733.0, rides 142\n",
      "Initial State is  [0, 12, 6]\n",
      "episode 4549, reward 1024.0, memory_length 2000, epsilon 0.0166250637743424, time 722.0, rides 121\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 4550, reward 819.0, memory_length 2000, epsilon 0.016610101216945495, time 725.0, rides 139\n",
      "Initial State is  [1, 9, 5]\n",
      "episode 4551, reward 872.0, memory_length 2000, epsilon 0.016595152125850245, time 729.0, rides 145\n",
      "Initial State is  [2, 0, 4]\n",
      "episode 4552, reward 964.0, memory_length 2000, epsilon 0.01658021648893698, time 727.0, rides 144\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 4553, reward 682.0, memory_length 2000, epsilon 0.016565294294096936, time 728.0, rides 133\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 4554, reward 879.0, memory_length 2000, epsilon 0.01655038552923225, time 726.0, rides 131\n",
      "Initial State is  [1, 20, 2]\n",
      "episode 4555, reward 738.0, memory_length 2000, epsilon 0.01653549018225594, time 728.0, rides 150\n",
      "Initial State is  [3, 13, 5]\n",
      "episode 4556, reward 619.0, memory_length 2000, epsilon 0.01652060824109191, time 730.0, rides 142\n",
      "Initial State is  [1, 17, 1]\n",
      "episode 4557, reward 667.0, memory_length 2000, epsilon 0.016505739693674925, time 727.0, rides 136\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 4558, reward 791.0, memory_length 2000, epsilon 0.016490884527950618, time 747.0, rides 134\n",
      "Initial State is  [2, 17, 0]\n",
      "episode 4559, reward 539.0, memory_length 2000, epsilon 0.016476042731875463, time 728.0, rides 128\n",
      "Initial State is  [0, 15, 0]\n",
      "episode 4560, reward 852.0, memory_length 2000, epsilon 0.016461214293416775, time 722.0, rides 128\n",
      "Initial State is  [2, 11, 2]\n",
      "episode 4561, reward 681.0, memory_length 2000, epsilon 0.0164463992005527, time 729.0, rides 132\n",
      "Initial State is  [4, 9, 6]\n",
      "episode 4562, reward 783.0, memory_length 2000, epsilon 0.016431597441272202, time 723.0, rides 136\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 4563, reward 1035.0, memory_length 2000, epsilon 0.016416809003575058, time 730.0, rides 140\n",
      "Initial State is  [2, 3, 6]\n",
      "episode 4564, reward 904.0, memory_length 2000, epsilon 0.01640203387547184, time 727.0, rides 137\n",
      "Initial State is  [3, 18, 1]\n",
      "episode 4565, reward 1112.0, memory_length 2000, epsilon 0.016387272044983917, time 730.0, rides 140\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 4566, reward 651.0, memory_length 2000, epsilon 0.01637252350014343, time 735.0, rides 144\n",
      "Initial State is  [1, 6, 2]\n",
      "episode 4567, reward 671.0, memory_length 2000, epsilon 0.0163577882289933, time 728.0, rides 141\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 4568, reward 822.0, memory_length 2000, epsilon 0.016343066219587206, time 728.0, rides 142\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 4569, reward 473.0, memory_length 2000, epsilon 0.016328357459989576, time 738.0, rides 156\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 4570, reward 940.0, memory_length 2000, epsilon 0.016313661938275586, time 733.0, rides 118\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 4571, reward 797.0, memory_length 2000, epsilon 0.016298979642531138, time 731.0, rides 138\n",
      "Initial State is  [2, 2, 1]\n",
      "episode 4572, reward 1207.0, memory_length 2000, epsilon 0.01628431056085286, time 726.0, rides 143\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4573, reward 948.0, memory_length 2000, epsilon 0.016269654681348094, time 727.0, rides 141\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4574, reward 866.0, memory_length 2000, epsilon 0.01625501199213488, time 726.0, rides 155\n",
      "Initial State is  [1, 18, 0]\n",
      "episode 4575, reward 869.0, memory_length 2000, epsilon 0.01624038248134196, time 735.0, rides 125\n",
      "Initial State is  [2, 18, 6]\n",
      "episode 4576, reward 677.0, memory_length 2000, epsilon 0.016225766137108754, time 726.0, rides 146\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 4577, reward 378.0, memory_length 2000, epsilon 0.016211162947585355, time 727.0, rides 129\n",
      "Initial State is  [2, 19, 0]\n",
      "episode 4578, reward 600.0, memory_length 2000, epsilon 0.01619657290093253, time 724.0, rides 132\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 4579, reward 860.0, memory_length 2000, epsilon 0.01618199598532169, time 730.0, rides 132\n",
      "Initial State is  [0, 2, 3]\n",
      "episode 4580, reward 916.0, memory_length 2000, epsilon 0.0161674321889349, time 731.0, rides 145\n",
      "Initial State is  [1, 23, 3]\n",
      "episode 4581, reward 700.0, memory_length 2000, epsilon 0.01615288149996486, time 727.0, rides 142\n",
      "Initial State is  [1, 4, 4]\n",
      "episode 4582, reward 1175.0, memory_length 2000, epsilon 0.01613834390661489, time 727.0, rides 141\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 4583, reward 933.0, memory_length 2000, epsilon 0.016123819397098938, time 724.0, rides 150\n",
      "Initial State is  [0, 22, 6]\n",
      "episode 4584, reward 1149.0, memory_length 2000, epsilon 0.01610930795964155, time 730.0, rides 129\n",
      "Initial State is  [2, 7, 5]\n",
      "episode 4585, reward 581.0, memory_length 2000, epsilon 0.016094809582477873, time 726.0, rides 137\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 4586, reward 941.0, memory_length 2000, epsilon 0.016080324253853643, time 724.0, rides 127\n",
      "Initial State is  [4, 21, 4]\n",
      "episode 4587, reward 1097.0, memory_length 2000, epsilon 0.016065851962025174, time 726.0, rides 131\n",
      "Initial State is  [3, 5, 1]\n",
      "episode 4588, reward 758.0, memory_length 2000, epsilon 0.01605139269525935, time 730.0, rides 129\n",
      "Initial State is  [0, 21, 6]\n",
      "episode 4589, reward 1104.0, memory_length 2000, epsilon 0.016036946441833618, time 723.0, rides 136\n",
      "Initial State is  [0, 13, 5]\n",
      "episode 4590, reward 736.0, memory_length 2000, epsilon 0.016022513190035968, time 727.0, rides 138\n",
      "Initial State is  [4, 15, 3]\n",
      "episode 4591, reward 950.0, memory_length 2000, epsilon 0.016008092928164935, time 729.0, rides 131\n",
      "Initial State is  [1, 4, 6]\n",
      "episode 4592, reward 913.0, memory_length 2000, epsilon 0.015993685644529586, time 723.0, rides 142\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 4593, reward 1302.0, memory_length 2000, epsilon 0.015979291327449508, time 727.0, rides 126\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 4594, reward 776.0, memory_length 2000, epsilon 0.015964909965254803, time 725.0, rides 145\n",
      "Initial State is  [0, 9, 2]\n",
      "episode 4595, reward 581.0, memory_length 2000, epsilon 0.015950541546286074, time 729.0, rides 135\n",
      "Initial State is  [2, 4, 2]\n",
      "episode 4596, reward 837.0, memory_length 2000, epsilon 0.015936186058894415, time 738.0, rides 130\n",
      "Initial State is  [3, 17, 4]\n",
      "episode 4597, reward 845.0, memory_length 2000, epsilon 0.01592184349144141, time 729.0, rides 142\n",
      "Initial State is  [1, 1, 3]\n",
      "episode 4598, reward 791.0, memory_length 2000, epsilon 0.015907513832299113, time 727.0, rides 136\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 4599, reward 762.0, memory_length 2000, epsilon 0.015893197069850044, time 726.0, rides 151\n",
      "Initial State is  [1, 23, 5]\n",
      "episode 4600, reward 749.0, memory_length 2000, epsilon 0.015878893192487177, time 734.0, rides 128\n",
      "Initial State is  [0, 1, 2]\n",
      "episode 4601, reward 836.0, memory_length 2000, epsilon 0.01586460218861394, time 736.0, rides 138\n",
      "Initial State is  [2, 14, 6]\n",
      "episode 4602, reward 929.0, memory_length 2000, epsilon 0.015850324046644187, time 733.0, rides 132\n",
      "Initial State is  [0, 0, 4]\n",
      "episode 4603, reward 1119.0, memory_length 2000, epsilon 0.015836058755002207, time 731.0, rides 151\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 4604, reward 722.0, memory_length 2000, epsilon 0.015821806302122706, time 725.0, rides 132\n",
      "Initial State is  [2, 15, 3]\n",
      "episode 4605, reward 1127.0, memory_length 2000, epsilon 0.015807566676450797, time 731.0, rides 137\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 4606, reward 837.0, memory_length 2000, epsilon 0.01579333986644199, time 729.0, rides 133\n",
      "Initial State is  [2, 13, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4607, reward 403.0, memory_length 2000, epsilon 0.015779125860562192, time 721.0, rides 124\n",
      "Initial State is  [3, 21, 5]\n",
      "episode 4608, reward 770.0, memory_length 2000, epsilon 0.015764924647287685, time 729.0, rides 130\n",
      "Initial State is  [0, 20, 5]\n",
      "episode 4609, reward 760.0, memory_length 2000, epsilon 0.015750736215105126, time 731.0, rides 129\n",
      "Initial State is  [0, 12, 4]\n",
      "episode 4610, reward 749.0, memory_length 2000, epsilon 0.01573656055251153, time 732.0, rides 123\n",
      "Initial State is  [3, 11, 2]\n",
      "episode 4611, reward 748.0, memory_length 2000, epsilon 0.01572239764801427, time 736.0, rides 140\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 4612, reward 676.0, memory_length 2000, epsilon 0.015708247490131055, time 723.0, rides 147\n",
      "Initial State is  [1, 15, 3]\n",
      "episode 4613, reward 642.0, memory_length 2000, epsilon 0.015694110067389938, time 729.0, rides 129\n",
      "Initial State is  [4, 22, 5]\n",
      "episode 4614, reward 813.0, memory_length 2000, epsilon 0.015679985368329288, time 726.0, rides 147\n",
      "Initial State is  [1, 11, 2]\n",
      "episode 4615, reward 1191.0, memory_length 2000, epsilon 0.015665873381497792, time 723.0, rides 140\n",
      "Initial State is  [2, 16, 0]\n",
      "episode 4616, reward 635.0, memory_length 2000, epsilon 0.015651774095454443, time 725.0, rides 138\n",
      "Initial State is  [3, 3, 4]\n",
      "episode 4617, reward 1057.0, memory_length 2000, epsilon 0.015637687498768534, time 727.0, rides 129\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 4618, reward 925.0, memory_length 2000, epsilon 0.015623613580019643, time 730.0, rides 135\n",
      "Initial State is  [4, 22, 6]\n",
      "episode 4619, reward 1214.0, memory_length 2000, epsilon 0.015609552327797625, time 723.0, rides 126\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4620, reward 594.0, memory_length 2000, epsilon 0.015595503730702606, time 729.0, rides 124\n",
      "Initial State is  [3, 5, 5]\n",
      "episode 4621, reward 757.0, memory_length 2000, epsilon 0.015581467777344973, time 724.0, rides 116\n",
      "Initial State is  [1, 21, 6]\n",
      "episode 4622, reward 885.0, memory_length 2000, epsilon 0.015567444456345362, time 725.0, rides 142\n",
      "Initial State is  [1, 5, 0]\n",
      "episode 4623, reward 876.0, memory_length 2000, epsilon 0.015553433756334651, time 729.0, rides 127\n",
      "Initial State is  [0, 18, 3]\n",
      "episode 4624, reward 976.0, memory_length 2000, epsilon 0.01553943566595395, time 726.0, rides 137\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 4625, reward 869.0, memory_length 2000, epsilon 0.015525450173854592, time 727.0, rides 135\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 4626, reward 511.0, memory_length 2000, epsilon 0.015511477268698122, time 724.0, rides 132\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4627, reward 1039.0, memory_length 2000, epsilon 0.015497516939156294, time 725.0, rides 135\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 4628, reward 795.0, memory_length 2000, epsilon 0.015483569173911053, time 732.0, rides 146\n",
      "Initial State is  [2, 3, 2]\n",
      "episode 4629, reward 1092.0, memory_length 2000, epsilon 0.015469633961654534, time 733.0, rides 128\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 4630, reward 895.0, memory_length 2000, epsilon 0.015455711291089044, time 733.0, rides 139\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 4631, reward 1123.0, memory_length 2000, epsilon 0.015441801150927064, time 731.0, rides 140\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 4632, reward 1004.0, memory_length 2000, epsilon 0.01542790352989123, time 728.0, rides 136\n",
      "Initial State is  [0, 9, 4]\n",
      "episode 4633, reward 649.0, memory_length 2000, epsilon 0.015414018416714328, time 728.0, rides 148\n",
      "Initial State is  [4, 11, 4]\n",
      "episode 4634, reward 815.0, memory_length 2000, epsilon 0.015400145800139285, time 724.0, rides 137\n",
      "Initial State is  [1, 11, 0]\n",
      "episode 4635, reward 833.0, memory_length 2000, epsilon 0.01538628566891916, time 732.0, rides 145\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4636, reward 847.0, memory_length 2000, epsilon 0.015372438011817131, time 734.0, rides 136\n",
      "Initial State is  [2, 20, 3]\n",
      "episode 4637, reward 835.0, memory_length 2000, epsilon 0.015358602817606495, time 728.0, rides 143\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 4638, reward 849.0, memory_length 2000, epsilon 0.01534478007507065, time 731.0, rides 138\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 4639, reward 619.0, memory_length 2000, epsilon 0.015330969773003087, time 728.0, rides 141\n",
      "Initial State is  [4, 23, 1]\n",
      "episode 4640, reward 846.0, memory_length 2000, epsilon 0.015317171900207384, time 735.0, rides 145\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 4641, reward 1059.0, memory_length 2000, epsilon 0.015303386445497197, time 736.0, rides 140\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 4642, reward 661.0, memory_length 2000, epsilon 0.01528961339769625, time 729.0, rides 156\n",
      "Initial State is  [2, 6, 0]\n",
      "episode 4643, reward 744.0, memory_length 2000, epsilon 0.015275852745638323, time 729.0, rides 151\n",
      "Initial State is  [3, 15, 5]\n",
      "episode 4644, reward 673.0, memory_length 2000, epsilon 0.01526210447816725, time 729.0, rides 134\n",
      "Initial State is  [3, 2, 2]\n",
      "episode 4645, reward 1112.0, memory_length 2000, epsilon 0.015248368584136899, time 732.0, rides 133\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 4646, reward 817.0, memory_length 2000, epsilon 0.015234645052411176, time 731.0, rides 132\n",
      "Initial State is  [3, 2, 0]\n",
      "episode 4647, reward 762.0, memory_length 2000, epsilon 0.015220933871864005, time 727.0, rides 129\n",
      "Initial State is  [1, 20, 0]\n",
      "episode 4648, reward 656.0, memory_length 2000, epsilon 0.015207235031379327, time 734.0, rides 142\n",
      "Initial State is  [1, 1, 4]\n",
      "episode 4649, reward 1142.0, memory_length 2000, epsilon 0.015193548519851085, time 741.0, rides 141\n",
      "Initial State is  [4, 13, 6]\n",
      "episode 4650, reward 967.0, memory_length 2000, epsilon 0.015179874326183219, time 727.0, rides 145\n",
      "Initial State is  [2, 17, 2]\n",
      "episode 4651, reward 977.0, memory_length 2000, epsilon 0.015166212439289653, time 728.0, rides 141\n",
      "Initial State is  [3, 15, 4]\n",
      "episode 4652, reward 829.0, memory_length 2000, epsilon 0.015152562848094292, time 732.0, rides 152\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4653, reward 897.0, memory_length 2000, epsilon 0.015138925541531007, time 731.0, rides 141\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 4654, reward 785.0, memory_length 2000, epsilon 0.015125300508543629, time 731.0, rides 155\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 4655, reward 1171.0, memory_length 2000, epsilon 0.01511168773808594, time 722.0, rides 166\n",
      "Initial State is  [2, 14, 3]\n",
      "episode 4656, reward 865.0, memory_length 2000, epsilon 0.015098087219121663, time 726.0, rides 145\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 4657, reward 1071.0, memory_length 2000, epsilon 0.015084498940624453, time 734.0, rides 142\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 4658, reward 904.0, memory_length 2000, epsilon 0.015070922891577892, time 739.0, rides 157\n",
      "Initial State is  [4, 14, 3]\n",
      "episode 4659, reward 858.0, memory_length 2000, epsilon 0.015057359060975472, time 732.0, rides 149\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 4660, reward 860.0, memory_length 2000, epsilon 0.015043807437820593, time 728.0, rides 132\n",
      "Initial State is  [2, 2, 2]\n",
      "episode 4661, reward 944.0, memory_length 2000, epsilon 0.015030268011126554, time 730.0, rides 147\n",
      "Initial State is  [0, 15, 1]\n",
      "episode 4662, reward 865.0, memory_length 2000, epsilon 0.01501674076991654, time 732.0, rides 141\n",
      "Initial State is  [3, 19, 0]\n",
      "episode 4663, reward 792.0, memory_length 2000, epsilon 0.015003225703223613, time 729.0, rides 143\n",
      "Initial State is  [2, 10, 6]\n",
      "episode 4664, reward 1053.0, memory_length 2000, epsilon 0.014989722800090711, time 731.0, rides 144\n",
      "Initial State is  [0, 7, 0]\n",
      "episode 4665, reward 852.0, memory_length 2000, epsilon 0.01497623204957063, time 728.0, rides 139\n",
      "Initial State is  [1, 6, 6]\n",
      "episode 4666, reward 899.0, memory_length 2000, epsilon 0.014962753440726015, time 725.0, rides 144\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 4667, reward 547.0, memory_length 2000, epsilon 0.01494928696262936, time 725.0, rides 135\n",
      "Initial State is  [1, 6, 5]\n",
      "episode 4668, reward 961.0, memory_length 2000, epsilon 0.014935832604362995, time 724.0, rides 126\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 4669, reward 891.0, memory_length 2000, epsilon 0.014922390355019069, time 727.0, rides 137\n",
      "Initial State is  [3, 7, 3]\n",
      "episode 4670, reward 748.0, memory_length 2000, epsilon 0.014908960203699551, time 728.0, rides 136\n",
      "Initial State is  [4, 18, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4671, reward 1032.0, memory_length 2000, epsilon 0.014895542139516221, time 734.0, rides 140\n",
      "Initial State is  [4, 22, 2]\n",
      "episode 4672, reward 1015.0, memory_length 2000, epsilon 0.014882136151590656, time 720.0, rides 133\n",
      "Initial State is  [4, 0, 5]\n",
      "episode 4673, reward 979.0, memory_length 2000, epsilon 0.014868742229054224, time 727.0, rides 147\n",
      "Initial State is  [3, 18, 0]\n",
      "episode 4674, reward 1262.0, memory_length 2000, epsilon 0.014855360361048075, time 727.0, rides 142\n",
      "Initial State is  [2, 1, 2]\n",
      "episode 4675, reward 908.0, memory_length 2000, epsilon 0.01484199053672313, time 736.0, rides 138\n",
      "Initial State is  [4, 3, 6]\n",
      "episode 4676, reward 1190.0, memory_length 2000, epsilon 0.01482863274524008, time 731.0, rides 133\n",
      "Initial State is  [3, 12, 3]\n",
      "episode 4677, reward 929.0, memory_length 2000, epsilon 0.014815286975769363, time 726.0, rides 160\n",
      "Initial State is  [0, 10, 0]\n",
      "episode 4678, reward 1010.0, memory_length 2000, epsilon 0.01480195321749117, time 726.0, rides 162\n",
      "Initial State is  [0, 2, 4]\n",
      "episode 4679, reward 693.0, memory_length 2000, epsilon 0.014788631459595428, time 727.0, rides 140\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 4680, reward 946.0, memory_length 2000, epsilon 0.014775321691281791, time 723.0, rides 142\n",
      "Initial State is  [4, 11, 1]\n",
      "episode 4681, reward 1026.0, memory_length 2000, epsilon 0.014762023901759638, time 724.0, rides 138\n",
      "Initial State is  [2, 14, 0]\n",
      "episode 4682, reward 873.0, memory_length 2000, epsilon 0.014748738080248054, time 729.0, rides 133\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4683, reward 965.0, memory_length 2000, epsilon 0.014735464215975831, time 726.0, rides 127\n",
      "Initial State is  [2, 7, 1]\n",
      "episode 4684, reward 671.0, memory_length 2000, epsilon 0.014722202298181452, time 726.0, rides 131\n",
      "Initial State is  [2, 13, 4]\n",
      "episode 4685, reward 757.0, memory_length 2000, epsilon 0.014708952316113088, time 725.0, rides 133\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 4686, reward 983.0, memory_length 2000, epsilon 0.014695714259028585, time 732.0, rides 133\n",
      "Initial State is  [3, 11, 5]\n",
      "episode 4687, reward 557.0, memory_length 2000, epsilon 0.01468248811619546, time 732.0, rides 138\n",
      "Initial State is  [0, 19, 6]\n",
      "episode 4688, reward 856.0, memory_length 2000, epsilon 0.014669273876890885, time 725.0, rides 132\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 4689, reward 672.0, memory_length 2000, epsilon 0.014656071530401682, time 732.0, rides 140\n",
      "Initial State is  [1, 0, 4]\n",
      "episode 4690, reward 805.0, memory_length 2000, epsilon 0.014642881066024321, time 722.0, rides 132\n",
      "Initial State is  [4, 6, 5]\n",
      "episode 4691, reward 953.0, memory_length 2000, epsilon 0.0146297024730649, time 728.0, rides 150\n",
      "Initial State is  [0, 21, 3]\n",
      "episode 4692, reward 695.0, memory_length 2000, epsilon 0.01461653574083914, time 723.0, rides 146\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 4693, reward 1298.0, memory_length 2000, epsilon 0.014603380858672384, time 726.0, rides 145\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 4694, reward 761.0, memory_length 2000, epsilon 0.01459023781589958, time 736.0, rides 154\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 4695, reward 687.0, memory_length 2000, epsilon 0.01457710660186527, time 725.0, rides 129\n",
      "Initial State is  [4, 10, 0]\n",
      "episode 4696, reward 804.0, memory_length 2000, epsilon 0.014563987205923591, time 730.0, rides 142\n",
      "Initial State is  [0, 21, 4]\n",
      "episode 4697, reward 873.0, memory_length 2000, epsilon 0.01455087961743826, time 724.0, rides 140\n",
      "Initial State is  [4, 7, 0]\n",
      "episode 4698, reward 727.0, memory_length 2000, epsilon 0.014537783825782564, time 728.0, rides 139\n",
      "Initial State is  [1, 11, 5]\n",
      "episode 4699, reward 585.0, memory_length 2000, epsilon 0.01452469982033936, time 729.0, rides 142\n",
      "Initial State is  [2, 20, 4]\n",
      "episode 4700, reward 762.0, memory_length 2000, epsilon 0.014511627590501053, time 721.0, rides 130\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 4701, reward 902.0, memory_length 2000, epsilon 0.014498567125669602, time 730.0, rides 143\n",
      "Initial State is  [0, 23, 0]\n",
      "episode 4702, reward 841.0, memory_length 2000, epsilon 0.014485518415256499, time 721.0, rides 152\n",
      "Initial State is  [2, 13, 0]\n",
      "episode 4703, reward 791.0, memory_length 2000, epsilon 0.014472481448682767, time 727.0, rides 138\n",
      "Initial State is  [0, 6, 5]\n",
      "episode 4704, reward 1113.0, memory_length 2000, epsilon 0.014459456215378952, time 728.0, rides 140\n",
      "Initial State is  [2, 17, 4]\n",
      "episode 4705, reward 1018.0, memory_length 2000, epsilon 0.01444644270478511, time 729.0, rides 138\n",
      "Initial State is  [3, 7, 1]\n",
      "episode 4706, reward 811.0, memory_length 2000, epsilon 0.014433440906350804, time 726.0, rides 125\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 4707, reward 682.0, memory_length 2000, epsilon 0.014420450809535088, time 731.0, rides 137\n",
      "Initial State is  [0, 13, 4]\n",
      "episode 4708, reward 687.0, memory_length 2000, epsilon 0.014407472403806507, time 728.0, rides 137\n",
      "Initial State is  [3, 15, 3]\n",
      "episode 4709, reward 738.0, memory_length 2000, epsilon 0.014394505678643081, time 731.0, rides 136\n",
      "Initial State is  [0, 22, 1]\n",
      "episode 4710, reward 803.0, memory_length 2000, epsilon 0.014381550623532302, time 732.0, rides 123\n",
      "Initial State is  [1, 13, 3]\n",
      "episode 4711, reward 1115.0, memory_length 2000, epsilon 0.014368607227971123, time 732.0, rides 131\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 4712, reward 736.0, memory_length 2000, epsilon 0.014355675481465949, time 730.0, rides 141\n",
      "Initial State is  [3, 15, 0]\n",
      "episode 4713, reward 624.0, memory_length 2000, epsilon 0.014342755373532629, time 734.0, rides 141\n",
      "Initial State is  [4, 16, 3]\n",
      "episode 4714, reward 785.0, memory_length 2000, epsilon 0.014329846893696449, time 723.0, rides 145\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 4715, reward 883.0, memory_length 2000, epsilon 0.014316950031492122, time 730.0, rides 148\n",
      "Initial State is  [2, 7, 3]\n",
      "episode 4716, reward 762.0, memory_length 2000, epsilon 0.014304064776463779, time 724.0, rides 136\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 4717, reward 860.0, memory_length 2000, epsilon 0.01429119111816496, time 728.0, rides 141\n",
      "Initial State is  [4, 17, 0]\n",
      "episode 4718, reward 914.0, memory_length 2000, epsilon 0.014278329046158611, time 738.0, rides 131\n",
      "Initial State is  [0, 23, 2]\n",
      "episode 4719, reward 991.0, memory_length 2000, epsilon 0.014265478550017068, time 731.0, rides 141\n",
      "Initial State is  [4, 3, 5]\n",
      "episode 4720, reward 616.0, memory_length 2000, epsilon 0.014252639619322053, time 726.0, rides 137\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 4721, reward 638.0, memory_length 2000, epsilon 0.014239812243664662, time 723.0, rides 125\n",
      "Initial State is  [2, 1, 3]\n",
      "episode 4722, reward 915.0, memory_length 2000, epsilon 0.014226996412645364, time 730.0, rides 134\n",
      "Initial State is  [0, 1, 3]\n",
      "episode 4723, reward 553.0, memory_length 2000, epsilon 0.014214192115873983, time 725.0, rides 138\n",
      "Initial State is  [2, 8, 3]\n",
      "episode 4724, reward 766.0, memory_length 2000, epsilon 0.014201399342969696, time 723.0, rides 141\n",
      "Initial State is  [3, 1, 5]\n",
      "episode 4725, reward 568.0, memory_length 2000, epsilon 0.014188618083561023, time 730.0, rides 142\n",
      "Initial State is  [4, 9, 2]\n",
      "episode 4726, reward 645.0, memory_length 2000, epsilon 0.014175848327285818, time 727.0, rides 146\n",
      "Initial State is  [0, 7, 2]\n",
      "episode 4727, reward 870.0, memory_length 2000, epsilon 0.01416309006379126, time 731.0, rides 131\n",
      "Initial State is  [1, 9, 1]\n",
      "episode 4728, reward 642.0, memory_length 2000, epsilon 0.014150343282733848, time 733.0, rides 145\n",
      "Initial State is  [4, 15, 4]\n",
      "episode 4729, reward 880.0, memory_length 2000, epsilon 0.014137607973779387, time 722.0, rides 135\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 4730, reward 684.0, memory_length 2000, epsilon 0.014124884126602986, time 723.0, rides 133\n",
      "Initial State is  [4, 12, 4]\n",
      "episode 4731, reward 846.0, memory_length 2000, epsilon 0.014112171730889043, time 732.0, rides 131\n",
      "Initial State is  [0, 4, 0]\n",
      "episode 4732, reward 855.0, memory_length 2000, epsilon 0.014099470776331243, time 742.0, rides 133\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 4733, reward 916.0, memory_length 2000, epsilon 0.014086781252632545, time 724.0, rides 150\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4734, reward 692.0, memory_length 2000, epsilon 0.014074103149505175, time 724.0, rides 139\n",
      "Initial State is  [3, 11, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4735, reward 812.0, memory_length 2000, epsilon 0.01406143645667062, time 726.0, rides 126\n",
      "Initial State is  [2, 16, 5]\n",
      "episode 4736, reward 699.0, memory_length 2000, epsilon 0.014048781163859617, time 723.0, rides 138\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 4737, reward 742.0, memory_length 2000, epsilon 0.014036137260812143, time 730.0, rides 130\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 4738, reward 979.0, memory_length 2000, epsilon 0.014023504737277412, time 727.0, rides 150\n",
      "Initial State is  [2, 18, 1]\n",
      "episode 4739, reward 866.0, memory_length 2000, epsilon 0.014010883583013861, time 738.0, rides 144\n",
      "Initial State is  [3, 2, 5]\n",
      "episode 4740, reward 1122.0, memory_length 2000, epsilon 0.013998273787789148, time 724.0, rides 133\n",
      "Initial State is  [0, 21, 2]\n",
      "episode 4741, reward 762.0, memory_length 2000, epsilon 0.013985675341380139, time 735.0, rides 145\n",
      "Initial State is  [2, 5, 1]\n",
      "episode 4742, reward 981.0, memory_length 2000, epsilon 0.013973088233572897, time 735.0, rides 155\n",
      "Initial State is  [0, 18, 4]\n",
      "episode 4743, reward 968.0, memory_length 2000, epsilon 0.013960512454162681, time 728.0, rides 134\n",
      "Initial State is  [0, 0, 2]\n",
      "episode 4744, reward 309.0, memory_length 2000, epsilon 0.013947947992953935, time 734.0, rides 131\n",
      "Initial State is  [2, 20, 1]\n",
      "episode 4745, reward 760.0, memory_length 2000, epsilon 0.013935394839760275, time 723.0, rides 158\n",
      "Initial State is  [2, 1, 1]\n",
      "episode 4746, reward 558.0, memory_length 2000, epsilon 0.013922852984404491, time 731.0, rides 140\n",
      "Initial State is  [0, 19, 0]\n",
      "episode 4747, reward 1129.0, memory_length 2000, epsilon 0.013910322416718527, time 733.0, rides 135\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 4748, reward 821.0, memory_length 2000, epsilon 0.01389780312654348, time 731.0, rides 134\n",
      "Initial State is  [2, 18, 5]\n",
      "episode 4749, reward 866.0, memory_length 2000, epsilon 0.013885295103729592, time 724.0, rides 134\n",
      "Initial State is  [1, 4, 2]\n",
      "episode 4750, reward 998.0, memory_length 2000, epsilon 0.013872798338136235, time 723.0, rides 131\n",
      "Initial State is  [2, 22, 1]\n",
      "episode 4751, reward 719.0, memory_length 2000, epsilon 0.013860312819631912, time 728.0, rides 135\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 4752, reward 749.0, memory_length 2000, epsilon 0.013847838538094244, time 723.0, rides 129\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4753, reward 900.0, memory_length 2000, epsilon 0.013835375483409958, time 726.0, rides 139\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 4754, reward 746.0, memory_length 2000, epsilon 0.01382292364547489, time 725.0, rides 127\n",
      "Initial State is  [3, 13, 4]\n",
      "episode 4755, reward 995.0, memory_length 2000, epsilon 0.013810483014193962, time 730.0, rides 140\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 4756, reward 826.0, memory_length 2000, epsilon 0.013798053579481188, time 724.0, rides 143\n",
      "Initial State is  [2, 23, 2]\n",
      "episode 4757, reward 904.0, memory_length 2000, epsilon 0.013785635331259654, time 728.0, rides 126\n",
      "Initial State is  [0, 18, 5]\n",
      "episode 4758, reward 1021.0, memory_length 2000, epsilon 0.01377322825946152, time 723.0, rides 145\n",
      "Initial State is  [2, 12, 0]\n",
      "episode 4759, reward 1130.0, memory_length 2000, epsilon 0.013760832354028005, time 721.0, rides 156\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 4760, reward 931.0, memory_length 2000, epsilon 0.01374844760490938, time 729.0, rides 135\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 4761, reward 967.0, memory_length 2000, epsilon 0.013736074002064962, time 732.0, rides 142\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 4762, reward 996.0, memory_length 2000, epsilon 0.013723711535463104, time 728.0, rides 149\n",
      "Initial State is  [1, 3, 6]\n",
      "episode 4763, reward 905.0, memory_length 2000, epsilon 0.013711360195081186, time 727.0, rides 139\n",
      "Initial State is  [2, 0, 0]\n",
      "episode 4764, reward 1056.0, memory_length 2000, epsilon 0.013699019970905613, time 732.0, rides 146\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 4765, reward 958.0, memory_length 2000, epsilon 0.013686690852931798, time 732.0, rides 145\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 4766, reward 866.0, memory_length 2000, epsilon 0.013674372831164159, time 736.0, rides 135\n",
      "Initial State is  [3, 22, 4]\n",
      "episode 4767, reward 759.0, memory_length 2000, epsilon 0.013662065895616112, time 729.0, rides 143\n",
      "Initial State is  [4, 5, 4]\n",
      "episode 4768, reward 989.0, memory_length 2000, epsilon 0.013649770036310058, time 724.0, rides 138\n",
      "Initial State is  [0, 14, 6]\n",
      "episode 4769, reward 1150.0, memory_length 2000, epsilon 0.013637485243277379, time 731.0, rides 152\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 4770, reward 1118.0, memory_length 2000, epsilon 0.013625211506558429, time 727.0, rides 156\n",
      "Initial State is  [4, 16, 5]\n",
      "episode 4771, reward 1122.0, memory_length 2000, epsilon 0.013612948816202525, time 728.0, rides 143\n",
      "Initial State is  [4, 6, 0]\n",
      "episode 4772, reward 932.0, memory_length 2000, epsilon 0.013600697162267942, time 733.0, rides 122\n",
      "Initial State is  [0, 20, 3]\n",
      "episode 4773, reward 771.0, memory_length 2000, epsilon 0.0135884565348219, time 729.0, rides 137\n",
      "Initial State is  [3, 19, 4]\n",
      "episode 4774, reward 960.0, memory_length 2000, epsilon 0.013576226923940561, time 727.0, rides 143\n",
      "Initial State is  [3, 17, 5]\n",
      "episode 4775, reward 885.0, memory_length 2000, epsilon 0.013564008319709015, time 727.0, rides 132\n",
      "Initial State is  [0, 19, 2]\n",
      "episode 4776, reward 1070.0, memory_length 2000, epsilon 0.013551800712221276, time 728.0, rides 136\n",
      "Initial State is  [1, 5, 4]\n",
      "episode 4777, reward 842.0, memory_length 2000, epsilon 0.013539604091580277, time 723.0, rides 131\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 4778, reward 733.0, memory_length 2000, epsilon 0.013527418447897854, time 726.0, rides 153\n",
      "Initial State is  [0, 21, 1]\n",
      "episode 4779, reward 1027.0, memory_length 2000, epsilon 0.013515243771294745, time 724.0, rides 133\n",
      "Initial State is  [4, 9, 4]\n",
      "episode 4780, reward 830.0, memory_length 2000, epsilon 0.01350308005190058, time 728.0, rides 140\n",
      "Initial State is  [0, 12, 5]\n",
      "episode 4781, reward 1013.0, memory_length 2000, epsilon 0.013490927279853869, time 727.0, rides 132\n",
      "Initial State is  [4, 3, 2]\n",
      "episode 4782, reward 531.0, memory_length 2000, epsilon 0.013478785445302, time 727.0, rides 128\n",
      "Initial State is  [0, 9, 1]\n",
      "episode 4783, reward 772.0, memory_length 2000, epsilon 0.013466654538401228, time 724.0, rides 144\n",
      "Initial State is  [1, 22, 6]\n",
      "episode 4784, reward 946.0, memory_length 2000, epsilon 0.013454534549316667, time 728.0, rides 128\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4785, reward 710.0, memory_length 2000, epsilon 0.013442425468222283, time 721.0, rides 134\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 4786, reward 713.0, memory_length 2000, epsilon 0.013430327285300882, time 733.0, rides 139\n",
      "Initial State is  [3, 23, 1]\n",
      "episode 4787, reward 839.0, memory_length 2000, epsilon 0.013418239990744112, time 734.0, rides 154\n",
      "Initial State is  [0, 4, 4]\n",
      "episode 4788, reward 991.0, memory_length 2000, epsilon 0.013406163574752442, time 733.0, rides 134\n",
      "Initial State is  [0, 3, 1]\n",
      "episode 4789, reward 850.0, memory_length 2000, epsilon 0.013394098027535165, time 731.0, rides 140\n",
      "Initial State is  [0, 5, 1]\n",
      "episode 4790, reward 975.0, memory_length 2000, epsilon 0.013382043339310383, time 731.0, rides 119\n",
      "Initial State is  [1, 21, 1]\n",
      "episode 4791, reward 605.0, memory_length 2000, epsilon 0.013369999500305004, time 723.0, rides 135\n",
      "Initial State is  [4, 23, 0]\n",
      "episode 4792, reward 698.0, memory_length 2000, epsilon 0.01335796650075473, time 734.0, rides 134\n",
      "Initial State is  [2, 13, 5]\n",
      "episode 4793, reward 835.0, memory_length 2000, epsilon 0.013345944330904051, time 725.0, rides 135\n",
      "Initial State is  [3, 7, 6]\n",
      "episode 4794, reward 904.0, memory_length 2000, epsilon 0.013333932981006238, time 728.0, rides 145\n",
      "Initial State is  [3, 5, 4]\n",
      "episode 4795, reward 718.0, memory_length 2000, epsilon 0.013321932441323332, time 729.0, rides 136\n",
      "Initial State is  [3, 7, 5]\n",
      "episode 4796, reward 1105.0, memory_length 2000, epsilon 0.01330994270212614, time 733.0, rides 134\n",
      "Initial State is  [2, 18, 0]\n",
      "episode 4797, reward 858.0, memory_length 2000, epsilon 0.013297963753694226, time 728.0, rides 137\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 4798, reward 819.0, memory_length 2000, epsilon 0.013285995586315902, time 736.0, rides 141\n",
      "Initial State is  [4, 8, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4799, reward 1010.0, memory_length 2000, epsilon 0.013274038190288218, time 731.0, rides 127\n",
      "Initial State is  [0, 17, 3]\n",
      "episode 4800, reward 789.0, memory_length 2000, epsilon 0.013262091555916958, time 725.0, rides 136\n",
      "Initial State is  [2, 21, 3]\n",
      "episode 4801, reward 691.0, memory_length 2000, epsilon 0.013250155673516633, time 725.0, rides 147\n",
      "Initial State is  [2, 15, 6]\n",
      "episode 4802, reward 976.0, memory_length 2000, epsilon 0.013238230533410467, time 720.0, rides 140\n",
      "Initial State is  [1, 19, 0]\n",
      "episode 4803, reward 801.0, memory_length 2000, epsilon 0.013226316125930398, time 725.0, rides 140\n",
      "Initial State is  [2, 12, 5]\n",
      "episode 4804, reward 942.0, memory_length 2000, epsilon 0.013214412441417061, time 728.0, rides 133\n",
      "Initial State is  [0, 23, 3]\n",
      "episode 4805, reward 793.0, memory_length 2000, epsilon 0.013202519470219786, time 739.0, rides 145\n",
      "Initial State is  [0, 14, 3]\n",
      "episode 4806, reward 949.0, memory_length 2000, epsilon 0.013190637202696589, time 725.0, rides 146\n",
      "Initial State is  [1, 9, 6]\n",
      "episode 4807, reward 870.0, memory_length 2000, epsilon 0.013178765629214162, time 728.0, rides 151\n",
      "Initial State is  [3, 12, 0]\n",
      "episode 4808, reward 975.0, memory_length 2000, epsilon 0.013166904740147868, time 726.0, rides 138\n",
      "Initial State is  [3, 20, 4]\n",
      "episode 4809, reward 653.0, memory_length 2000, epsilon 0.013155054525881735, time 728.0, rides 135\n",
      "Initial State is  [2, 19, 6]\n",
      "episode 4810, reward 762.0, memory_length 2000, epsilon 0.013143214976808442, time 724.0, rides 147\n",
      "Initial State is  [0, 22, 4]\n",
      "episode 4811, reward 964.0, memory_length 2000, epsilon 0.013131386083329314, time 728.0, rides 132\n",
      "Initial State is  [4, 22, 4]\n",
      "episode 4812, reward 820.0, memory_length 2000, epsilon 0.013119567835854317, time 727.0, rides 142\n",
      "Initial State is  [2, 6, 2]\n",
      "episode 4813, reward 569.0, memory_length 2000, epsilon 0.013107760224802048, time 732.0, rides 129\n",
      "Initial State is  [1, 21, 5]\n",
      "episode 4814, reward 790.0, memory_length 2000, epsilon 0.013095963240599726, time 726.0, rides 124\n",
      "Initial State is  [4, 17, 4]\n",
      "episode 4815, reward 800.0, memory_length 2000, epsilon 0.013084176873683186, time 726.0, rides 127\n",
      "Initial State is  [1, 2, 0]\n",
      "episode 4816, reward 907.0, memory_length 2000, epsilon 0.013072401114496871, time 731.0, rides 142\n",
      "Initial State is  [4, 9, 0]\n",
      "episode 4817, reward 695.0, memory_length 2000, epsilon 0.013060635953493823, time 726.0, rides 154\n",
      "Initial State is  [3, 14, 5]\n",
      "episode 4818, reward 542.0, memory_length 2000, epsilon 0.013048881381135679, time 730.0, rides 132\n",
      "Initial State is  [1, 11, 4]\n",
      "episode 4819, reward 444.0, memory_length 2000, epsilon 0.013037137387892656, time 723.0, rides 141\n",
      "Initial State is  [3, 8, 4]\n",
      "episode 4820, reward 733.0, memory_length 2000, epsilon 0.013025403964243553, time 720.0, rides 129\n",
      "Initial State is  [2, 14, 2]\n",
      "episode 4821, reward 944.0, memory_length 2000, epsilon 0.013013681100675733, time 726.0, rides 146\n",
      "Initial State is  [2, 0, 5]\n",
      "episode 4822, reward 1059.0, memory_length 2000, epsilon 0.013001968787685125, time 726.0, rides 148\n",
      "Initial State is  [0, 19, 3]\n",
      "episode 4823, reward 800.0, memory_length 2000, epsilon 0.012990267015776208, time 737.0, rides 131\n",
      "Initial State is  [4, 0, 6]\n",
      "episode 4824, reward 742.0, memory_length 2000, epsilon 0.01297857577546201, time 727.0, rides 129\n",
      "Initial State is  [0, 7, 6]\n",
      "episode 4825, reward 1000.0, memory_length 2000, epsilon 0.012966895057264094, time 726.0, rides 143\n",
      "Initial State is  [1, 17, 5]\n",
      "episode 4826, reward 761.0, memory_length 2000, epsilon 0.012955224851712556, time 730.0, rides 118\n",
      "Initial State is  [0, 17, 6]\n",
      "episode 4827, reward 805.0, memory_length 2000, epsilon 0.012943565149346015, time 729.0, rides 129\n",
      "Initial State is  [1, 5, 1]\n",
      "episode 4828, reward 761.0, memory_length 2000, epsilon 0.012931915940711605, time 734.0, rides 149\n",
      "Initial State is  [4, 18, 2]\n",
      "episode 4829, reward 747.0, memory_length 2000, epsilon 0.012920277216364963, time 733.0, rides 141\n",
      "Initial State is  [4, 14, 4]\n",
      "episode 4830, reward 936.0, memory_length 2000, epsilon 0.012908648966870235, time 724.0, rides 145\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 4831, reward 679.0, memory_length 2000, epsilon 0.012897031182800051, time 729.0, rides 138\n",
      "Initial State is  [3, 1, 6]\n",
      "episode 4832, reward 914.0, memory_length 2000, epsilon 0.01288542385473553, time 727.0, rides 147\n",
      "Initial State is  [2, 3, 1]\n",
      "episode 4833, reward 695.0, memory_length 2000, epsilon 0.01287382697326627, time 727.0, rides 138\n",
      "Initial State is  [4, 11, 5]\n",
      "episode 4834, reward 798.0, memory_length 2000, epsilon 0.01286224052899033, time 726.0, rides 143\n",
      "Initial State is  [4, 3, 0]\n",
      "episode 4835, reward 673.0, memory_length 2000, epsilon 0.012850664512514237, time 730.0, rides 150\n",
      "Initial State is  [2, 4, 1]\n",
      "episode 4836, reward 591.0, memory_length 2000, epsilon 0.012839098914452974, time 729.0, rides 135\n",
      "Initial State is  [4, 14, 0]\n",
      "episode 4837, reward 730.0, memory_length 2000, epsilon 0.012827543725429966, time 732.0, rides 137\n",
      "Initial State is  [1, 7, 1]\n",
      "episode 4838, reward 952.0, memory_length 2000, epsilon 0.012815998936077079, time 722.0, rides 133\n",
      "Initial State is  [3, 0, 0]\n",
      "episode 4839, reward 587.0, memory_length 2000, epsilon 0.01280446453703461, time 730.0, rides 129\n",
      "Initial State is  [2, 10, 3]\n",
      "episode 4840, reward 858.0, memory_length 2000, epsilon 0.012792940518951279, time 732.0, rides 147\n",
      "Initial State is  [2, 18, 2]\n",
      "episode 4841, reward 1314.0, memory_length 2000, epsilon 0.012781426872484222, time 732.0, rides 152\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4842, reward 833.0, memory_length 2000, epsilon 0.012769923588298987, time 735.0, rides 122\n",
      "Initial State is  [2, 4, 0]\n",
      "episode 4843, reward 938.0, memory_length 2000, epsilon 0.012758430657069518, time 729.0, rides 147\n",
      "Initial State is  [2, 10, 1]\n",
      "episode 4844, reward 785.0, memory_length 2000, epsilon 0.012746948069478155, time 735.0, rides 130\n",
      "Initial State is  [4, 11, 6]\n",
      "episode 4845, reward 713.0, memory_length 2000, epsilon 0.012735475816215624, time 730.0, rides 137\n",
      "Initial State is  [1, 7, 3]\n",
      "episode 4846, reward 1003.0, memory_length 2000, epsilon 0.01272401388798103, time 727.0, rides 143\n",
      "Initial State is  [3, 21, 4]\n",
      "episode 4847, reward 1142.0, memory_length 2000, epsilon 0.012712562275481848, time 729.0, rides 135\n",
      "Initial State is  [3, 22, 3]\n",
      "episode 4848, reward 1070.0, memory_length 2000, epsilon 0.012701120969433913, time 738.0, rides 137\n",
      "Initial State is  [3, 22, 2]\n",
      "episode 4849, reward 669.0, memory_length 2000, epsilon 0.012689689960561423, time 734.0, rides 137\n",
      "Initial State is  [4, 16, 6]\n",
      "episode 4850, reward 853.0, memory_length 2000, epsilon 0.012678269239596918, time 734.0, rides 132\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4851, reward 589.0, memory_length 2000, epsilon 0.01266685879728128, time 727.0, rides 121\n",
      "Initial State is  [1, 22, 2]\n",
      "episode 4852, reward 635.0, memory_length 2000, epsilon 0.012655458624363727, time 731.0, rides 126\n",
      "Initial State is  [3, 9, 1]\n",
      "episode 4853, reward 662.0, memory_length 2000, epsilon 0.0126440687116018, time 731.0, rides 154\n",
      "Initial State is  [2, 8, 4]\n",
      "episode 4854, reward 703.0, memory_length 2000, epsilon 0.012632689049761357, time 730.0, rides 124\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 4855, reward 925.0, memory_length 2000, epsilon 0.012621319629616571, time 724.0, rides 148\n",
      "Initial State is  [3, 23, 0]\n",
      "episode 4856, reward 1128.0, memory_length 2000, epsilon 0.012609960441949916, time 725.0, rides 146\n",
      "Initial State is  [2, 7, 2]\n",
      "episode 4857, reward 861.0, memory_length 2000, epsilon 0.01259861147755216, time 728.0, rides 142\n",
      "Initial State is  [1, 13, 2]\n",
      "episode 4858, reward 832.0, memory_length 2000, epsilon 0.012587272727222362, time 736.0, rides 137\n",
      "Initial State is  [3, 2, 1]\n",
      "episode 4859, reward 1028.0, memory_length 2000, epsilon 0.012575944181767862, time 732.0, rides 150\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 4860, reward 673.0, memory_length 2000, epsilon 0.01256462583200427, time 727.0, rides 138\n",
      "Initial State is  [1, 20, 4]\n",
      "episode 4861, reward 618.0, memory_length 2000, epsilon 0.012553317668755467, time 724.0, rides 150\n",
      "Initial State is  [2, 19, 3]\n",
      "episode 4862, reward 830.0, memory_length 2000, epsilon 0.012542019682853588, time 736.0, rides 134\n",
      "Initial State is  [2, 20, 5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4863, reward 762.0, memory_length 2000, epsilon 0.01253073186513902, time 729.0, rides 140\n",
      "Initial State is  [1, 19, 2]\n",
      "episode 4864, reward 831.0, memory_length 2000, epsilon 0.012519454206460393, time 732.0, rides 131\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 4865, reward 599.0, memory_length 2000, epsilon 0.012508186697674579, time 727.0, rides 132\n",
      "Initial State is  [0, 18, 6]\n",
      "episode 4866, reward 674.0, memory_length 2000, epsilon 0.012496929329646671, time 727.0, rides 149\n",
      "Initial State is  [0, 0, 0]\n",
      "episode 4867, reward 547.0, memory_length 2000, epsilon 0.012485682093249989, time 724.0, rides 146\n",
      "Initial State is  [2, 21, 5]\n",
      "episode 4868, reward 699.0, memory_length 2000, epsilon 0.012474444979366063, time 733.0, rides 133\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 4869, reward 806.0, memory_length 2000, epsilon 0.012463217978884633, time 730.0, rides 158\n",
      "Initial State is  [2, 20, 5]\n",
      "episode 4870, reward 853.0, memory_length 2000, epsilon 0.012452001082703636, time 724.0, rides 152\n",
      "Initial State is  [3, 8, 0]\n",
      "episode 4871, reward 627.0, memory_length 2000, epsilon 0.012440794281729202, time 739.0, rides 134\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 4872, reward 996.0, memory_length 2000, epsilon 0.012429597566875646, time 732.0, rides 135\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 4873, reward 1091.0, memory_length 2000, epsilon 0.012418410929065458, time 727.0, rides 159\n",
      "Initial State is  [1, 18, 1]\n",
      "episode 4874, reward 928.0, memory_length 2000, epsilon 0.012407234359229299, time 734.0, rides 137\n",
      "Initial State is  [2, 1, 4]\n",
      "episode 4875, reward 862.0, memory_length 2000, epsilon 0.012396067848305992, time 727.0, rides 140\n",
      "Initial State is  [1, 1, 6]\n",
      "episode 4876, reward 762.0, memory_length 2000, epsilon 0.012384911387242516, time 726.0, rides 138\n",
      "Initial State is  [2, 3, 3]\n",
      "episode 4877, reward 1041.0, memory_length 2000, epsilon 0.012373764966993998, time 731.0, rides 146\n",
      "Initial State is  [2, 11, 4]\n",
      "episode 4878, reward 510.0, memory_length 2000, epsilon 0.012362628578523703, time 721.0, rides 136\n",
      "Initial State is  [3, 5, 2]\n",
      "episode 4879, reward 715.0, memory_length 2000, epsilon 0.012351502212803032, time 727.0, rides 142\n",
      "Initial State is  [3, 3, 3]\n",
      "episode 4880, reward 838.0, memory_length 2000, epsilon 0.01234038586081151, time 731.0, rides 151\n",
      "Initial State is  [2, 23, 3]\n",
      "episode 4881, reward 799.0, memory_length 2000, epsilon 0.01232927951353678, time 735.0, rides 135\n",
      "Initial State is  [0, 5, 3]\n",
      "episode 4882, reward 904.0, memory_length 2000, epsilon 0.012318183161974597, time 731.0, rides 134\n",
      "Initial State is  [4, 5, 5]\n",
      "episode 4883, reward 614.0, memory_length 2000, epsilon 0.01230709679712882, time 724.0, rides 158\n",
      "Initial State is  [3, 10, 2]\n",
      "episode 4884, reward 763.0, memory_length 2000, epsilon 0.012296020410011403, time 731.0, rides 146\n",
      "Initial State is  [3, 11, 4]\n",
      "episode 4885, reward 683.0, memory_length 2000, epsilon 0.012284953991642393, time 727.0, rides 153\n",
      "Initial State is  [4, 22, 0]\n",
      "episode 4886, reward 409.0, memory_length 2000, epsilon 0.012273897533049914, time 729.0, rides 136\n",
      "Initial State is  [1, 2, 1]\n",
      "episode 4887, reward 845.0, memory_length 2000, epsilon 0.01226285102527017, time 730.0, rides 135\n",
      "Initial State is  [4, 18, 1]\n",
      "episode 4888, reward 975.0, memory_length 2000, epsilon 0.012251814459347426, time 731.0, rides 145\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 4889, reward 710.0, memory_length 2000, epsilon 0.012240787826334013, time 724.0, rides 144\n",
      "Initial State is  [0, 8, 4]\n",
      "episode 4890, reward 972.0, memory_length 2000, epsilon 0.012229771117290312, time 729.0, rides 148\n",
      "Initial State is  [0, 22, 2]\n",
      "episode 4891, reward 1146.0, memory_length 2000, epsilon 0.01221876432328475, time 737.0, rides 151\n",
      "Initial State is  [3, 13, 2]\n",
      "episode 4892, reward 667.0, memory_length 2000, epsilon 0.012207767435393794, time 725.0, rides 142\n",
      "Initial State is  [0, 23, 1]\n",
      "episode 4893, reward 660.0, memory_length 2000, epsilon 0.01219678044470194, time 730.0, rides 144\n",
      "Initial State is  [3, 12, 2]\n",
      "episode 4894, reward 968.0, memory_length 2000, epsilon 0.012185803342301708, time 732.0, rides 154\n",
      "Initial State is  [3, 3, 1]\n",
      "episode 4895, reward 1053.0, memory_length 2000, epsilon 0.012174836119293637, time 722.0, rides 129\n",
      "Initial State is  [0, 9, 6]\n",
      "episode 4896, reward 784.0, memory_length 2000, epsilon 0.012163878766786273, time 725.0, rides 144\n",
      "Initial State is  [2, 11, 3]\n",
      "episode 4897, reward 1012.0, memory_length 2000, epsilon 0.012152931275896166, time 726.0, rides 149\n",
      "Initial State is  [1, 4, 3]\n",
      "episode 4898, reward 889.0, memory_length 2000, epsilon 0.012141993637747858, time 722.0, rides 139\n",
      "Initial State is  [3, 8, 2]\n",
      "episode 4899, reward 925.0, memory_length 2000, epsilon 0.012131065843473884, time 725.0, rides 143\n",
      "Initial State is  [0, 16, 4]\n",
      "episode 4900, reward 936.0, memory_length 2000, epsilon 0.012120147884214758, time 730.0, rides 129\n",
      "Initial State is  [2, 13, 2]\n",
      "episode 4901, reward 965.0, memory_length 2000, epsilon 0.012109239751118965, time 725.0, rides 130\n",
      "Initial State is  [3, 23, 4]\n",
      "episode 4902, reward 790.0, memory_length 2000, epsilon 0.012098341435342958, time 723.0, rides 138\n",
      "Initial State is  [4, 0, 3]\n",
      "episode 4903, reward 651.0, memory_length 2000, epsilon 0.01208745292805115, time 727.0, rides 137\n",
      "Initial State is  [4, 14, 5]\n",
      "episode 4904, reward 899.0, memory_length 2000, epsilon 0.012076574220415904, time 727.0, rides 132\n",
      "Initial State is  [3, 10, 3]\n",
      "episode 4905, reward 1068.0, memory_length 2000, epsilon 0.01206570530361753, time 731.0, rides 137\n",
      "Initial State is  [3, 11, 0]\n",
      "episode 4906, reward 747.0, memory_length 2000, epsilon 0.012054846168844275, time 722.0, rides 148\n",
      "Initial State is  [4, 10, 4]\n",
      "episode 4907, reward 894.0, memory_length 2000, epsilon 0.012043996807292314, time 725.0, rides 142\n",
      "Initial State is  [4, 2, 3]\n",
      "episode 4908, reward 1060.0, memory_length 2000, epsilon 0.01203315721016575, time 720.0, rides 167\n",
      "Initial State is  [2, 11, 6]\n",
      "episode 4909, reward 663.0, memory_length 2000, epsilon 0.0120223273686766, time 724.0, rides 147\n",
      "Initial State is  [1, 18, 4]\n",
      "episode 4910, reward 807.0, memory_length 2000, epsilon 0.012011507274044791, time 735.0, rides 147\n",
      "Initial State is  [4, 20, 4]\n",
      "episode 4911, reward 981.0, memory_length 2000, epsilon 0.01200069691749815, time 726.0, rides 144\n",
      "Initial State is  [4, 7, 1]\n",
      "episode 4912, reward 711.0, memory_length 2000, epsilon 0.011989896290272401, time 731.0, rides 131\n",
      "Initial State is  [0, 15, 6]\n",
      "episode 4913, reward 994.0, memory_length 2000, epsilon 0.011979105383611157, time 731.0, rides 135\n",
      "Initial State is  [1, 2, 5]\n",
      "episode 4914, reward 782.0, memory_length 2000, epsilon 0.011968324188765906, time 728.0, rides 131\n",
      "Initial State is  [3, 18, 3]\n",
      "episode 4915, reward 1048.0, memory_length 2000, epsilon 0.011957552696996016, time 734.0, rides 150\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 4916, reward 768.0, memory_length 2000, epsilon 0.01194679089956872, time 729.0, rides 128\n",
      "Initial State is  [4, 2, 2]\n",
      "episode 4917, reward 647.0, memory_length 2000, epsilon 0.01193603878775911, time 722.0, rides 141\n",
      "Initial State is  [3, 17, 6]\n",
      "episode 4918, reward 854.0, memory_length 2000, epsilon 0.011925296352850126, time 727.0, rides 125\n",
      "Initial State is  [4, 3, 1]\n",
      "episode 4919, reward 818.0, memory_length 2000, epsilon 0.01191456358613256, time 726.0, rides 136\n",
      "Initial State is  [2, 4, 3]\n",
      "episode 4920, reward 822.0, memory_length 2000, epsilon 0.011903840478905041, time 724.0, rides 126\n",
      "Initial State is  [3, 1, 4]\n",
      "episode 4921, reward 1108.0, memory_length 2000, epsilon 0.011893127022474026, time 729.0, rides 137\n",
      "Initial State is  [3, 9, 3]\n",
      "episode 4922, reward 777.0, memory_length 2000, epsilon 0.0118824232081538, time 728.0, rides 138\n",
      "Initial State is  [3, 9, 2]\n",
      "episode 4923, reward 1064.0, memory_length 2000, epsilon 0.011871729027266461, time 733.0, rides 148\n",
      "Initial State is  [2, 8, 6]\n",
      "episode 4924, reward 834.0, memory_length 2000, epsilon 0.011861044471141922, time 728.0, rides 136\n",
      "Initial State is  [2, 16, 1]\n",
      "episode 4925, reward 912.0, memory_length 2000, epsilon 0.011850369531117894, time 723.0, rides 135\n",
      "Initial State is  [1, 12, 0]\n",
      "episode 4926, reward 847.0, memory_length 2000, epsilon 0.011839704198539887, time 724.0, rides 145\n",
      "Initial State is  [3, 14, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4927, reward 1119.0, memory_length 2000, epsilon 0.011829048464761202, time 727.0, rides 129\n",
      "Initial State is  [0, 5, 4]\n",
      "episode 4928, reward 876.0, memory_length 2000, epsilon 0.011818402321142917, time 735.0, rides 144\n",
      "Initial State is  [1, 7, 5]\n",
      "episode 4929, reward 1040.0, memory_length 2000, epsilon 0.011807765759053887, time 723.0, rides 142\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4930, reward 869.0, memory_length 2000, epsilon 0.011797138769870739, time 725.0, rides 141\n",
      "Initial State is  [1, 14, 5]\n",
      "episode 4931, reward 1013.0, memory_length 2000, epsilon 0.011786521344977855, time 727.0, rides 141\n",
      "Initial State is  [4, 14, 2]\n",
      "episode 4932, reward 819.0, memory_length 2000, epsilon 0.011775913475767374, time 726.0, rides 129\n",
      "Initial State is  [0, 23, 5]\n",
      "episode 4933, reward 881.0, memory_length 2000, epsilon 0.011765315153639183, time 729.0, rides 148\n",
      "Initial State is  [3, 2, 3]\n",
      "episode 4934, reward 819.0, memory_length 2000, epsilon 0.011754726370000908, time 725.0, rides 143\n",
      "Initial State is  [3, 12, 1]\n",
      "episode 4935, reward 1064.0, memory_length 2000, epsilon 0.011744147116267907, time 729.0, rides 146\n",
      "Initial State is  [3, 5, 3]\n",
      "episode 4936, reward 778.0, memory_length 2000, epsilon 0.011733577383863266, time 729.0, rides 141\n",
      "Initial State is  [3, 10, 5]\n",
      "episode 4937, reward 750.0, memory_length 2000, epsilon 0.01172301716421779, time 728.0, rides 155\n",
      "Initial State is  [1, 19, 6]\n",
      "episode 4938, reward 748.0, memory_length 2000, epsilon 0.011712466448769993, time 733.0, rides 143\n",
      "Initial State is  [0, 20, 0]\n",
      "episode 4939, reward 756.0, memory_length 2000, epsilon 0.0117019252289661, time 725.0, rides 132\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 4940, reward 899.0, memory_length 2000, epsilon 0.011691393496260031, time 736.0, rides 149\n",
      "Initial State is  [0, 14, 0]\n",
      "episode 4941, reward 1127.0, memory_length 2000, epsilon 0.011680871242113396, time 724.0, rides 135\n",
      "Initial State is  [0, 10, 1]\n",
      "episode 4942, reward 812.0, memory_length 2000, epsilon 0.011670358457995494, time 723.0, rides 125\n",
      "Initial State is  [0, 17, 0]\n",
      "episode 4943, reward 937.0, memory_length 2000, epsilon 0.011659855135383299, time 728.0, rides 135\n",
      "Initial State is  [0, 21, 0]\n",
      "episode 4944, reward 888.0, memory_length 2000, epsilon 0.011649361265761453, time 728.0, rides 134\n",
      "Initial State is  [0, 4, 2]\n",
      "episode 4945, reward 857.0, memory_length 2000, epsilon 0.011638876840622267, time 724.0, rides 153\n",
      "Initial State is  [4, 7, 2]\n",
      "episode 4946, reward 580.0, memory_length 2000, epsilon 0.011628401851465707, time 728.0, rides 148\n",
      "Initial State is  [4, 14, 6]\n",
      "episode 4947, reward 1236.0, memory_length 2000, epsilon 0.011617936289799388, time 726.0, rides 140\n",
      "Initial State is  [4, 5, 2]\n",
      "episode 4948, reward 877.0, memory_length 2000, epsilon 0.011607480147138569, time 732.0, rides 144\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 4949, reward 705.0, memory_length 2000, epsilon 0.011597033415006144, time 729.0, rides 146\n",
      "Initial State is  [4, 17, 3]\n",
      "episode 4950, reward 783.0, memory_length 2000, epsilon 0.011586596084932638, time 728.0, rides 143\n",
      "Initial State is  [2, 19, 2]\n",
      "episode 4951, reward 991.0, memory_length 2000, epsilon 0.011576168148456198, time 731.0, rides 139\n",
      "Initial State is  [4, 18, 4]\n",
      "episode 4952, reward 858.0, memory_length 2000, epsilon 0.011565749597122588, time 726.0, rides 130\n",
      "Initial State is  [2, 12, 2]\n",
      "episode 4953, reward 761.0, memory_length 2000, epsilon 0.011555340422485178, time 726.0, rides 138\n",
      "Initial State is  [1, 20, 6]\n",
      "episode 4954, reward 941.0, memory_length 2000, epsilon 0.011544940616104941, time 729.0, rides 133\n",
      "Initial State is  [0, 3, 0]\n",
      "episode 4955, reward 598.0, memory_length 2000, epsilon 0.011534550169550448, time 722.0, rides 130\n",
      "Initial State is  [4, 12, 2]\n",
      "episode 4956, reward 749.0, memory_length 2000, epsilon 0.011524169074397852, time 724.0, rides 148\n",
      "Initial State is  [0, 5, 5]\n",
      "episode 4957, reward 810.0, memory_length 2000, epsilon 0.011513797322230894, time 737.0, rides 161\n",
      "Initial State is  [3, 21, 1]\n",
      "episode 4958, reward 799.0, memory_length 2000, epsilon 0.011503434904640886, time 731.0, rides 142\n",
      "Initial State is  [2, 23, 1]\n",
      "episode 4959, reward 887.0, memory_length 2000, epsilon 0.011493081813226709, time 725.0, rides 149\n",
      "Initial State is  [3, 4, 5]\n",
      "episode 4960, reward 743.0, memory_length 2000, epsilon 0.011482738039594804, time 722.0, rides 134\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 4961, reward 398.0, memory_length 2000, epsilon 0.011472403575359169, time 728.0, rides 131\n",
      "Initial State is  [0, 20, 6]\n",
      "episode 4962, reward 833.0, memory_length 2000, epsilon 0.011462078412141346, time 741.0, rides 148\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 4963, reward 720.0, memory_length 2000, epsilon 0.011451762541570418, time 724.0, rides 154\n",
      "Initial State is  [1, 22, 0]\n",
      "episode 4964, reward 1125.0, memory_length 2000, epsilon 0.011441455955283003, time 725.0, rides 128\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 4965, reward 292.0, memory_length 2000, epsilon 0.011431158644923249, time 730.0, rides 128\n",
      "Initial State is  [4, 16, 4]\n",
      "episode 4966, reward 746.0, memory_length 2000, epsilon 0.011420870602142818, time 723.0, rides 147\n",
      "Initial State is  [2, 21, 1]\n",
      "episode 4967, reward 602.0, memory_length 2000, epsilon 0.01141059181860089, time 724.0, rides 139\n",
      "Initial State is  [1, 12, 1]\n",
      "episode 4968, reward 720.0, memory_length 2000, epsilon 0.011400322285964149, time 728.0, rides 150\n",
      "Initial State is  [4, 19, 0]\n",
      "episode 4969, reward 652.0, memory_length 2000, epsilon 0.01139006199590678, time 726.0, rides 143\n",
      "Initial State is  [2, 10, 0]\n",
      "episode 4970, reward 845.0, memory_length 2000, epsilon 0.011379810940110464, time 736.0, rides 138\n",
      "Initial State is  [0, 10, 3]\n",
      "episode 4971, reward 796.0, memory_length 2000, epsilon 0.011369569110264365, time 733.0, rides 153\n",
      "Initial State is  [0, 11, 5]\n",
      "episode 4972, reward 791.0, memory_length 2000, epsilon 0.011359336498065127, time 730.0, rides 156\n",
      "Initial State is  [1, 5, 6]\n",
      "episode 4973, reward 739.0, memory_length 2000, epsilon 0.011349113095216868, time 730.0, rides 139\n",
      "Initial State is  [2, 22, 3]\n",
      "episode 4974, reward 916.0, memory_length 2000, epsilon 0.011338898893431173, time 734.0, rides 129\n",
      "Initial State is  [1, 8, 6]\n",
      "episode 4975, reward 1158.0, memory_length 2000, epsilon 0.011328693884427084, time 726.0, rides 129\n",
      "Initial State is  [2, 22, 2]\n",
      "episode 4976, reward 601.0, memory_length 2000, epsilon 0.0113184980599311, time 728.0, rides 139\n",
      "Initial State is  [3, 23, 2]\n",
      "episode 4977, reward 863.0, memory_length 2000, epsilon 0.01130831141167716, time 727.0, rides 135\n",
      "Initial State is  [3, 0, 6]\n",
      "episode 4978, reward 919.0, memory_length 2000, epsilon 0.011298133931406652, time 728.0, rides 135\n",
      "Initial State is  [1, 4, 1]\n",
      "episode 4979, reward 993.0, memory_length 2000, epsilon 0.011287965610868386, time 727.0, rides 153\n",
      "Initial State is  [4, 23, 5]\n",
      "episode 4980, reward 723.0, memory_length 2000, epsilon 0.011277806441818604, time 725.0, rides 132\n",
      "Initial State is  [1, 3, 0]\n",
      "episode 4981, reward 1025.0, memory_length 2000, epsilon 0.011267656416020967, time 726.0, rides 124\n",
      "Initial State is  [0, 14, 1]\n",
      "episode 4982, reward 1009.0, memory_length 2000, epsilon 0.011257515525246549, time 725.0, rides 140\n",
      "Initial State is  [4, 2, 6]\n",
      "episode 4983, reward 901.0, memory_length 2000, epsilon 0.011247383761273827, time 730.0, rides 142\n",
      "Initial State is  [2, 5, 0]\n",
      "episode 4984, reward 1071.0, memory_length 2000, epsilon 0.01123726111588868, time 723.0, rides 143\n",
      "Initial State is  [0, 13, 1]\n",
      "episode 4985, reward 641.0, memory_length 2000, epsilon 0.01122714758088438, time 725.0, rides 145\n",
      "Initial State is  [1, 4, 0]\n",
      "episode 4986, reward 519.0, memory_length 2000, epsilon 0.011217043148061585, time 723.0, rides 122\n",
      "Initial State is  [2, 19, 5]\n",
      "episode 4987, reward 1100.0, memory_length 2000, epsilon 0.01120694780922833, time 726.0, rides 152\n",
      "Initial State is  [3, 9, 4]\n",
      "episode 4988, reward 694.0, memory_length 2000, epsilon 0.011196861556200024, time 736.0, rides 161\n",
      "Initial State is  [0, 7, 5]\n",
      "episode 4989, reward 1037.0, memory_length 2000, epsilon 0.011186784380799444, time 731.0, rides 153\n",
      "Initial State is  [0, 19, 1]\n",
      "episode 4990, reward 562.0, memory_length 2000, epsilon 0.011176716274856724, time 736.0, rides 133\n",
      "Initial State is  [0, 23, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 4991, reward 1010.0, memory_length 2000, epsilon 0.011166657230209353, time 734.0, rides 135\n",
      "Initial State is  [4, 18, 3]\n",
      "episode 4992, reward 915.0, memory_length 2000, epsilon 0.011156607238702165, time 727.0, rides 141\n",
      "Initial State is  [1, 15, 2]\n",
      "episode 4993, reward 934.0, memory_length 2000, epsilon 0.011146566292187332, time 727.0, rides 136\n",
      "Initial State is  [1, 0, 5]\n",
      "episode 4994, reward 1175.0, memory_length 2000, epsilon 0.011136534382524363, time 735.0, rides 150\n",
      "Initial State is  [0, 14, 2]\n",
      "episode 4995, reward 862.0, memory_length 2000, epsilon 0.01112651150158009, time 728.0, rides 145\n",
      "Initial State is  [1, 3, 4]\n",
      "episode 4996, reward 771.0, memory_length 2000, epsilon 0.011116497641228669, time 728.0, rides 130\n",
      "Initial State is  [4, 11, 0]\n",
      "episode 4997, reward 667.0, memory_length 2000, epsilon 0.011106492793351562, time 732.0, rides 144\n",
      "Initial State is  [4, 23, 2]\n",
      "episode 4998, reward 983.0, memory_length 2000, epsilon 0.011096496949837546, time 728.0, rides 141\n",
      "Initial State is  [2, 16, 4]\n",
      "episode 4999, reward 672.0, memory_length 2000, epsilon 0.011086510102582693, time 729.0, rides 141\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(36,21)\n",
    "rewards_per_episode, episodes = [], []\n",
    "\n",
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    env = CabDriver()\n",
    "    # Call all the initialised variables of the environment\n",
    "    state_space = env.state_space\n",
    "    action_space = env.action_space\n",
    "    state = env.state_init\n",
    "    print(\"Initial State is \",state)\n",
    "    time = 0\n",
    "    #Call the DQN agent\n",
    "    terminal_state = False\n",
    "    score = 0\n",
    "    action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "    score += env.reward_func(state,action_space[action],Time_matrix)\n",
    "    next_state,ride_time = env.next_state_func(state,action_space[action],Time_matrix)\n",
    "    time += ride_time\n",
    "    if time >= 24*30:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),True)\n",
    "    else:\n",
    "        agent.append_sample(env.state_encod_arch1(state),action,score,env.state_encod_arch1(next_state),False)\n",
    "    loop = 0\n",
    "    \n",
    "    while not terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        \n",
    "        if time >= 24*30:\n",
    "            terminal_state = True\n",
    "            pass\n",
    "        state = next_state\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(env.state_encod_arch1(state),env)\n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward_curr_ride = env.reward_func(state,action_space[action],Time_matrix)\n",
    "        score+= reward_curr_ride\n",
    "        next_state,ride_time = env.next_state_func(next_state,action_space[action],Time_matrix)\n",
    "        time += ride_time\n",
    "        # 3. Append the experience to the memory\n",
    "        if time >= 24*30:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),True)\n",
    "        else:\n",
    "            agent.append_sample(env.state_encod_arch1(state),action,reward_curr_ride,env.state_encod_arch1(next_state),False)\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        agent.train_model(env)\n",
    "        #print('Time elapsed {} and current loop {}'.format(time,loop))\n",
    "        loop+= 1\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "    \n",
    "    rewards_per_episode.append(score)   \n",
    "    episodes.append(episode)\n",
    "    \n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    # every episode:\n",
    "    print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3}, time {4}, rides {5}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon,time,loop))\n",
    "    # every few episodes:\n",
    "    if episode % 1000 == 0:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "\n",
    "        # save model weights\n",
    "        agent.save(name=\"model_weights.h5\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x636492940>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episodes,rewards_per_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is evident from the above graph that the rewards increased and stabilised after 3000 epochs as given by the epsilon function at the end of fileÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon2=[]\n",
    "for i in range(0,10000):\n",
    "    if i==0:\n",
    "        epsilon2.append(1)\n",
    "    else:\n",
    "        epsilon2.append(0.9991 * epsilon2[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(time, epsilon2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is evident from the above graph that epsilon decays and stabilized after 3000 epochs as required by the previous epsilon decay graph. It has been used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
