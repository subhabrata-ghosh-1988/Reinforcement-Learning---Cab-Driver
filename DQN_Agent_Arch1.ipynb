{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "DQN_Agent_Arch1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/blob/main/DQN_Agent_Arch1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Ij1vxZQ6Ix"
      },
      "source": [
        "### Cab-Driver Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV-YeUj6Q9B3",
        "outputId": "d6e83573-959d-4f05-bc90-bad57e1d585d"
      },
      "source": [
        "!pip install requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8s99idSRm7H"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/main/Env.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "with open('Env.py','w') as f:\n",
        "     f.write(r.text)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWfJ0Z5QQ6JE"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from collections import deque\n",
        "import collections\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# for building DQN model\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# for plotting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the environment\n",
        "from Env import CabDriver"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsxhZZvsQ6JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe11166-4239-4c24-8b87-8fced0a4bf72"
      },
      "source": [
        "!wget https://github.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/raw/main/TM.npy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-04 03:32:59--  https://github.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/raw/main/TM.npy\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/main/TM.npy [following]\n",
            "--2021-05-04 03:32:59--  https://raw.githubusercontent.com/subhabrata-ghosh-1988/Reinforcement-Learning---Cab-Driver/main/TM.npy\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33728 (33K) [application/octet-stream]\n",
            "Saving to: ‘TM.npy’\n",
            "\n",
            "\rTM.npy                0%[                    ]       0  --.-KB/s               \rTM.npy              100%[===================>]  32.94K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2021-05-04 03:32:59 (13.1 MB/s) - ‘TM.npy’ saved [33728/33728]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWx99N1vmze6"
      },
      "source": [
        "#tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blTVtc4MQ6JK"
      },
      "source": [
        "#### Defining Time Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGCYPABJQ6JM"
      },
      "source": [
        "# Loading the time matrix provided\n",
        "Time_matrix = np.load(\"TM.npy\", allow_pickle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573Lxpckb6dO"
      },
      "source": [
        "#### Check what the max, min and mean time values are. This will help us in defining the 'next_step' function in the Environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3OrekPlbwp4",
        "outputId": "b6f39a1c-d4f1-45ab-daf1-8ee8369cbac9"
      },
      "source": [
        "print(type(Time_matrix))\n",
        "print(Time_matrix.max())\n",
        "print(Time_matrix.min())\n",
        "print(Time_matrix.mean())\n",
        "print(Time_matrix.var())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "11.0\n",
            "0.0\n",
            "3.0542857142857143\n",
            "7.93705306122449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cBe6IZqcDbF"
      },
      "source": [
        "#### Since the max time is 11 hours between any 2 points, the next state of the cab driver may increase at most by 1 day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJvDTRo3Q6JN"
      },
      "source": [
        "#### Tracking the state-action pairs for checking convergence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQnwpFsrp3Tv"
      },
      "source": [
        "#Defining a function to save the Q-dictionary as a pickle file\n",
        "def save_obj(obj, name ):\n",
        "    with open(name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUeWmqVJQ6JS"
      },
      "source": [
        "### Agent Class\n",
        "\n",
        "If you are using this framework, you need to fill the following to complete the following code block:\n",
        "1. State and Action Size\n",
        "2. Hyperparameters\n",
        "3. Create a neural-network model in function 'build_model()'\n",
        "4. Define epsilon-greedy strategy in function 'get_action()'\n",
        "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
        "6. Complete the 'train_model()' function with following logic:\n",
        "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
        "      - Initialise your input and output batch for training the model\n",
        "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
        "      - Get Q(s', a) values from the last trained model\n",
        "      - Update the input batch as your encoded state and output batch as your Q-values\n",
        "      - Then fit your DQN model using the updated input and output batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCagoCqNQ6JU"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        # Define size of state and action\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Write here: Specify you hyper parameters for the DQN\n",
        "        self.discount_factor = 0.95\n",
        "        self.learning_rate = 0.01\n",
        "        self.epsilon = 1\n",
        "        self.epsilon_max = 1\n",
        "        self.epsilon_decay = -0.0005 #for 15k\n",
        "        #self.epsilon_decay = -0.00015 #for 20k\n",
        "        self.epsilon_min = 0.00001\n",
        "        \n",
        "        self.batch_size = 32\n",
        "\n",
        "        # create replay memory using deque\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        # Initialize the value of the states tracked\n",
        "        self.states_tracked = []\n",
        "        \n",
        "        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n",
        "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
        "\n",
        "        # create main model and target model\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    # approximate Q function using Neural Network\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Function that takes in the agent and constructs the network\n",
        "        to train it\n",
        "        @return model\n",
        "        @params agent\n",
        "        \"\"\"\n",
        "        input_shape = self.state_size\n",
        "        model = Sequential()\n",
        "        # Write your code here: Add layers to your neural nets       \n",
        "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
        "        # the output layer: output is of size num_actions\n",
        "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        model.summary\n",
        "        return model\n",
        "\n",
        "    def get_action(self, state, possible_actions_index, actions):\n",
        "        \"\"\"\n",
        "        get action in a state according to an epsilon-greedy approach\n",
        "        possible_actions_index, actions are the 'ride requests' that teh driver got.\n",
        "        \"\"\"        \n",
        "        # get action from model using epsilon-greedy policy\n",
        "        # Decay in ε after each episode       \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            # explore: choose a random action from the ride requests\n",
        "            return random.choice(possible_actions_index)\n",
        "        else:\n",
        "            # choose the action with the highest q(s, a)\n",
        "            # the first index corresponds to the batch size, so\n",
        "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
        "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
        "\n",
        "            # Use the model to predict the Q_values.\n",
        "            q_value = self.model.predict(state)\n",
        "\n",
        "            # truncate the array to only those actions that are part of the ride  requests.\n",
        "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
        "\n",
        "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
        "\n",
        "    def append_sample(self, state, action_index, reward, next_state, done):\n",
        "        \"\"\"appends the new agent run output to replay buffer\"\"\"\n",
        "        self.memory.append((state, action_index, reward, next_state, done))\n",
        "        \n",
        "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
        "    def train_model(self):\n",
        "        \"\"\" \n",
        "        Function to train the model on eacg step run.\n",
        "        Picks the random memory events according to batch size and \n",
        "        runs it through the network to train it.\n",
        "        \"\"\"\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            # Sample batch from the memory\n",
        "            mini_batch = random.sample(self.memory, self.batch_size)\n",
        "            # initialise two matrices - update_input and update_output\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\n",
        "            actions, rewards, done = [], [], []\n",
        "\n",
        "            # populate update_input and update_output and the lists rewards, actions, done\n",
        "            for i in range(self.batch_size):\n",
        "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
        "                update_input[i] = env.state_encod_arch1(state)     \n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                update_output[i] = env.state_encod_arch1(next_state)\n",
        "                done.append(done_boolean)\n",
        "\n",
        "            # predict the target q-values from states s\n",
        "            target = self.model.predict(update_input)\n",
        "            # target for q-network\n",
        "            target_qval = self.model.predict(update_output)\n",
        "\n",
        "\n",
        "            # update the target values\n",
        "            for i in range(self.batch_size):\n",
        "                if done[i]:\n",
        "                    target[i][actions[i]] = rewards[i]\n",
        "                else: # non-terminal state\n",
        "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
        "            # model fit\n",
        "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
        "            \n",
        "    def save_tracking_states(self):\n",
        "        # Use the model to predict the q_value of the state we are tacking.\n",
        "        q_value = self.model.predict(self.track_state)\n",
        "        \n",
        "        # Grab the q_value of the action index that we are tracking.\n",
        "        self.states_tracked.append(q_value[0][2])\n",
        "        \n",
        "    def save_test_states(self):\n",
        "        # Use the model to predict the q_value of the state we are tacking.\n",
        "        q_value = self.model.predict(self.track_state)\n",
        "        \n",
        "        # Grab the q_value of the action index that we are tracking.\n",
        "        self.states_test.append(q_value[0][2])\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save(name)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSRJU7n8Q6JW"
      },
      "source": [
        "### DQN block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WTQx9gYeNTQ"
      },
      "source": [
        "for episode in range(Episodes):\n",
        "\n",
        "    # Write code here\n",
        "    # Call the environment\n",
        "    # Call all the initialised variables of the environment\n",
        "    \n",
        "\n",
        "    #Call the DQN agent\n",
        "    \n",
        "    \n",
        "    while !terminal_state:\n",
        "        \n",
        "        # Write your code here\n",
        "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
        "        # 2. Evaluate your reward and next state\n",
        "        # 3. Append the experience to the memory\n",
        "        # 4. Train the model by calling function agent.train_model\n",
        "        # 5. Keep a track of rewards, Q-values, loss\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKOKQKCceIHB"
      },
      "source": [
        "episode_time = 24*30 #30 days before which car has to be recharged\n",
        "n_episodes = 15000\n",
        "m = 5\n",
        "t = 24\n",
        "d = 7\n",
        "\n",
        "# Invoke Env class\n",
        "env = CabDriver()\n",
        "action_space, state_space, state = env.reset()\n",
        "\n",
        "# Set up state and action sizes.\n",
        "state_size = m+t+d\n",
        "action_size = len(action_space)\n",
        "\n",
        "# Invoke agent class\n",
        "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
        "\n",
        "# to store rewards in each episode\n",
        "rewards_per_episode, episodes = [], []\n",
        "# Rewards for state [0,0,0] being tracked.\n",
        "rewards_init_state = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-LjiJlpeiJU"
      },
      "source": [
        "#### Run the episodes, build up replay buffer and train the model.\n",
        "Note:\n",
        "The moment total episode time exceeds 720 (30 days), we ignore the most recent ride and do NOT save that experience in the replay memory\n",
        "The init state is randomly picked from the state space for each episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUED4EPpQ6JX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d1eef8-7876-4d1b-98fc-e867956139e1"
      },
      "source": [
        "start_time = time.time()\n",
        "score_tracked = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "\n",
        "    done = False\n",
        "    score = 0\n",
        "    track_reward = False\n",
        "\n",
        "    # reset at the start of each episode\n",
        "    env = CabDriver()\n",
        "    action_space, state_space, state = env.reset()\n",
        "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
        "    initial_state = env.state_init\n",
        "\n",
        "\n",
        "    total_time = 0  # Total time driver rode in this episode\n",
        "    while not done:\n",
        "        # 1. Get a list of the ride requests driver got.\n",
        "        possible_actions_indices, actions = env.requests(state)\n",
        "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
        "        action = agent.get_action(state, possible_actions_indices, actions)\n",
        "\n",
        "        # 3. Evaluate your reward and next state\n",
        "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
        "        # 4. Total time driver rode in this episode\n",
        "        total_time += step_time\n",
        "        if (total_time > episode_time):\n",
        "            # if ride does not complete in stipu;ated time skip\n",
        "            # it and move to next episode.\n",
        "            done = True\n",
        "        else:\n",
        "            # 5. Append the experience to the memory\n",
        "            agent.append_sample(state, action, reward, next_state, done)\n",
        "            # 6. Train the model by calling function agent.train_model\n",
        "            agent.train_model()\n",
        "            # 7. Keep a track of rewards, Q-values, loss\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "    # store total reward obtained in this episode\n",
        "    rewards_per_episode.append(score)\n",
        "    episodes.append(episode)\n",
        "    \n",
        "\n",
        "    # epsilon decay\n",
        "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
        "\n",
        "    # every 10 episodes:\n",
        "    if ((episode + 1) % 10 == 0):\n",
        "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
        "                                                                         score,\n",
        "                                                                         len(agent.memory),\n",
        "                                                                         agent.epsilon, total_time))\n",
        "    # Save the Q_value of the state, action pair we are tracking\n",
        "    if ((episode + 1) % 5 == 0):\n",
        "        agent.save_tracking_states()\n",
        "\n",
        "    # Total rewards per episode\n",
        "    score_tracked.append(score)\n",
        "\n",
        "    if(episode % 1000 == 0):\n",
        "        print(\"Saving Model {}\".format(episode))\n",
        "        agent.save(name=\"model_weights.h5\")\n",
        "    \n",
        "elapsed_time = time.time() - start_time\n",
        "print(elapsed_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving Model 0\n",
            "episode 9, reward 90.0, memory_length 1363, epsilon 0.9955001547284723 total_time 721.0\n",
            "episode 19, reward -70.0, memory_length 2000, epsilon 0.9905350769930761 total_time 723.0\n",
            "episode 29, reward -192.0, memory_length 2000, epsilon 0.9855947626861951 total_time 728.0\n",
            "episode 39, reward -423.0, memory_length 2000, epsilon 0.9806790882997144 total_time 727.0\n",
            "episode 49, reward -201.0, memory_length 2000, epsilon 0.9757879309415182 total_time 724.0\n",
            "episode 59, reward 36.0, memory_length 2000, epsilon 0.9709211683324178 total_time 730.0\n",
            "episode 69, reward 116.0, memory_length 2000, epsilon 0.9660786788030947 total_time 726.0\n",
            "episode 79, reward -224.0, memory_length 2000, epsilon 0.9612603412910584 total_time 722.0\n",
            "episode 89, reward -31.0, memory_length 2000, epsilon 0.9564660353376199 total_time 727.0\n",
            "episode 99, reward 27.0, memory_length 2000, epsilon 0.9516956410848808 total_time 721.0\n",
            "episode 109, reward 28.0, memory_length 2000, epsilon 0.9469490392727365 total_time 723.0\n",
            "episode 119, reward -246.0, memory_length 2000, epsilon 0.9422261112358942 total_time 728.0\n",
            "episode 129, reward -35.0, memory_length 2000, epsilon 0.9375267389009072 total_time 725.0\n",
            "episode 139, reward 36.0, memory_length 2000, epsilon 0.9328508047832221 total_time 721.0\n",
            "episode 149, reward -241.0, memory_length 2000, epsilon 0.9281981919842428 total_time 727.0\n",
            "episode 159, reward 258.0, memory_length 2000, epsilon 0.9235687841884068 total_time 726.0\n",
            "episode 169, reward -202.0, memory_length 2000, epsilon 0.918962465660278 total_time 723.0\n",
            "episode 179, reward 238.0, memory_length 2000, epsilon 0.9143791212416534 total_time 721.0\n",
            "episode 189, reward -17.0, memory_length 2000, epsilon 0.9098186363486838 total_time 722.0\n",
            "episode 199, reward 226.0, memory_length 2000, epsilon 0.9052808969690094 total_time 724.0\n",
            "episode 209, reward 135.0, memory_length 2000, epsilon 0.9007657896589091 total_time 722.0\n",
            "episode 219, reward 37.0, memory_length 2000, epsilon 0.8962732015404654 total_time 724.0\n",
            "episode 229, reward -107.0, memory_length 2000, epsilon 0.891803020298741 total_time 725.0\n",
            "episode 239, reward -27.0, memory_length 2000, epsilon 0.8873551341789723 total_time 721.0\n",
            "episode 249, reward -234.0, memory_length 2000, epsilon 0.8829294319837746 total_time 731.0\n",
            "episode 259, reward 144.0, memory_length 2000, epsilon 0.8785258030703623 total_time 724.0\n",
            "episode 269, reward 280.0, memory_length 2000, epsilon 0.8741441373477834 total_time 724.0\n",
            "episode 279, reward -61.0, memory_length 2000, epsilon 0.8697843252741666 total_time 726.0\n",
            "episode 289, reward 245.0, memory_length 2000, epsilon 0.8654462578539829 total_time 724.0\n",
            "episode 299, reward 169.0, memory_length 2000, epsilon 0.8611298266353209 total_time 725.0\n",
            "episode 309, reward 104.0, memory_length 2000, epsilon 0.8568349237071754 total_time 722.0\n",
            "episode 319, reward 265.0, memory_length 2000, epsilon 0.8525614416967494 total_time 723.0\n",
            "episode 329, reward 28.0, memory_length 2000, epsilon 0.84830927376677 total_time 722.0\n",
            "episode 339, reward -9.0, memory_length 2000, epsilon 0.8440783136128177 total_time 728.0\n",
            "episode 349, reward 248.0, memory_length 2000, epsilon 0.8398684554606681 total_time 732.0\n",
            "episode 359, reward 46.0, memory_length 2000, epsilon 0.8356795940636483 total_time 728.0\n",
            "episode 369, reward 83.0, memory_length 2000, epsilon 0.8315116247000052 total_time 728.0\n",
            "episode 379, reward -66.0, memory_length 2000, epsilon 0.8273644431702872 total_time 727.0\n",
            "episode 389, reward -6.0, memory_length 2000, epsilon 0.8232379457947406 total_time 725.0\n",
            "episode 399, reward -44.0, memory_length 2000, epsilon 0.819132029410716 total_time 723.0\n",
            "episode 409, reward 93.0, memory_length 2000, epsilon 0.8150465913700896 total_time 722.0\n",
            "episode 419, reward 132.0, memory_length 2000, epsilon 0.8109815295366979 total_time 733.0\n",
            "episode 429, reward 273.0, memory_length 2000, epsilon 0.8069367422837833 total_time 722.0\n",
            "episode 439, reward 50.0, memory_length 2000, epsilon 0.8029121284914538 total_time 722.0\n",
            "episode 449, reward 6.0, memory_length 2000, epsilon 0.7989075875441549 total_time 723.0\n",
            "episode 459, reward -98.0, memory_length 2000, epsilon 0.7949230193281545 total_time 726.0\n",
            "episode 469, reward -69.0, memory_length 2000, epsilon 0.7909583242290396 total_time 721.0\n",
            "episode 479, reward -130.0, memory_length 2000, epsilon 0.7870134031292261 total_time 733.0\n",
            "episode 489, reward -27.0, memory_length 2000, epsilon 0.7830881574054811 total_time 730.0\n",
            "episode 499, reward -360.0, memory_length 2000, epsilon 0.7791824889264571 total_time 729.0\n",
            "episode 509, reward -163.0, memory_length 2000, epsilon 0.7752963000502389 total_time 721.0\n",
            "episode 519, reward -140.0, memory_length 2000, epsilon 0.7714294936219019 total_time 723.0\n",
            "episode 529, reward -82.0, memory_length 2000, epsilon 0.7675819729710842 total_time 722.0\n",
            "episode 539, reward -153.0, memory_length 2000, epsilon 0.763753641909569 total_time 728.0\n",
            "episode 549, reward 63.0, memory_length 2000, epsilon 0.7599444047288803 total_time 734.0\n",
            "episode 559, reward 11.0, memory_length 2000, epsilon 0.7561541661978903 total_time 723.0\n",
            "episode 569, reward -434.0, memory_length 2000, epsilon 0.7523828315604384 total_time 721.0\n",
            "episode 579, reward -54.0, memory_length 2000, epsilon 0.7486303065329623 total_time 730.0\n",
            "episode 589, reward 144.0, memory_length 2000, epsilon 0.7448964973021404 total_time 724.0\n",
            "episode 599, reward -184.0, memory_length 2000, epsilon 0.7411813105225479 total_time 723.0\n",
            "episode 609, reward 222.0, memory_length 2000, epsilon 0.7374846533143217 total_time 724.0\n",
            "episode 619, reward -34.0, memory_length 2000, epsilon 0.733806433260839 total_time 729.0\n",
            "episode 629, reward 114.0, memory_length 2000, epsilon 0.7301465584064071 total_time 732.0\n",
            "episode 639, reward 410.0, memory_length 2000, epsilon 0.7265049372539636 total_time 730.0\n",
            "episode 649, reward -49.0, memory_length 2000, epsilon 0.7228814787627905 total_time 726.0\n",
            "episode 659, reward 121.0, memory_length 2000, epsilon 0.7192760923462365 total_time 725.0\n",
            "episode 669, reward 9.0, memory_length 2000, epsilon 0.7156886878694535 total_time 729.0\n",
            "episode 679, reward 93.0, memory_length 2000, epsilon 0.7121191756471427 total_time 726.0\n",
            "episode 689, reward 609.0, memory_length 2000, epsilon 0.7085674664413126 total_time 723.0\n",
            "episode 699, reward -151.0, memory_length 2000, epsilon 0.7050334714590482 total_time 721.0\n",
            "episode 709, reward 423.0, memory_length 2000, epsilon 0.7015171023502909 total_time 730.0\n",
            "episode 719, reward 432.0, memory_length 2000, epsilon 0.6980182712056295 total_time 723.0\n",
            "episode 729, reward -61.0, memory_length 2000, epsilon 0.6945368905541035 total_time 721.0\n",
            "episode 739, reward 279.0, memory_length 2000, epsilon 0.6910728733610152 total_time 731.0\n",
            "episode 749, reward 90.0, memory_length 2000, epsilon 0.6876261330257543 total_time 725.0\n",
            "episode 759, reward 379.0, memory_length 2000, epsilon 0.684196583379633 total_time 726.0\n",
            "episode 769, reward 119.0, memory_length 2000, epsilon 0.6807841386837313 total_time 727.0\n",
            "episode 779, reward 231.0, memory_length 2000, epsilon 0.6773887136267543 total_time 721.0\n",
            "episode 789, reward 144.0, memory_length 2000, epsilon 0.6740102233228988 total_time 721.0\n",
            "episode 799, reward 693.0, memory_length 2000, epsilon 0.670648583309731 total_time 726.0\n",
            "episode 809, reward 433.0, memory_length 2000, epsilon 0.6673037095460755 total_time 721.0\n",
            "episode 819, reward 405.0, memory_length 2000, epsilon 0.6639755184099142 total_time 721.0\n",
            "episode 829, reward 227.0, memory_length 2000, epsilon 0.6606639266962953 total_time 726.0\n",
            "episode 839, reward 638.0, memory_length 2000, epsilon 0.6573688516152534 total_time 724.0\n",
            "episode 849, reward 563.0, memory_length 2000, epsilon 0.6540902107897397 total_time 721.0\n",
            "episode 859, reward 365.0, memory_length 2000, epsilon 0.6508279222535631 total_time 721.0\n",
            "episode 869, reward 609.0, memory_length 2000, epsilon 0.64758190444934 total_time 729.0\n",
            "episode 879, reward 464.0, memory_length 2000, epsilon 0.6443520762264566 total_time 725.0\n",
            "episode 889, reward 128.0, memory_length 2000, epsilon 0.6411383568390387 total_time 721.0\n",
            "episode 899, reward -49.0, memory_length 2000, epsilon 0.6379406659439346 total_time 727.0\n",
            "episode 909, reward 190.0, memory_length 2000, epsilon 0.6347589235987051 total_time 724.0\n",
            "episode 919, reward 387.0, memory_length 2000, epsilon 0.631593050259626 total_time 721.0\n",
            "episode 929, reward 683.0, memory_length 2000, epsilon 0.6284429667796988 total_time 729.0\n",
            "episode 939, reward 268.0, memory_length 2000, epsilon 0.6253085944066726 total_time 726.0\n",
            "episode 949, reward 709.0, memory_length 2000, epsilon 0.6221898547810748 total_time 727.0\n",
            "episode 959, reward 276.0, memory_length 2000, epsilon 0.6190866699342522 total_time 721.0\n",
            "episode 969, reward 325.0, memory_length 2000, epsilon 0.6159989622864221 total_time 725.0\n",
            "episode 979, reward 558.0, memory_length 2000, epsilon 0.6129266546447325 total_time 721.0\n",
            "episode 989, reward 376.0, memory_length 2000, epsilon 0.6098696702013323 total_time 725.0\n",
            "episode 999, reward 333.0, memory_length 2000, epsilon 0.6068279325314512 total_time 721.0\n",
            "Saving Model 1000\n",
            "episode 1009, reward 554.0, memory_length 2000, epsilon 0.6038013655914889 total_time 731.0\n",
            "episode 1019, reward 678.0, memory_length 2000, epsilon 0.6007898937171146 total_time 723.0\n",
            "episode 1029, reward 68.0, memory_length 2000, epsilon 0.5977934416213744 total_time 725.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB5JiWZtQ6JZ"
      },
      "source": [
        "agent.save(name=\"model_weights.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J34clOaAQ6JY"
      },
      "source": [
        "### Tracking Convergence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahZvx4XpQ6JZ"
      },
      "source": [
        "agent.states_tracked"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGYxuCR3Q6JZ"
      },
      "source": [
        "state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6JWcayI0wNq"
      },
      "source": [
        "plt.figure(0, figsize=(16,7))\n",
        "plt.title('Q_value for state [0,0,0]  action (0,2)')\n",
        "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
        "plt.semilogy(xaxis,np.asarray(agent.states_tracked))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8-rEJ1N08Qa"
      },
      "source": [
        "score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EiYguio09uy"
      },
      "source": [
        "plt.figure(0, figsize=(16,7))\n",
        "plt.title('Rewards per episode')\n",
        "xaxis = np.asarray(range(0, len(score_tracked_sample)))\n",
        "plt.plot(xaxis,np.asarray(score_tracked_sample))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stD5OunxQ6Jg"
      },
      "source": [
        "#### Epsilon-decay sample function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Qc7xMkRQ6Jg"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "Try building a similar epsilon-decay function for your model.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tm6xR85Q6Jg"
      },
      "source": [
        "time = np.arange(0,10000)\n",
        "epsilon = []\n",
        "for i in range(0,10000):\n",
        "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg5jfTOwQ6Jh"
      },
      "source": [
        "plt.plot(time, epsilon)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o38wv7RxQ6Ji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}